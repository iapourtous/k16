"""
Module de recherche pour K16.
Interface simplifiÃ©e pour TreeFlat compressÃ© avec Numba JIT.
"""

import numpy as np
import time
from typing import List, Dict, Any, Tuple, Optional
import faiss

from k16.core.tree import K16Tree
from k16.io.reader import VectorReader

class Searcher:
    """
    Classe principale pour la recherche TreeFlat compressÃ©e.
    Utilise uniquement la structure plate optimisÃ©e avec compression et Numba JIT.
    """

    def __init__(self, k16tree: K16Tree, vectors_reader: VectorReader, use_faiss: bool = True,
                 search_type: str = "single", beam_width: int = 3, max_data: int = 4000):
        """
        Initialise le chercheur avec TreeFlat uniquement.

        Args:
            k16tree: Instance de K16Tree avec flat_tree
            vectors_reader: Lecteur de vecteurs
            use_faiss: Utiliser FAISS pour accÃ©lÃ©rer la recherche
            search_type: Type de recherche - "single" ou "beam"
            beam_width: Nombre de branches Ã  explorer en recherche par faisceau
            max_data: Nombre de vecteurs Ã  utiliser pour le remplissage
        """
        # TreeFlat uniquement
        self.k16tree = k16tree
        self.flat_tree = k16tree.flat_tree

        if self.flat_tree is None:
            raise ValueError("TreeFlat requis - structure non-plate non supportÃ©e")

        self.vectors_reader = vectors_reader
        self.use_faiss = use_faiss
        self.search_type = search_type
        self.beam_width = beam_width
        self.max_data = max_data
        self.faiss_available = True

        print("âœ“ Structure TreeFlat compressÃ©e activÃ©e")

        # VÃ©rifier si FAISS est disponible
        try:
            import faiss
            self.faiss_available = True
        except ImportError:
            self.faiss_available = False
            if use_faiss:
                print("âš ï¸ FAISS n'est pas disponible. Utilisation de numpy Ã  la place.")
                self.use_faiss = False

    def search_tree(self, query: np.ndarray) -> List[int]:
        """
        Recherche TreeFlat - retourne les candidats bruts.

        Args:
            query: Vecteur de requÃªte

        Returns:
            List[int]: Liste des indices candidats
        """
        if self.search_type == "beam":
            return self.flat_tree.search_tree_beam(query, self.beam_width)
        else:
            return self.flat_tree.search_tree_single(query)

    def filter_candidates(self, candidates: List[int], query: np.ndarray, k: int) -> List[int]:
        """
        Filtre les candidats pour ne garder que les k plus proches.
        
        Args:
            candidates: Liste des indices candidats
            query: Vecteur de requÃªte
            k: Nombre de voisins Ã  retourner
            
        Returns:
            List[int]: Liste des indices des k voisins les plus proches
        """
        if len(candidates) <= k:
            return candidates
        
        if self.use_faiss and self.faiss_available:
            # Filtrage avec FAISS
            candidate_vectors = self.vectors_reader[candidates]
            dimension = query.shape[0]
            index = faiss.IndexFlatIP(dimension)
            index.add(candidate_vectors)
            
            D, I = index.search(query.reshape(1, -1), k)
            results = [candidates[idx] for idx in I[0]]
        else:
            # Utiliser VectorReader optimisÃ©
            similarities = self.vectors_reader.dot(candidates, query)
            top_k_indices = np.argsort(-similarities)[:k]
            results = [candidates[idx] for idx in top_k_indices]
        
        return results
    
    def brute_force_search(self, query: np.ndarray, k: int = 10) -> List[int]:
        """
        Recherche naÃ¯ve des k voisins les plus proches.
        
        Args:
            query: Vecteur de requÃªte
            k: Nombre de voisins Ã  retourner
            
        Returns:
            List[int]: Liste des indices des k voisins les plus proches
        """
        if self.use_faiss and self.faiss_available:
            dimension = query.shape[0]
            
            if self.vectors_reader.mode == "ram":
                index = faiss.IndexFlatIP(dimension)
                index.add(self.vectors_reader.vectors)
                D, I = index.search(query.reshape(1, -1), k)
                return I[0].tolist()
            else:
                # Mode mmap par lots
                batch_size = 100000
                n_batches = (len(self.vectors_reader) + batch_size - 1) // batch_size
                
                final_indices = []
                final_distances = []
                
                for i in range(n_batches):
                    start_idx = i * batch_size
                    end_idx = min((i + 1) * batch_size, len(self.vectors_reader))
                    batch_indices = list(range(start_idx, end_idx))
                    
                    batch_vectors = self.vectors_reader[batch_indices]
                    index_batch = faiss.IndexFlatIP(dimension)
                    index_batch.add(batch_vectors)
                    
                    D, I = index_batch.search(query.reshape(1, -1), min(k, len(batch_indices)))
                    
                    global_indices = [batch_indices[idx] for idx in I[0]]
                    
                    for idx, dist in zip(global_indices, D[0]):
                        final_indices.append(idx)
                        final_distances.append(dist)
                    
                    if len(final_indices) >= 2*k:
                        sorted_pairs = sorted(zip(final_indices, final_distances), key=lambda x: x[1], reverse=True)
                        final_indices = [idx for idx, _ in sorted_pairs[:k]]
                        final_distances = [dist for _, dist in sorted_pairs[:k]]
                
                if len(final_indices) > k:
                    sorted_pairs = sorted(zip(final_indices, final_distances), key=lambda x: x[1], reverse=True)
                    final_indices = [idx for idx, _ in sorted_pairs[:k]]
                
                return final_indices
        else:
            # Recherche numpy
            if self.vectors_reader.mode == "ram":
                similarities = self.vectors_reader.dot(list(range(len(self.vectors_reader))), query)
                return np.argsort(-similarities)[:k].tolist()
            else:
                # Mode mmap par lots
                batch_size = 10000
                n_batches = (len(self.vectors_reader) + batch_size - 1) // batch_size
                
                top_k_indices = np.array([], dtype=int)
                top_k_similarities = np.array([], dtype=np.float32)
                
                for i in range(n_batches):
                    start_idx = i * batch_size
                    end_idx = min((i + 1) * batch_size, len(self.vectors_reader))
                    batch_indices = list(range(start_idx, end_idx))
                    
                    batch_similarities = self.vectors_reader.dot(batch_indices, query)
                    local_top_indices = np.argsort(-batch_similarities)[:k]
                    local_top_similarities = batch_similarities[local_top_indices]
                    
                    global_top_indices = np.array([batch_indices[idx] for idx in local_top_indices])
                    
                    if len(top_k_indices) > 0:
                        combined_indices = np.concatenate([top_k_indices, global_top_indices])
                        combined_similarities = np.concatenate([top_k_similarities, local_top_similarities])
                    else:
                        combined_indices = global_top_indices
                        combined_similarities = local_top_similarities
                    
                    top_k_positions = np.argsort(-combined_similarities)[:k]
                    top_k_indices = np.array([combined_indices[i] for i in top_k_positions])
                    top_k_similarities = combined_similarities[top_k_positions]
                
                return top_k_indices.tolist()
    
    def evaluate_search(self, queries: np.ndarray, k: int = 10) -> Dict[str, Any]:
        """
        Ã‰value les performances TreeFlat vs recherche naÃ¯ve.

        Args:
            queries: Vecteurs requÃªtes
            k: Nombre de voisins Ã  retourner

        Returns:
            Dict[str, Any]: Dictionnaire de mÃ©triques de performance
        """
        search_type_desc = f"{self.search_type} (beam_width={self.beam_width})" if self.search_type == "beam" else self.search_type
        print(f"\nâ³ Ã‰valuation avec {len(queries)} requÃªtes, k={k}, type de recherche: {search_type_desc} avec TreeFlat...")

        tree_search_time = 0
        tree_filter_time = 0
        naive_search_time = 0
        recall_sum = 0
        candidates_count = []

        try:
            from tqdm.auto import tqdm
            use_tqdm = True
        except ImportError:
            use_tqdm = False
            print("Info: tqdm not available, progress will not be shown")

        # Warmup Numba JIT
        print("ğŸ”¥ Warmup Numba JIT compilation...")
        warmup_query = queries[0]
        for _ in range(3):
            _ = self.flat_tree.search_tree_single(warmup_query)

        if use_tqdm:
            queries_iter = tqdm(queries, desc="Ã‰valuation")
        else:
            queries_iter = queries
            print(f"â³ Traitement de {len(queries)} requÃªtes...")

        for i, query in enumerate(queries_iter):
            # Recherche TreeFlat
            start_time = time.time()
            tree_candidates = self.search_tree(query)
            tree_search_time += time.time() - start_time

            candidates_count.append(len(tree_candidates))

            # Filtrage
            filter_start_time = time.time()
            tree_results = self.filter_candidates(tree_candidates, query, k)
            tree_filter_time += time.time() - filter_start_time

            # Recherche naÃ¯ve
            start_time = time.time()
            naive_results = self.brute_force_search(query, k)
            naive_search_time += time.time() - start_time

            # Calcul du recall
            intersection = set(tree_results).intersection(set(naive_results))
            recall = len(intersection) / k if k > 0 else 0
            recall_sum += recall

            if not use_tqdm and ((i + 1) % 10 == 0 or (i + 1) == len(queries)):
                print(f"  â†’ RequÃªte {i+1}/{len(queries)}: Recall = {recall:.4f}, Candidats = {len(tree_candidates)}")

        # Calcul des moyennes
        avg_tree_time = tree_search_time / len(queries)
        avg_filter_time = tree_filter_time / len(queries)
        avg_total_time = avg_tree_time + avg_filter_time
        avg_naive_time = naive_search_time / len(queries)
        avg_recall = recall_sum / len(queries)
        speedup = avg_naive_time / avg_total_time if avg_total_time > 0 else 0

        avg_candidates = sum(candidates_count) / len(candidates_count) if candidates_count else 0
        min_candidates = min(candidates_count) if candidates_count else 0
        max_candidates = max(candidates_count) if candidates_count else 0

        print("\nâœ“ RÃ©sultats de l'Ã©valuation:")
        print(f"  - Nombre de requÃªtes     : {len(queries)}")
        print(f"  - k (voisins demandÃ©s)   : {k}")
        print(f"  - Mode                   : {self.vectors_reader.mode.upper()}")
        print(f"  - Type de recherche      : {search_type_desc}")
        print(f"  - Statistiques candidats:")
        print(f"    â€¢ Moyenne             : {avg_candidates:.1f}")
        print(f"    â€¢ Minimum             : {min_candidates}")
        print(f"    â€¢ Maximum             : {max_candidates}")
        print(f"  - Temps moyen (arbre)    : {avg_tree_time*1000:.2f} ms")
        print(f"  - Temps moyen (filtre)   : {avg_filter_time*1000:.2f} ms")
        print(f"  - Temps moyen (total)    : {avg_total_time*1000:.2f} ms")
        print(f"  - Temps moyen (naÃ¯f)     : {avg_naive_time*1000:.2f} ms")
        print(f"  - AccÃ©lÃ©ration           : {speedup:.2f}x")
        print(f"  - Recall moyen           : {avg_recall:.4f} ({avg_recall*100:.2f}%)")

        return {
            "search_type": self.search_type,
            "beam_width": self.beam_width if self.search_type == "beam" else None,
            "avg_tree_time": avg_tree_time,
            "avg_filter_time": avg_filter_time,
            "avg_total_time": avg_total_time,
            "avg_naive_time": avg_naive_time,
            "speedup": speedup,
            "avg_recall": avg_recall,
            "avg_candidates": avg_candidates,
            "min_candidates": min_candidates,
            "max_candidates": max_candidates
        }

    def search(self, query: np.ndarray, k: int = 10) -> Tuple[List[int], List[float]]:
        """
        Effectue une recherche des k plus proches voisins.

        Args:
            query: Vecteur de requÃªte
            k: Nombre de voisins Ã  retourner

        Returns:
            Tuple[List[int], List[float]]: Tuple contenant (indices, scores)
        """
        # Normaliser la requÃªte pour des rÃ©sultats cohÃ©rents
        query_norm = np.linalg.norm(query)
        if query_norm > 0:
            query = query / query_norm

        # Trouver les candidats avec l'arbre
        candidates = self.search_tree(query)

        # Filtrer les candidats pour ne garder que les k plus proches
        top_indices = self.filter_candidates(candidates, query, k)

        # Calculer les scores de similaritÃ©
        top_vectors = self.vectors_reader[top_indices]
        scores = [np.dot(query, top_vectors[i]) for i in range(len(top_indices))]

        # Trier par score dÃ©croissant
        sorted_pairs = sorted(zip(top_indices, scores), key=lambda x: x[1], reverse=True)
        indices = [idx for idx, _ in sorted_pairs]
        scores = [score for _, score in sorted_pairs]

        return indices, scores

def search(tree: K16Tree, vectors_reader: VectorReader, query: np.ndarray, k: int = 10, 
           use_faiss: bool = True, search_type: str = "beam", beam_width: int = 3) -> Tuple[List[int], List[float]]:
    """
    Fonction utilitaire pour effectuer une recherche avec K16.

    Args:
        tree: Instance de K16Tree
        vectors_reader: Lecteur de vecteurs
        query: Vecteur de requÃªte
        k: Nombre de voisins Ã  retourner
        use_faiss: Utiliser FAISS pour accÃ©lÃ©rer la recherche
        search_type: Type de recherche - "single" ou "beam"
        beam_width: Nombre de branches Ã  explorer en recherche par faisceau

    Returns:
        Tuple[List[int], List[float]]: Tuple contenant (indices, scores)
    """
    searcher = Searcher(
        k16tree=tree,
        vectors_reader=vectors_reader,
        use_faiss=use_faiss,
        search_type=search_type,
        beam_width=beam_width
    )
    return searcher.search(query, k)