### k16/cli.py

```python
"""
Interface en ligne de commande pour K16.
Fournit des commandes pour t√©l√©charger des donn√©es, construire des arbres,
tester les performances et effectuer des recherches interactives.
"""

import os
import sys
import argparse

from k16.utils.config import ConfigManager
from k16.utils.cli_build import build_command
from k16.utils.cli_test import test_command
from k16.utils.cli_get_data import get_data_command
from k16.utils.cli_search import search_command
from k16.utils.cli_api import api_command

def main() -> int:
    """
    Point d'entr√©e principal pour l'interface en ligne de commande.
    
    Returns:
        int: Code de retour (0 pour succ√®s, autre pour erreur)
    """
    # Initialisation du gestionnaire de configuration par d√©faut
    config_manager = ConfigManager()

    # R√©cup√©ration des param√®tres pour la configuration par d√©faut
    build_config = config_manager.get_section("build_tree")
    search_config = config_manager.get_section("search")
    files_config = config_manager.get_section("files")
    flat_tree_config = config_manager.get_section("flat_tree")
    prepare_data_config = config_manager.get_section("prepare_data")

    # D√©finir les chemins par d√©faut
    default_vectors_path = os.path.join(files_config["vectors_dir"], files_config["default_vectors"])
    default_tree_path = os.path.join(files_config["trees_dir"], files_config["default_tree"])
    default_qa_path = os.path.join(files_config["vectors_dir"], files_config.get("default_qa", "qa.txt"))

    # Parseur principal
    parser = argparse.ArgumentParser(
        description="K16 - Biblioth√®que pour la recherche rapide de vecteurs d'embedding similaires",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--config", default=config_manager.config_path,
                        help=f"Chemin vers le fichier de configuration")
    parser.add_argument("--version", action="version", version="K16 v1.0.0")

    # Sous-parseurs pour les diff√©rentes commandes
    subparsers = parser.add_subparsers(dest="command", help="Commandes disponibles")

    # Commande API
    api_parser = subparsers.add_parser("api", help="D√©marrer l'API de recherche")
    api_parser.add_argument("--host", default=None,
                        help="Adresse d'h√¥te pour l'API (par d√©faut: 127.0.0.1)")
    api_parser.add_argument("--port", type=int, default=None,
                        help="Port pour l'API (par d√©faut: 8000)")
    api_parser.add_argument("--reload", action="store_true", default=None,
                        help="Activer le rechargement automatique en cas de modification du code")
    api_parser.set_defaults(func=api_command)

    # Commande build
    build_parser = subparsers.add_parser("build", help="Construire un arbre optimis√©")
    build_parser.add_argument("vectors_file", nargs="?", default=default_vectors_path,
                       help=f"Fichier binaire contenant les vecteurs embeddings")
    build_parser.add_argument("tree_file", nargs="?", default=default_tree_path,
                       help=f"Fichier de sortie pour l'arbre")
    build_parser.add_argument("--max_depth", type=int, default=build_config["max_depth"],
                       help=f"Profondeur maximale de l'arbre")
    build_parser.add_argument("--k", type=int, default=build_config["k"],
                       help=f"Nombre de branches par n≈ìud")
    build_parser.add_argument("--k_adaptive", action="store_true", default=False,
                       help="Utiliser la m√©thode du coude pour d√©terminer k automatiquement")
    build_parser.add_argument("--max_leaf_size", type=int, default=build_config["max_leaf_size"],
                       help=f"Taille maximale d'une feuille pour l'arr√™t de la subdivision")
    build_parser.add_argument("--max_data", type=int, default=build_config["max_data"],
                       help=f"MAX_DATA: Nombre de vecteurs √† stocker dans chaque feuille")
    build_parser.add_argument("--max_dims", type=int, default=flat_tree_config.get("max_dims", 128),
                       help=f"Nombre de dimensions √† conserver pour la r√©duction dimensionnelle")
    build_parser.add_argument("--hnsw", action="store_true", default=build_config.get("use_hnsw_improvement", True),
                       help="Activer l'am√©lioration des candidats par HNSW apr√®s construction")
    build_parser.set_defaults(func=build_command)

    # Commande getData
    data_parser = subparsers.add_parser("getData", help="T√©l√©charger et pr√©parer les donn√©es")
    data_parser.add_argument("out_text", nargs="?", default=default_qa_path,
                      help=f"Fichier texte QA (par d√©faut: {default_qa_path})")
    data_parser.add_argument("out_vec", nargs="?", default=default_vectors_path,
                      help=f"Fichier binaire embeddings (par d√©faut: {default_vectors_path})")
    data_parser.add_argument("--model", default=prepare_data_config.get("model", "intfloat/multilingual-e5-large"),
                      help=f"Mod√®le d'embedding √† utiliser (par d√©faut: {prepare_data_config.get('model', 'intfloat/multilingual-e5-large')})")
    data_parser.add_argument("--batch-size", type=int, default=prepare_data_config.get("batch_size", 128),
                      help=f"Taille des lots pour l'encodage (par d√©faut: {prepare_data_config.get('batch_size', 128)})")
    data_parser.add_argument("--force", action="store_true", default=False,
                      help="Forcer le recalcul des embeddings m√™me si le fichier existe d√©j√†")
    data_parser.set_defaults(func=get_data_command)

    # Commande test
    test_parser = subparsers.add_parser("test", help="Tester la performance de la recherche")
    test_parser.add_argument("vectors_file", nargs="?", default=default_vectors_path,
                       help=f"Fichier binaire contenant les vecteurs embeddings")
    test_parser.add_argument("tree_file", nargs="?", default=default_tree_path,
                       help=f"Fichier de l'arbre √† utiliser pour la recherche")
    test_parser.add_argument("--k", type=int, default=search_config["k"],
                       help=f"Nombre de voisins √† retourner")
    test_parser.add_argument("--mode", choices=["ram", "mmap"], default=search_config["mode"],
                       help=f"Mode de chargement des vecteurs")
    test_parser.add_argument("--cache_size", type=int, default=search_config["cache_size_mb"],
                       help=f"Taille du cache en MB pour le mode mmap")
    test_parser.add_argument("--queries", type=int, default=search_config["queries"],
                       help=f"Nombre de requ√™tes al√©atoires √† effectuer")
    test_parser.add_argument("--search_type", choices=["single", "beam"], default=search_config["search_type"],
                       help=f"Type de recherche dans l'arbre")
    test_parser.add_argument("--beam_width", type=int, default=search_config.get("beam_width", 3),
                       help=f"Largeur du faisceau pour la recherche beam")
    test_parser.add_argument("--use_faiss", action="store_true", default=search_config["use_faiss"],
                       help=f"Utiliser FAISS pour acc√©l√©rer le filtrage final")
    test_parser.add_argument("--evaluate", action="store_true", default=True,
                       help=f"√âvaluer les performances (recall et acc√©l√©ration)")
    test_parser.add_argument("--no-evaluate", dest="evaluate", action="store_false",
                       help=f"D√©sactiver l'√©valuation des performances")
    test_parser.set_defaults(func=test_command)
    
    # Commande search
    search_parser = subparsers.add_parser("search", help="Recherche interactive en ligne de commande")
    search_parser.add_argument("vectors_file", nargs="?", default=default_vectors_path,
                         help=f"Fichier binaire contenant les vecteurs embeddings")
    search_parser.add_argument("tree_file", nargs="?", default=default_tree_path,
                         help=f"Fichier de l'arbre √† utiliser pour la recherche")
    search_parser.add_argument("qa_file", nargs="?", default=default_qa_path,
                         help=f"Fichier contenant les questions et r√©ponses")
    search_parser.add_argument("--k", type=int, default=search_config["k"],
                         help=f"Nombre de r√©sultats √† afficher")
    search_parser.add_argument("--mode", choices=["ram", "mmap"], default=search_config["mode"],
                         help=f"Mode de chargement des vecteurs")
    search_parser.add_argument("--cache_size", type=int, default=search_config["cache_size_mb"],
                         help=f"Taille du cache en MB pour le mode mmap")
    search_parser.add_argument("--search_type", choices=["single", "beam"], default=search_config["search_type"],
                         help=f"Type de recherche dans l'arbre")
    search_parser.add_argument("--beam_width", type=int, default=search_config.get("beam_width", 3),
                         help=f"Largeur du faisceau pour la recherche beam")
    search_parser.add_argument("--use_faiss", action="store_true", default=search_config["use_faiss"],
                         help=f"Utiliser FAISS pour acc√©l√©rer le filtrage final")
    search_parser.add_argument("--model", default=prepare_data_config.get("model", "intfloat/multilingual-e5-large"),
                         help=f"Mod√®le d'embedding √† utiliser")
    search_parser.set_defaults(func=search_command)

    # Traitement des arguments
    args = parser.parse_args()

    # Ex√©cution de la commande sp√©cifi√©e
    if hasattr(args, "func"):
        return args.func(args)
    else:
        parser.print_help()
        return 0

if __name__ == "__main__":
    sys.exit(main())
```

### k16/search/searcher.py

```python
"""
Module de recherche pour K16.
Interface simplifi√©e pour TreeFlat compress√© avec Numba JIT.
"""

import numpy as np
import time
from typing import List, Dict, Any, Tuple, Optional
import faiss

from k16.core.tree import K16Tree
from k16.io.reader import VectorReader

class Searcher:
    """
    Classe principale pour la recherche TreeFlat compress√©e.
    Utilise uniquement la structure plate optimis√©e avec compression et Numba JIT.
    """

    def __init__(self, k16tree: K16Tree, vectors_reader: VectorReader, use_faiss: bool = True,
                 search_type: str = "single", beam_width: int = 3, max_data: int = 4000):
        """
        Initialise le chercheur avec TreeFlat uniquement.

        Args:
            k16tree: Instance de K16Tree avec flat_tree
            vectors_reader: Lecteur de vecteurs
            use_faiss: Utiliser FAISS pour acc√©l√©rer la recherche
            search_type: Type de recherche - "single" ou "beam"
            beam_width: Nombre de branches √† explorer en recherche par faisceau
            max_data: Nombre de vecteurs √† utiliser pour le remplissage
        """
        # TreeFlat uniquement
        self.k16tree = k16tree
        self.flat_tree = k16tree.flat_tree

        if self.flat_tree is None:
            raise ValueError("TreeFlat requis - structure non-plate non support√©e")

        self.vectors_reader = vectors_reader
        self.use_faiss = use_faiss
        self.search_type = search_type
        self.beam_width = beam_width
        self.max_data = max_data
        self.faiss_available = True

        print("‚úì Structure TreeFlat compress√©e activ√©e")

        # V√©rifier si FAISS est disponible
        try:
            import faiss
            self.faiss_available = True
        except ImportError:
            self.faiss_available = False
            if use_faiss:
                print("‚ö†Ô∏è FAISS n'est pas disponible. Utilisation de numpy √† la place.")
                self.use_faiss = False

    def search_tree(self, query: np.ndarray) -> List[int]:
        """
        Recherche TreeFlat - retourne les candidats bruts.

        Args:
            query: Vecteur de requ√™te

        Returns:
            List[int]: Liste des indices candidats
        """
        if self.search_type == "beam":
            return self.flat_tree.search_tree_beam(query, self.beam_width)
        else:
            return self.flat_tree.search_tree_single(query)

    def filter_candidates(self, candidates: List[int], query: np.ndarray, k: int) -> List[int]:
        """
        Filtre les candidats pour ne garder que les k plus proches.
        
        Args:
            candidates: Liste des indices candidats
            query: Vecteur de requ√™te
            k: Nombre de voisins √† retourner
            
        Returns:
            List[int]: Liste des indices des k voisins les plus proches
        """
        if len(candidates) <= k:
            return candidates
        
        if self.use_faiss and self.faiss_available:
            # Filtrage avec FAISS
            candidate_vectors = self.vectors_reader[candidates]
            dimension = query.shape[0]
            index = faiss.IndexFlatIP(dimension)
            index.add(candidate_vectors)
            
            D, I = index.search(query.reshape(1, -1), k)
            results = [candidates[idx] for idx in I[0]]
        else:
            # Utiliser VectorReader optimis√©
            similarities = self.vectors_reader.dot(candidates, query)
            top_k_indices = np.argsort(-similarities)[:k]
            results = [candidates[idx] for idx in top_k_indices]
        
        return results
    
    def brute_force_search(self, query: np.ndarray, k: int = 10) -> List[int]:
        """
        Recherche na√Øve des k voisins les plus proches.
        
        Args:
            query: Vecteur de requ√™te
            k: Nombre de voisins √† retourner
            
        Returns:
            List[int]: Liste des indices des k voisins les plus proches
        """
        if self.use_faiss and self.faiss_available:
            dimension = query.shape[0]
            
            if self.vectors_reader.mode == "ram":
                index = faiss.IndexFlatIP(dimension)
                index.add(self.vectors_reader.vectors)
                D, I = index.search(query.reshape(1, -1), k)
                return I[0].tolist()
            else:
                # Mode mmap par lots
                batch_size = 100000
                n_batches = (len(self.vectors_reader) + batch_size - 1) // batch_size
                
                final_indices = []
                final_distances = []
                
                for i in range(n_batches):
                    start_idx = i * batch_size
                    end_idx = min((i + 1) * batch_size, len(self.vectors_reader))
                    batch_indices = list(range(start_idx, end_idx))
                    
                    batch_vectors = self.vectors_reader[batch_indices]
                    index_batch = faiss.IndexFlatIP(dimension)
                    index_batch.add(batch_vectors)
                    
                    D, I = index_batch.search(query.reshape(1, -1), min(k, len(batch_indices)))
                    
                    global_indices = [batch_indices[idx] for idx in I[0]]
                    
                    for idx, dist in zip(global_indices, D[0]):
                        final_indices.append(idx)
                        final_distances.append(dist)
                    
                    if len(final_indices) >= 2*k:
                        sorted_pairs = sorted(zip(final_indices, final_distances), key=lambda x: x[1], reverse=True)
                        final_indices = [idx for idx, _ in sorted_pairs[:k]]
                        final_distances = [dist for _, dist in sorted_pairs[:k]]
                
                if len(final_indices) > k:
                    sorted_pairs = sorted(zip(final_indices, final_distances), key=lambda x: x[1], reverse=True)
                    final_indices = [idx for idx, _ in sorted_pairs[:k]]
                
                return final_indices
        else:
            # Recherche numpy
            if self.vectors_reader.mode == "ram":
                similarities = self.vectors_reader.dot(list(range(len(self.vectors_reader))), query)
                return np.argsort(-similarities)[:k].tolist()
            else:
                # Mode mmap par lots
                batch_size = 10000
                n_batches = (len(self.vectors_reader) + batch_size - 1) // batch_size
                
                top_k_indices = np.array([], dtype=int)
                top_k_similarities = np.array([], dtype=np.float32)
                
                for i in range(n_batches):
                    start_idx = i * batch_size
                    end_idx = min((i + 1) * batch_size, len(self.vectors_reader))
                    batch_indices = list(range(start_idx, end_idx))
                    
                    batch_similarities = self.vectors_reader.dot(batch_indices, query)
                    local_top_indices = np.argsort(-batch_similarities)[:k]
                    local_top_similarities = batch_similarities[local_top_indices]
                    
                    global_top_indices = np.array([batch_indices[idx] for idx in local_top_indices])
                    
                    if len(top_k_indices) > 0:
                        combined_indices = np.concatenate([top_k_indices, global_top_indices])
                        combined_similarities = np.concatenate([top_k_similarities, local_top_similarities])
                    else:
                        combined_indices = global_top_indices
                        combined_similarities = local_top_similarities
                    
                    top_k_positions = np.argsort(-combined_similarities)[:k]
                    top_k_indices = np.array([combined_indices[i] for i in top_k_positions])
                    top_k_similarities = combined_similarities[top_k_positions]
                
                return top_k_indices.tolist()
    
    def evaluate_search(self, queries: np.ndarray, k: int = 10) -> Dict[str, Any]:
        """
        √âvalue les performances TreeFlat vs recherche na√Øve.

        Args:
            queries: Vecteurs requ√™tes
            k: Nombre de voisins √† retourner

        Returns:
            Dict[str, Any]: Dictionnaire de m√©triques de performance
        """
        search_type_desc = f"{self.search_type} (beam_width={self.beam_width})" if self.search_type == "beam" else self.search_type
        print(f"\n‚è≥ √âvaluation avec {len(queries)} requ√™tes, k={k}, type de recherche: {search_type_desc} avec TreeFlat...")

        tree_search_time = 0
        tree_filter_time = 0
        naive_search_time = 0
        recall_sum = 0
        candidates_count = []

        try:
            from tqdm.auto import tqdm
            use_tqdm = True
        except ImportError:
            use_tqdm = False
            print("Info: tqdm not available, progress will not be shown")

        # Warmup Numba JIT
        print("üî• Warmup Numba JIT compilation...")
        warmup_query = queries[0]
        for _ in range(3):
            _ = self.flat_tree.search_tree_single(warmup_query)

        if use_tqdm:
            queries_iter = tqdm(queries, desc="√âvaluation")
        else:
            queries_iter = queries
            print(f"‚è≥ Traitement de {len(queries)} requ√™tes...")

        for i, query in enumerate(queries_iter):
            # Recherche TreeFlat
            start_time = time.time()
            tree_candidates = self.search_tree(query)
            tree_search_time += time.time() - start_time

            candidates_count.append(len(tree_candidates))

            # Filtrage
            filter_start_time = time.time()
            tree_results = self.filter_candidates(tree_candidates, query, k)
            tree_filter_time += time.time() - filter_start_time

            # Recherche na√Øve
            start_time = time.time()
            naive_results = self.brute_force_search(query, k)
            naive_search_time += time.time() - start_time

            # Calcul du recall
            intersection = set(tree_results).intersection(set(naive_results))
            recall = len(intersection) / k if k > 0 else 0
            recall_sum += recall

            if not use_tqdm and ((i + 1) % 10 == 0 or (i + 1) == len(queries)):
                print(f"  ‚Üí Requ√™te {i+1}/{len(queries)}: Recall = {recall:.4f}, Candidats = {len(tree_candidates)}")

        # Calcul des moyennes
        avg_tree_time = tree_search_time / len(queries)
        avg_filter_time = tree_filter_time / len(queries)
        avg_total_time = avg_tree_time + avg_filter_time
        avg_naive_time = naive_search_time / len(queries)
        avg_recall = recall_sum / len(queries)
        speedup = avg_naive_time / avg_total_time if avg_total_time > 0 else 0

        avg_candidates = sum(candidates_count) / len(candidates_count) if candidates_count else 0
        min_candidates = min(candidates_count) if candidates_count else 0
        max_candidates = max(candidates_count) if candidates_count else 0

        print("\n‚úì R√©sultats de l'√©valuation:")
        print(f"  - Nombre de requ√™tes     : {len(queries)}")
        print(f"  - k (voisins demand√©s)   : {k}")
        print(f"  - Mode                   : {self.vectors_reader.mode.upper()}")
        print(f"  - Type de recherche      : {search_type_desc}")
        print(f"  - Statistiques candidats:")
        print(f"    ‚Ä¢ Moyenne             : {avg_candidates:.1f}")
        print(f"    ‚Ä¢ Minimum             : {min_candidates}")
        print(f"    ‚Ä¢ Maximum             : {max_candidates}")
        print(f"  - Temps moyen (arbre)    : {avg_tree_time*1000:.2f} ms")
        print(f"  - Temps moyen (filtre)   : {avg_filter_time*1000:.2f} ms")
        print(f"  - Temps moyen (total)    : {avg_total_time*1000:.2f} ms")
        print(f"  - Temps moyen (na√Øf)     : {avg_naive_time*1000:.2f} ms")
        print(f"  - Acc√©l√©ration           : {speedup:.2f}x")
        print(f"  - Recall moyen           : {avg_recall:.4f} ({avg_recall*100:.2f}%)")

        return {
            "search_type": self.search_type,
            "beam_width": self.beam_width if self.search_type == "beam" else None,
            "avg_tree_time": avg_tree_time,
            "avg_filter_time": avg_filter_time,
            "avg_total_time": avg_total_time,
            "avg_naive_time": avg_naive_time,
            "speedup": speedup,
            "avg_recall": avg_recall,
            "avg_candidates": avg_candidates,
            "min_candidates": min_candidates,
            "max_candidates": max_candidates
        }

    def search(self, query: np.ndarray, k: int = 10) -> Tuple[List[int], List[float]]:
        """
        Effectue une recherche des k plus proches voisins.

        Args:
            query: Vecteur de requ√™te
            k: Nombre de voisins √† retourner

        Returns:
            Tuple[List[int], List[float]]: Tuple contenant (indices, scores)
        """
        # Normaliser la requ√™te pour des r√©sultats coh√©rents
        query_norm = np.linalg.norm(query)
        if query_norm > 0:
            query = query / query_norm

        # Trouver les candidats avec l'arbre
        candidates = self.search_tree(query)

        # Filtrer les candidats pour ne garder que les k plus proches
        top_indices = self.filter_candidates(candidates, query, k)

        # Calculer les scores de similarit√©
        top_vectors = self.vectors_reader[top_indices]
        scores = [np.dot(query, top_vectors[i]) for i in range(len(top_indices))]

        # Trier par score d√©croissant
        sorted_pairs = sorted(zip(top_indices, scores), key=lambda x: x[1], reverse=True)
        indices = [idx for idx, _ in sorted_pairs]
        scores = [score for _, score in sorted_pairs]

        return indices, scores

def search(tree: K16Tree, vectors_reader: VectorReader, query: np.ndarray, k: int = 10, 
           use_faiss: bool = True, search_type: str = "beam", beam_width: int = 3) -> Tuple[List[int], List[float]]:
    """
    Fonction utilitaire pour effectuer une recherche avec K16.

    Args:
        tree: Instance de K16Tree
        vectors_reader: Lecteur de vecteurs
        query: Vecteur de requ√™te
        k: Nombre de voisins √† retourner
        use_faiss: Utiliser FAISS pour acc√©l√©rer la recherche
        search_type: Type de recherche - "single" ou "beam"
        beam_width: Nombre de branches √† explorer en recherche par faisceau

    Returns:
        Tuple[List[int], List[float]]: Tuple contenant (indices, scores)
    """
    searcher = Searcher(
        k16tree=tree,
        vectors_reader=vectors_reader,
        use_faiss=use_faiss,
        search_type=search_type,
        beam_width=beam_width
    )
    return searcher.search(query, k)
```

### k16/io/writer.py

```python
"""
Module d'√©criture de vecteurs et d'arbres pour K16.
Fournit des classes optimis√©es pour sauvegarder des vecteurs et des arbres.
"""

import os
import struct
import time
import numpy as np
from typing import Optional

from k16.core.tree import K16Tree

class VectorWriter:
    """Classe pour √©crire des vecteurs dans un fichier binaire."""
    
    @staticmethod
    def write_bin(vectors: np.ndarray, file_path: str) -> None:
        """
        √âcrit des vecteurs dans un fichier binaire.
        Format: header (n, d: uint64) suivi des donn√©es en float32.
        
        Args:
            vectors: Tableau numpy contenant les vecteurs (shape: [n, d])
            file_path: Chemin du fichier de sortie
        """
        n, d = vectors.shape
        start_time = time.time()
        print(f"‚è≥ √âcriture de {n:,} vecteurs (dim {d}) vers {file_path}...")
        
        # Cr√©er le r√©pertoire si n√©cessaire
        os.makedirs(os.path.dirname(os.path.abspath(file_path)), exist_ok=True)
        
        # Convertir en float32 si ce n'est pas d√©j√† le cas
        vectors_float32 = vectors.astype(np.float32)
        
        with open(file_path, "wb") as f:
            # √âcrire l'ent√™te: nombre de vecteurs (n) et dimension (d)
            f.write(struct.pack("<QQ", n, d))
            # √âcrire les donn√©es
            f.write(vectors_float32.tobytes())
        
        elapsed = time.time() - start_time
        print(f"‚úì {n:,} vecteurs (dim {d}) √©crits dans {file_path} [termin√© en {elapsed:.2f}s]")

def write_vectors(vectors: np.ndarray, file_path: str) -> None:
    """
    Fonction utilitaire pour √©crire des vecteurs dans un fichier.
    
    Args:
        vectors: Tableau numpy contenant les vecteurs (shape: [n, d])
        file_path: Chemin du fichier de sortie
    """
    VectorWriter.write_bin(vectors, file_path)

def write_tree(tree: K16Tree, file_path: str, mmap_dir: bool = False, minimal: bool = True) -> None:
    """
    Sauvegarde un arbre K16 au format plat optimis√©.
    
    Args:
        tree: L'arbre K16 √† sauvegarder
        file_path: Chemin du fichier de sortie
        mmap_dir: Si True, cr√©e un r√©pertoire de fichiers numpy pour le memory-mapping
        minimal: Si True (par d√©faut), sauvegarde uniquement les structures essentielles
    """
    # V√©rifier que l'arbre a une structure plate
    if tree.flat_tree is None:
        raise ValueError("L'arbre doit avoir une structure plate (flat_tree) pour √™tre sauvegard√©")
    
    # D√©terminer le chemin de sauvegarde
    if not file_path.endswith('.flat.npy'):
        flat_path = os.path.splitext(file_path)[0] + '.flat.npy'
    else:
        flat_path = file_path
    
    print(f"‚è≥ Sauvegarde de la structure plate vers {flat_path}...")
    
    # Cr√©er le r√©pertoire si n√©cessaire
    os.makedirs(os.path.dirname(os.path.abspath(flat_path)), exist_ok=True)
    
    # Sauvegarder la structure plate
    tree.flat_tree.save(flat_path, mmap_dir=mmap_dir, minimal=minimal)
    
    print(f"‚úì Structure plate sauvegard√©e vers {flat_path}")
    
    return flat_path
```

### k16/io/reader.py

```python
"""
Module de lecture de vecteurs et d'arbres pour K16.
Fournit des classes optimis√©es pour charger des vecteurs et des arbres.
"""

import os
import struct
import time
import numpy as np
import mmap
import sys
from typing import Tuple, List, Dict, Any, Optional, Union
from collections import OrderedDict

from k16.core.tree import K16Tree
from k16.utils.config import ConfigManager
from k16.core.flat_tree import TreeFlat

class VectorReader:
    """
    Classe pour lire des vecteurs depuis un fichier binaire.
    Supporte deux modes: RAM et mmap.
    """
    
    def __init__(self, file_path: str, mode: str = "ram", cache_size_mb: Optional[int] = None):
        """
        Initialise le lecteur de vecteurs.
        
        Args:
            file_path: Chemin vers le fichier binaire contenant les vecteurs
            mode: Mode de lecture - "ram" (d√©faut) ou "mmap"
            cache_size_mb: Taille du cache en m√©gaoctets pour le mode mmap (si None, utilise la valeur de ConfigManager)
        """
        self.file_path = file_path
        self.mode = mode.lower()
        
        if self.mode not in ["ram", "mmap"]:
            raise ValueError("Mode doit √™tre 'ram', 'mmap'")
        
        # Param√®tres des vecteurs
        self.n = 0  # Nombre de vecteurs
        self.d = 0  # Dimension des vecteurs
        self.header_size = 16  # 2 entiers 64 bits
        self.vector_size = 0  # Taille d'un vecteur en octets
        
        # Stockage des vecteurs
        self.vectors = None  # Pour le mode RAM
        self.mmap_file = None  # Pour le mode mmap
        self.mmap_obj = None  # Pour le mode mmap
        
        # Cache LRU pour le mode mmap
        self.cache_enabled = False
        self.cache_size_mb = cache_size_mb
        if self.cache_size_mb is None:
            # Obtenir la valeur par d√©faut de la configuration
            config_manager = ConfigManager()
            self.cache_size_mb = config_manager.get("search", "cache_size_mb", 500)
        
        self.cache = None
        self.cache_hits = 0
        self.cache_misses = 0
        self.max_cache_size = 0
        self.cache_capacity = 0
        
        # Charger les vecteurs
        self._load_vectors()
    
    def _load_vectors(self) -> None:
        """Charge les vecteurs selon le mode choisi."""
        start_time = time.time()
        print(f"‚è≥ Chargement des vecteurs depuis {self.file_path} en mode {self.mode.upper()}...")
        
        with open(self.file_path, "rb") as f:
            # Lire l'en-t√™te (nombre et dimension des vecteurs)
            self.n, self.d = struct.unpack("<QQ", f.read(self.header_size))
            print(f"  ‚Üí Format d√©tect√©: {self.n:,} vecteurs de dimension {self.d}")
            
            # Calculer la taille d'un vecteur en octets
            self.vector_size = self.d * 4  # float32 = 4 octets
            
            if self.mode == "ram":
                # Mode RAM: charger tous les vecteurs en m√©moire
                buffer = f.read(self.n * self.vector_size)
                self.vectors = np.frombuffer(buffer, dtype=np.float32).reshape(self.n, self.d)
            else:
                # Mode mmap: mapper le fichier en m√©moire
                self.mmap_file = open(self.file_path, "rb")
                self.mmap_obj = mmap.mmap(self.mmap_file.fileno(), 0, access=mmap.ACCESS_READ)
                
                # Initialiser le cache LRU
                self.max_cache_size = self.cache_size_mb * 1024 * 1024  # Convertir MB en octets
                self.cache = OrderedDict()  # Cache LRU
                self.cache_enabled = True
                
                # Limiter la taille du cache si trop grande
                max_possible_vectors = self.n
                max_possible_size = max_possible_vectors * self.vector_size
                if self.max_cache_size > max_possible_size:
                    adjusted_size = max_possible_size
                    adjusted_mb = adjusted_size / (1024 * 1024)
                    print(f"  ‚Üí Cache limit√© √† {adjusted_mb:.1f} MB (taille totale des donn√©es)")
                    self.max_cache_size = adjusted_size
                
                # Calculer combien de vecteurs peuvent tenir dans le cache
                self.cache_capacity = self.max_cache_size // self.vector_size
                print(f"  ‚Üí Cache LRU initialis√©: {self.cache_size_mb} MB ({self.cache_capacity:,} vecteurs)")
        
        elapsed = time.time() - start_time
        
        if self.mode == "ram":
            memory_usage = f"{self.vectors.nbytes / (1024**2):.1f} MB"
        else:
            memory_usage = f"Mmap + Cache {self.cache_size_mb} MB"
            
        print(f"‚úì {self.n:,} vecteurs (dim {self.d}) pr√™ts en mode {self.mode.upper()} [termin√© en {elapsed:.2f}s]")
        print(f"  ‚Üí M√©moire utilis√©e: {memory_usage}")
    
    def _update_cache_stats(self, force: bool = False) -> bool:
        """
        Affiche les statistiques du cache si assez d'acc√®s ont √©t√© effectu√©s.
        
        Args:
            force: Si True, affiche les statistiques m√™me si le seuil n'est pas atteint
            
        Returns:
            bool: True si les statistiques ont √©t√© affich√©es, False sinon
        """
        if not self.cache_enabled:
            return False
            
        total = self.cache_hits + self.cache_misses
        
        # Afficher les stats tous les 100000 acc√®s ou si forc√©
        if force or (total > 0 and total % 100000 == 0):
            hit_rate = self.cache_hits / total * 100 if total > 0 else 0
            cache_usage = len(self.cache) / self.cache_capacity * 100 if self.cache_capacity > 0 else 0
            print(f"  ‚Üí Cache stats: {hit_rate:.1f}% hits, {cache_usage:.1f}% rempli "
                  f"({len(self.cache):,}/{self.cache_capacity:,} vecteurs)")
            return True
        return False
    
    def __getitem__(self, index):
        """
        R√©cup√®re un ou plusieurs vecteurs par leur indice, avec cache LRU en mode mmap.
        
        Args:
            index: Un entier, une liste d'entiers ou un slice
            
        Returns:
            Un vecteur numpy ou un tableau de vecteurs
        """
        if self.mode == "ram":
            # En mode RAM, juste indexer le tableau numpy
            return self.vectors[index]
        else:
            # En mode mmap, utiliser le cache si disponible, sinon lire depuis le fichier
            if isinstance(index, (int, np.integer)):
                # Cas d'un seul indice - v√©rifier le cache d'abord
                if self.cache_enabled and index in self.cache:
                    # Cache hit
                    self.cache_hits += 1
                    # D√©placer l'√©l√©ment √† la fin (MRU)
                    vector = self.cache.pop(index)
                    self.cache[index] = vector
                    self._update_cache_stats()
                    return vector
                else:
                    # Cache miss - lire depuis le fichier
                    if self.cache_enabled:
                        self.cache_misses += 1
                    
                    # Lire le vecteur depuis mmap
                    offset = self.header_size + index * self.vector_size
                    self.mmap_obj.seek(offset)
                    vector_bytes = self.mmap_obj.read(self.vector_size)
                    vector = np.frombuffer(vector_bytes, dtype=np.float32)
                    
                    # Mettre en cache si activ√©
                    if self.cache_enabled:
                        # Si le cache est plein, supprimer l'√©l√©ment le moins r√©cemment utilis√© (LRU)
                        if len(self.cache) >= self.cache_capacity and self.cache_capacity > 0:
                            self.cache.popitem(last=False)  # Supprimer le premier √©l√©ment (LRU)
                        
                        # Ajouter au cache
                        self.cache[index] = vector
                        
                        self._update_cache_stats()
                    
                    return vector
            elif isinstance(index, slice):
                # Cas d'un slice
                start = index.start or 0
                stop = index.stop or self.n
                step = index.step or 1
                
                if step == 1:
                    # Optimisation: lecture en bloc pour les slices cons√©cutifs
                    size = stop - start
                    
                    # V√©rifier si tous les √©l√©ments sont dans le cache
                    if self.cache_enabled and size <= 1000:  # Pour √©viter de trop grandes v√©rifications
                        cache_indices = [i for i in range(start, stop) if i in self.cache]
                        # Si plus de 80% des indices sont dans le cache, utiliser le cache
                        if len(cache_indices) > 0.8 * size:
                            return np.array([self[i] for i in range(start, stop)])
                    
                    # Lecture directe depuis le fichier
                    offset = self.header_size + start * self.vector_size
                    self.mmap_obj.seek(offset)
                    buffer = self.mmap_obj.read(size * self.vector_size)
                    vectors = np.frombuffer(buffer, dtype=np.float32).reshape(size, self.d)
                    
                    # Mettre en cache les vecteurs fr√©quemment utilis√©s
                    if self.cache_enabled and size <= 50:  # Limiter aux petites plages
                        for i, idx in enumerate(range(start, stop)):
                            # Si le cache est plein, supprimer l'√©l√©ment LRU
                            if len(self.cache) >= self.cache_capacity:
                                self.cache.popitem(last=False)
                            self.cache[idx] = vectors[i]
                    
                    return vectors
                else:
                    # Cas non optimis√© pour les slices avec step > 1
                    indices = range(start, stop, step)
                    return np.array([self[i] for i in indices])
            else:
                # Cas d'une liste d'indices
                if len(index) > 200:
                    # Optimisation 1: V√©rifier le cache pour tous les indices d'abord
                    if self.cache_enabled:
                        # S√©parer les indices pr√©sents dans le cache et ceux absents
                        cached_indices = []
                        missing_indices = []
                        
                        for idx in index:
                            if idx in self.cache:
                                cached_indices.append(idx)
                            else:
                                missing_indices.append(idx)
                        
                        # Si plus de 80% sont dans le cache, acc√©der individuellement
                        if len(cached_indices) > 0.8 * len(index):
                            return np.array([self[i] for i in index])
                    
                    # Optimisation 2: regrouper les indices cons√©cutifs pour les grandes listes
                    sorted_indices = np.sort(index)
                    groups = []
                    current_group = [sorted_indices[0]]
                    
                    # Identifier les groupes d'indices cons√©cutifs
                    for i in range(1, len(sorted_indices)):
                        if sorted_indices[i] == sorted_indices[i-1] + 1:
                            current_group.append(sorted_indices[i])
                        else:
                            groups.append(current_group)
                            current_group = [sorted_indices[i]]
                    groups.append(current_group)
                    
                    # Lire chaque groupe en bloc
                    vectors = []
                    for group in groups:
                        if len(group) == 1:
                            # Un seul indice
                            vectors.append(self[group[0]])
                        else:
                            # Bloc d'indices cons√©cutifs
                            start, end = group[0], group[-1] + 1
                            vectors.append(self[start:end])
                    
                    # R√©organiser les vecteurs selon l'ordre original des indices
                    result = np.vstack([v if len(v.shape) > 1 else v.reshape(1, -1) for v in vectors])
                    index_map = {idx: i for i, idx in enumerate(sorted_indices)}
                    return result[[index_map[idx] for idx in index]]
                else:
                    # Pour les petites listes, utiliser le cache pour chaque vecteur
                    return np.array([self[i] for i in index])
    
    def __len__(self) -> int:
        """Retourne le nombre de vecteurs."""
        return self.n
    
    def dot(self, vectors, query):
        """
        Calcule le produit scalaire entre les vecteurs et une requ√™te.
        Optimis√© pour SIMD avec des tailles adapt√©es (multiples de 16/32/64).

        Args:
            vectors: Les vecteurs (indice ou liste d'indices)
            query: Le vecteur requ√™te

        Returns:
            Un tableau numpy avec les scores de similarit√©
        """
        # S'assurer que la requ√™te est en float32 et contigu√´ (optimal pour SIMD)
        query = np.ascontiguousarray(query, dtype=np.float32)

        if self.mode == "ram":
            # En mode RAM, utiliser un dot product vectoris√© pour SIMD
            vecs = self[vectors]
            # Assurer contigu√Øt√© m√©moire pour optimisations SIMD
            if not vecs.flags.c_contiguous:
                vecs = np.ascontiguousarray(vecs, dtype=np.float32)
            return np.dot(vecs, query)
        else:
            # En mode mmap, strat√©gie optimis√©e pour diff√©rentes tailles
            if isinstance(vectors, (int, np.integer)):
                # Cas d'un seul vecteur
                return np.dot(self[vectors], query)
            else:
                # Adapter la taille des lots pour maximiser SIMD
                # Choisir une taille de lot qui est multiple de 16
                # pour l'alignement m√©moire optimal avec AVX-512
                n_vectors = len(vectors)

                if n_vectors <= 256:
                    # Petits lots: lire en une fois pour √©viter l'overhead
                    vectors_array = self[vectors]
                    if not vectors_array.flags.c_contiguous:
                        vectors_array = np.ascontiguousarray(vectors_array, dtype=np.float32)
                    return np.dot(vectors_array, query)
                else:
                    # Pour les grandes listes, traiter par lots optimis√©s pour SIMD
                    # 256 est un bon compromis (registres AVX2/AVX-512, cache L1/L2)
                    batch_size = 256  # Multiple de 16 pour SIMD optimal
                    n_batches = (n_vectors + batch_size - 1) // batch_size

                    # Pr√©allouer avec alignement m√©moire optimal
                    results = np.empty(n_vectors, dtype=np.float32)

                    for i in range(n_batches):
                        start_idx = i * batch_size
                        end_idx = min((i + 1) * batch_size, n_vectors)
                        batch_indices = vectors[start_idx:end_idx]

                        # Lecture optimis√©e des vecteurs
                        batch_vectors = self[batch_indices]
                        # Assurer contigu√Øt√© pour SIMD
                        if not batch_vectors.flags.c_contiguous:
                            batch_vectors = np.ascontiguousarray(batch_vectors, dtype=np.float32)

                        # Dot product vectoris√© (exploite AVX2/AVX-512)
                        results[start_idx:end_idx] = np.dot(batch_vectors, query)

                    return results
    
    def close(self) -> None:
        """Lib√®re les ressources, y compris le cache."""
        if self.mode == "mmap" and self.mmap_obj is not None:
            # Afficher les statistiques finales du cache
            if self.cache_enabled:
                self._update_cache_stats(force=True)
                print(f"  ‚Üí Cache final: {len(self.cache):,} vecteurs, {self.cache_hits:,} hits, {self.cache_misses:,} misses")
                
                # Vider le cache
                self.cache.clear()
                self.cache = None
            
            # Fermer mmap
            self.mmap_obj.close()
            self.mmap_file.close()
            self.mmap_obj = None
            self.mmap_file = None

def read_vectors(file_path: str = None, mode: str = "ram", cache_size_mb: Optional[int] = None, vectors: Optional[np.ndarray] = None) -> VectorReader:
    """
    Fonction utilitaire pour lire des vecteurs depuis un fichier ou un tableau numpy.

    Args:
        file_path: Chemin vers le fichier binaire contenant les vecteurs (None si vectors est fourni)
        mode: Mode de lecture - "ram" (d√©faut) ou "mmap"
        cache_size_mb: Taille du cache en m√©gaoctets pour le mode mmap (si None, utilise la valeur de ConfigManager)
        vectors: Tableau numpy de vecteurs √† utiliser directement (None si file_path est fourni)

    Returns:
        VectorReader: Instance de lecteur de vecteurs
    """
    if vectors is not None:
        # Cr√©er un VectorReader avec les vecteurs fournis
        reader = VectorReader.__new__(VectorReader)
        reader.file_path = None
        reader.mode = "ram"
        reader.n, reader.d = vectors.shape
        reader.header_size = 16
        reader.vector_size = reader.d * 4
        reader.vectors = vectors
        reader.mmap_file = None
        reader.mmap_obj = None
        reader.cache_enabled = False
        reader.cache = None
        reader.cache_hits = 0
        reader.cache_misses = 0
        reader.max_cache_size = 0
        reader.cache_capacity = 0
        return reader
    elif file_path is not None:
        return VectorReader(file_path, mode, cache_size_mb)
    else:
        raise ValueError("Soit file_path soit vectors doit √™tre fourni")

def load_tree(file_path: str, mmap_tree: bool = False) -> K16Tree:
    """
    Charge la structure plate optimis√©e de l'arbre depuis le fichier pr√©compil√©.

    Args:
        file_path: Chemin du fichier binaire de l'arbre ou du fichier plat (.flat.npy)
        mmap_tree: Si True, charge la structure plate en mode mmap pour √©conomiser la RAM

    Returns:
        K16Tree: Arbre K16 charg√© avec la structure plate
    """
    # D√©terminer le chemin du fichier plat
    if file_path.endswith('.flat') or file_path.endswith('.flat.npy'):
        flat_path = file_path
    else:
        flat_path = os.path.splitext(file_path)[0] + '.flat.npy'

    print(f"‚è≥ Chargement de la structure plate optimis√©e depuis {flat_path}...")
    start_time = time.time()

    # Essayer de charger l'arbre
    try:
        if mmap_tree:
            try:
                flat_tree = TreeFlat.load(flat_path, mmap_mode='r')
            except ValueError as e:
                print(f"‚ö†Ô∏è √âchec du memory-mapping de l'arbre ({e}), chargement en RAM.")
                flat_tree = TreeFlat.load(flat_path)
        else:
            flat_tree = TreeFlat.load(flat_path)
        print(f"‚úì Structure TreeFlat compress√©e activ√©e")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors du chargement de l'arbre: {e}")
        raise

    tree = K16Tree(None)
    tree.flat_tree = flat_tree
    elapsed = time.time() - start_time
    print(f"‚úì Structure plate charg√©e en {elapsed:.2f}s")
    return tree
```

### k16/utils/cli_get_data.py

```python
"""
Module pour le t√©l√©chargement et la pr√©paration des donn√©es.
Fournit des fonctions pour t√©l√©charger le dataset Natural Questions et g√©n√©rer les embeddings.
"""

import os
import sys
import time
import datetime
import struct
import numpy as np
import argparse
from typing import List, Dict, Any
from tqdm.auto import tqdm

from k16.utils.config import ConfigManager

def format_time(seconds: float) -> str:
    """Formate le temps en heures, minutes, secondes."""
    return str(datetime.timedelta(seconds=int(seconds)))

def write_vectors(vecs: np.ndarray, path: str):
    """
    √âcrit les vecteurs dans un binaire : header (n, d) + float32 data.
    
    Args:
        vecs: Vecteurs √† √©crire
        path: Chemin du fichier de sortie
    """
    n, d = vecs.shape
    start_time = time.time()
    print(f"‚è≥ √âcriture de {n:,} vecteurs (dim {d}) vers {path}...")
    
    # S'assurer que le r√©pertoire existe
    os.makedirs(os.path.dirname(path) if os.path.dirname(path) else ".", exist_ok=True)
    
    # Convertir en float32 et s'assurer que les vecteurs sont normalis√©s
    vecs_float32 = vecs.astype(np.float32)
    
    # Format exact attendu par build_candidates
    with open(path, "wb") as f:
        # Utiliser QQ (uint64_t) pour assurer la compatibilit√© avec endianness explicite
        f.write(struct.pack("<QQ", n, d))
        f.write(vecs_float32.tobytes())
    
    elapsed = time.time() - start_time
    print(f"‚úì {n:,} vecteurs (dim {d}) √©crits dans {path} [termin√© en {elapsed:.2f}s]")

def get_data_command(args: argparse.Namespace) -> int:
    """
    Commande pour t√©l√©charger et pr√©parer les donn√©es.
    
    Args:
        args: Arguments de ligne de commande
        
    Returns:
        int: Code de retour (0 pour succ√®s, autre pour erreur)
    """
    # Initialisation du gestionnaire de configuration
    config_manager = ConfigManager(args.config)
    
    # R√©cup√©ration des param√®tres pour la pr√©paration des donn√©es
    prepare_data_config = config_manager.get_section("prepare_data")
    files_config = config_manager.get_section("files")
    
    # Enregistrer le temps de d√©part pour calculer la dur√©e totale
    total_start_time = time.time()
    
    try:
        print(f"üì• T√©l√©chargement et pr√©paration des donn√©es...")
        print(f"  - Fichier QA: {args.out_text}")
        print(f"  - Fichier vecteurs: {args.out_vec}")
        print(f"  - Mod√®le: {args.model}")
        print(f"  - Taille de batch: {args.batch_size}")
        
        # 1. T√©l√©charger NQ‚Äëopen (train)
        print("‚è≥ T√©l√©chargement et pr√©paration des donn√©es Natural Questions (open)...")
        print("  Cela peut prendre plusieurs minutes, veuillez patienter...")
        download_start = time.time()
        
        # Import dynamique pour √©viter des d√©pendances inutiles
        try:
            from datasets import load_dataset
        except ImportError:
            print("‚ö†Ô∏è Biblioth√®que 'datasets' non install√©e. Installation en cours...")
            import subprocess
            subprocess.check_call([sys.executable, "-m", "pip", "install", "datasets"])
            from datasets import load_dataset
            
        ds = load_dataset("nq_open", split="train")
        download_time = time.time() - download_start
        print(f"‚úì T√©l√©chargement termin√© en {format_time(download_time)} - {len(ds):,} exemples charg√©s")
        
        # 2. Cr√©ation du fichier QA
        print(f"‚è≥ Cr√©ation du fichier QA {args.out_text}...")
        qa_start = time.time()
        os.makedirs(os.path.dirname(args.out_text) if os.path.dirname(args.out_text) else ".", exist_ok=True)
        
        lines = []
        with tqdm(total=len(ds), desc="Extraction Q&R") as pbar:
            for ex in ds:
                lines.append(f"{ex['question'].strip()} ||| {ex['answer'][0].strip()}")
                pbar.update(1)
        
        with open(args.out_text, "w", encoding="utf-8") as f_out:
            for i, ln in enumerate(tqdm(lines, desc="√âcriture vers fichier")):
                f_out.write(ln.replace("\n", " ") + "\n")
                if (i+1) % 10000 == 0:
                    print(f"  ‚Üí {i+1:,}/{len(lines):,} lignes √©crites ({((i+1)/len(lines))*100:.1f}%)")
        
        qa_time = time.time() - qa_start
        print(f"‚úì qa.txt √©crit : {len(lines):,} lignes ‚Üí {args.out_text} [termin√© en {format_time(qa_time)}]")
        
        # 3. Embeddings
        # V√©rifier si le fichier d'embeddings existe d√©j√†
        recalculate = True
        if os.path.exists(args.out_vec):
            if not args.force:
                # Demander √† l'utilisateur s'il veut recalculer les embeddings
                print(f"\nLe fichier d'embeddings {args.out_vec} existe d√©j√†.")
                while True:
                    response = input("Voulez-vous recalculer les embeddings ? (o/n): ").lower()
                    if response in ['o', 'oui', 'y', 'yes']:
                        recalculate = True
                        break
                    elif response in ['n', 'non', 'no']:
                        recalculate = False
                        break
                    else:
                        print("R√©ponse non reconnue. Veuillez r√©pondre par 'o' (oui) ou 'n' (non).")
            else:
                print(f"‚ö†Ô∏è Remplacement forc√© du fichier d'embeddings existant: {args.out_vec}")
        
        encode_time = 0
        if recalculate:
            # Calculer les embeddings
            print(f"‚è≥ Chargement du mod√®le {args.model}...")
            encode_start = time.time()
            
            try:
                from sentence_transformers import SentenceTransformer
            except ImportError:
                print("‚ö†Ô∏è Biblioth√®que 'sentence-transformers' non install√©e. Installation en cours...")
                import subprocess
                subprocess.check_call([sys.executable, "-m", "pip", "install", "sentence-transformers"])
                from sentence_transformers import SentenceTransformer
            
            model = SentenceTransformer(args.model)
            print(f"‚úì Mod√®le charg√© en {time.time() - encode_start:.2f}s")
            
            print(f"‚è≥ Encodage avec {args.model}...")
            total_batches = (len(lines) + args.batch_size - 1) // args.batch_size
            print(f"  ‚Üí Encodage de {len(lines):,} lignes en {total_batches:,} batches de taille {args.batch_size}...")
            
            vecs = []
            encoded_examples = 0
            encode_start_time = time.time()
            
            for i in tqdm(range(0, len(lines), args.batch_size), desc="Batches", total=total_batches):
                batch = [f"passage: {t}" for t in lines[i : i + args.batch_size]]
                batch_size = len(batch)
                encoded_examples += batch_size
                
                batch_start = time.time()
                batch_vecs = model.encode(batch, normalize_embeddings=True, show_progress_bar=False)
                batch_time = time.time() - batch_start
                
                current_time = time.time() - encode_start_time
                examples_per_sec = encoded_examples / current_time if current_time > 0 else 0
                remaining = (len(lines) - encoded_examples) / examples_per_sec if examples_per_sec > 0 else 0
                
                vecs.append(batch_vecs)
                
                if (i + args.batch_size) % (args.batch_size * 10) == 0 or (i + batch_size) >= len(lines):
                    print(f"  ‚Üí Batch {(i // args.batch_size) + 1}/{total_batches}: {batch_size} exemples en {batch_time:.2f}s "
                          f"({batch_size/batch_time:.1f} ex/s)")
                    print(f"  ‚Üí Progr√®s: {encoded_examples:,}/{len(lines):,} exemples "
                          f"({encoded_examples/len(lines)*100:.1f}%) - Vitesse: {examples_per_sec:.1f} ex/s")
                    print(f"  ‚Üí Temps √©coul√©: {format_time(current_time)} - Temps restant estim√©: {format_time(remaining)}")
            
            vecs = np.vstack(vecs)
            encode_time = time.time() - encode_start_time
            print(f"‚úì Encodage termin√© en {format_time(encode_time)} - {len(lines):,} exemples @ {len(lines)/encode_time:.1f} ex/s")
            
            # √âcrire les embeddings
            write_vectors(vecs, args.out_vec)
        else:
            print(f"Utilisation du fichier d'embeddings existant: {args.out_vec}")
        
        total_time = time.time() - total_start_time
        print("\n‚úì Traitement termin√©.")
        print(f"  - Configuration  : {args.config}")
        print(f"  - QA             : {args.out_text}")
        print(f"  - Embeddings     : {args.out_vec}")
        print(f"  - Mod√®le         : {args.model}")
        print(f"  - Batch size     : {args.batch_size}")
        print(f"  - Temps total    : {format_time(total_time)}")
        print(f"    ‚îú‚îÄ T√©l√©chargement : {format_time(download_time)} ({download_time/total_time*100:.1f}%)")
        print(f"    ‚îú‚îÄ Cr√©ation QA    : {format_time(qa_time)} ({qa_time/total_time*100:.1f}%)")
        if encode_time > 0:
            print(f"    ‚îî‚îÄ Encodage       : {format_time(encode_time)} ({encode_time/total_time*100:.1f}%)")
        else:
            print(f"    ‚îî‚îÄ Encodage       : (utilis√© fichier existant)")
        
        # Instructions pour les √©tapes suivantes
        print("\nPour construire l'arbre avec ces vecteurs :")
        print(f"  python -m k16.cli build {args.out_vec} --config {args.config}")
        
    except Exception as e:
        print(f"\n‚ùå Erreur: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0
```

### k16/utils/optimization.py

```python
"""
Module d'optimisations pour K16.
Configure et g√®re les optimisations num√©riques (SIMD, Numba JIT, etc.).
"""

import os
import numpy as np
import platform
from typing import Dict, Any, Optional, List

# V√©rifier si Numba est disponible
try:
    import numba
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False

def configure_simd():
    """
    Configure l'environnement pour utiliser les optimisations SIMD.
    Force l'utilisation des instructions SIMD dans NumPy.
    """
    # Forcer l'utilisation des instructions SIMD
    os.environ["OMP_NUM_THREADS"] = str(os.cpu_count())
    os.environ["MKL_NUM_THREADS"] = str(os.cpu_count())
    os.environ["OPENBLAS_NUM_THREADS"] = str(os.cpu_count())
    
    # Optimisations d'alignement m√©moire pour SIMD
    # Am√©liore significativement les performances des dot products
    try:
        # Alignement m√©moire pour vectorisation optimale
        np.config.add_option_enable_numpy_api(False)

        # Sur certaines plateformes, ces options peuvent √™tre disponibles
        try:
            # Pour Intel MKL
            np.config.add_option_mkl_vml_accuracy("high")
            np.config.add_option_mkl_enable_instructions("AVX2")
            # Si AVX-512 est disponible
            if "avx512" in platform.processor().lower():
                np.config.add_option_mkl_enable_instructions("AVX512")
        except Exception:
            pass
    except Exception:
        pass

def check_simd_support():
    """
    V√©rifier les extensions SIMD support√©es par la configuration NumPy.
    
    Returns:
        List[str]: Liste des extensions SIMD disponibles.
    """
    simd_extensions = []
    try:
        config_info = np.__config__.show()
        if "SIMD Extensions" in config_info:
            print("‚úì Extensions SIMD disponibles pour NumPy:")
            capture = False
            for line in config_info.split("\n"):
                if "SIMD Extensions" in line:
                    capture = True
                elif capture and line.startswith("  "):
                    ext = line.strip()
                    simd_extensions.append(ext)
                    print(f"  - {ext}")
                elif capture and not line.startswith("  "):
                    break
        return simd_extensions
    except Exception:
        print("‚úì NumPy configur√© pour utiliser les instructions SIMD disponibles")
        return simd_extensions

def check_numba_support():
    """
    V√©rifie si Numba est disponible et configur√© correctement.
    
    Returns:
        bool: True si Numba est disponible et configur√© correctement.
    """
    if NUMBA_AVAILABLE:
        print("‚úì Numba JIT est disponible pour l'optimisation")
        return True
    else:
        print("‚ö†Ô∏è Numba JIT n'est pas disponible. Les performances seront r√©duites.")
        return False

def optimize_functions():
    """
    Configure toutes les optimisations num√©riques pour K16.
    
    Returns:
        Dict[str, Any]: Un dictionnaire contenant les informations sur les optimisations.
    """
    # Configuration SIMD
    configure_simd()
    
    # V√©rifier le support
    simd_extensions = check_simd_support()
    numba_available = check_numba_support()
    
    return {
        "simd": {
            "available": len(simd_extensions) > 0,
            "extensions": simd_extensions
        },
        "numba": {
            "available": numba_available
        }
    }
```

### k16/utils/config.py

```python
"""
Module de gestion de la configuration pour K16.
Centralise le chargement et l'acc√®s √† la configuration YAML.
"""

import os
import yaml

# Chemin par d√©faut du fichier de configuration
DEFAULT_CONFIG_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "config.yaml")

# Configuration par d√©faut
DEFAULT_CONFIG = {
    "general": {
        "debug": False
    },
    "build_tree": {
        "max_depth": 6,
        "k": 16,
        "k_adaptive": False,  # Valeur fixe k=16 par d√©faut
        "k_min": 2,
        "k_max": 32,
        "max_leaf_size": 100,
        "max_data": 256,  # Multiple de 16 pour optimisation SIMD
        "max_workers": 8,
        "use_gpu": True,
        "prune_unused": False,  # Param√®tre obsol√®te
        # Le pruning des feuilles inutilis√©es est maintenant automatique et ne peut plus √™tre d√©sactiv√©
        "hnsw_batch_size": 1000,
        "grouping_batch_size": 5000,
        "hnsw_m": 16,
        "hnsw_ef_construction": 200,
    },
    "flat_tree": {
        "max_dims": 512,  # Multiple de 16 pour optimisation SIMD
        "reduction_method": "variance"  # ou "directional"
    },
    "search": {
        "k": 100,
        "queries": 100,
        "mode": "ram",
        "cache_size_mb": 500,
        "use_faiss": True
    },
    "prepare_data": {
        "model": "intfloat/multilingual-e5-large",
        "batch_size": 128,
        "normalize": True
    },
    "files": {
        "vectors_dir": ".",
        "trees_dir": ".",
        "default_qa": "qa.txt",
        "default_vectors": "vectors.bin",
        "default_tree": "tree.bin"
    }
}

class ConfigManager:
    """Gestionnaire de configuration pour K16."""
    
    def __init__(self, config_path=None):
        """
        Initialise le gestionnaire de configuration.
        
        Param√®tres :
            config_path: Chemin vers le fichier de configuration YAML.
                         Si None, utilise le chemin par d√©faut.
        """
        self.config_path = config_path or DEFAULT_CONFIG_PATH
        self.config = self.load_config()
        
    def load_config(self):
        """
        Charge la configuration depuis le fichier YAML.
        
        Retourne :
            Dict: La configuration charg√©e, ou la configuration par d√©faut en cas d'erreur.
        """
        try:
            with open(self.config_path, "r") as f:
                config = yaml.safe_load(f)
            
            # V√©rifier et compl√©ter la configuration
            self._ensure_complete_config(config)
            
            return config
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du chargement de la configuration: {str(e)}")
            print(f"‚ö†Ô∏è Utilisation des param√®tres par d√©faut")
            return DEFAULT_CONFIG.copy()
    
    def _ensure_complete_config(self, config):
        """
        S'assure que la configuration contient toutes les sections n√©cessaires.
        Compl√®te avec les valeurs par d√©faut si n√©cessaire.
        
        Param√®tres :
            config: Configuration √† v√©rifier et compl√©ter.
        """
        for section, default_values in DEFAULT_CONFIG.items():
            if section not in config:
                config[section] = default_values.copy()
            else:
                for key, value in default_values.items():
                    if key not in config[section]:
                        config[section][key] = value
    
    def get_section(self, section):
        """
        R√©cup√®re une section compl√®te de la configuration.
        
        Param√®tres :
            section: Nom de la section √† r√©cup√©rer.

        Retourne :
            Dict: La section demand√©e, ou un dictionnaire vide si la section n'existe pas.
        """
        return self.config.get(section, {})
    
    def get(self, section, key, default=None):
        """
        R√©cup√®re une valeur sp√©cifique de la configuration.
        
        Param√®tres :
            section: La section contenant la cl√©.
            key: La cl√© √† r√©cup√©rer.
            default: Valeur par d√©faut si la cl√© n'existe pas.

        Retourne :
            La valeur associ√©e √† la cl√©, ou la valeur par d√©faut si la cl√© n'existe pas.
        """
        section_data = self.get_section(section)
        return section_data.get(key, default)
    
    def get_file_path(self, file_key, default=None):
        """
        Construit le chemin complet vers un fichier sp√©cifi√© dans la configuration.
        
        Param√®tres :
            file_key: Cl√© du fichier dans la section 'files'.
            default: Valeur par d√©faut si la cl√© n'existe pas.

        Retourne :
            Le chemin complet vers le fichier.
        """
        files_section = self.get_section("files")
        
        if file_key.startswith("default_"):
            # Pour les fichiers par d√©faut, construire le chemin complet
            file_name = files_section.get(file_key, default)
            
            # D√©terminer le r√©pertoire appropri√©
            if "vectors" in file_key:
                dir_key = "vectors_dir"
            elif "tree" in file_key:
                dir_key = "trees_dir"
            else:
                dir_key = "vectors_dir"  # Par d√©faut
            
            dir_path = files_section.get(dir_key, ".")
            return os.path.join(dir_path, file_name)
        else:
            # Pour les autres cl√©s, retourner directement la valeur
            return files_section.get(file_key, default)
    
    def reload(self, config_path=None):
        """
        Recharge la configuration depuis un nouveau fichier.
        
        Param√®tres :
            config_path: Nouveau chemin de configuration. Si None, utilise le chemin actuel.
        """
        if config_path:
            self.config_path = config_path
        self.config = self.load_config()
        
    def __str__(self):
        """Repr√©sentation de la configuration pour le d√©bogage."""
        return f"Configuration charg√©e depuis: {self.config_path}"

# Fonction utilitaire pour charger une configuration
def load_config(config_path=None):
    """
    Fonction utilitaire pour charger rapidement une configuration.
    
    Param√®tres :
        config_path: Chemin vers le fichier de configuration YAML.
                     Si None, utilise le chemin par d√©faut.
    
    Retourne :
        ConfigManager: Instance du gestionnaire de configuration.
    """
    return ConfigManager(config_path)
```

### k16/utils/cli_search.py

```python
"""
Module pour la recherche interactive en ligne de commande.
Fournit une interface pour rechercher des questions similaires dans un terminal.
"""

import os
import time
import numpy as np
from typing import List, Dict, Any, Tuple
import argparse

from k16.utils.config import ConfigManager
from k16.io.reader import read_vectors, load_tree
from k16.search.searcher import search, Searcher

def format_results(results: List[Dict], timings: Dict[str, float]) -> str:
    """
    Formate les r√©sultats de recherche pour l'affichage en terminal.
    
    Args:
        results: Liste des r√©sultats de recherche
        timings: Dictionnaire des temps d'ex√©cution
        
    Returns:
        str: R√©sultats format√©s
    """
    output = []
    
    # Afficher les m√©triques
    output.append("\nüïí Temps:")
    output.append(f"  ‚Üí Encodage      : {timings['encode']*1000:.2f} ms")
    output.append(f"  ‚Üí Recherche arbre: {timings['tree_search']*1000:.2f} ms")
    output.append(f"  ‚Üí Filtrage      : {timings['filter']*1000:.2f} ms")
    search_only = timings['tree_search'] + timings['filter']
    total_time = timings['encode'] + search_only
    output.append(f"  ‚Üí Recherche totale: {search_only*1000:.2f} ms")
    output.append(f"  ‚Üí Temps total    : {total_time*1000:.2f} ms")
    
    # Afficher les r√©sultats
    output.append("\nüìã R√©sultats:")
    for i, result in enumerate(results, 1):
        output.append(f"\n{i}. {result['question']}")
        output.append(f"   ‚Üí {result['answer']}")
    
    return "\n".join(output)

def search_once(model, searcher, qa_lines: List[str], query: str, k: int = 10) -> Tuple[List[Dict], Dict[str, float]]:
    """
    Effectue une seule recherche.

    Args:
        model: Mod√®le d'embeddings
        searcher: Chercheur K16
        qa_lines: Lignes de questions-r√©ponses
        query: Question √† rechercher
        k: Nombre de r√©sultats √† retourner

    Returns:
        Tuple[List[Dict], Dict[str, float]]: R√©sultats et timings
    """
    # Encoder la requ√™te
    encode_start = time.time()
    query_vector = model.encode(f"query: {query}", normalize_embeddings=True)
    encode_time = time.time() - encode_start

    # Recherche avec l'arbre
    tree_search_start = time.time()
    tree_candidates = searcher.search_tree(query_vector)
    tree_search_time = time.time() - tree_search_start

    # Filtrer pour obtenir les k meilleurs
    filter_start = time.time()
    indices = searcher.filter_candidates(tree_candidates, query_vector, k)
    filter_time = time.time() - filter_start

    # R√©cup√©rer les r√©sultats
    results = []
    for idx in indices:
        if idx < len(qa_lines):
            parts = qa_lines[idx].strip().split(" ||| ")
            if len(parts) == 2:
                question, answer = parts
                results.append({
                    "question": question,
                    "answer": answer,
                    "index": idx
                })

    timings = {
        "encode": encode_time,
        "tree_search": tree_search_time,
        "filter": filter_time
    }

    return results, timings

def search_interactive(model, searcher, qa_lines: List[str], k: int = 10) -> Tuple[List[Dict], Dict[str, float]]:
    """
    Effectue une recherche interactive.

    Args:
        model: Mod√®le d'embeddings
        searcher: Chercheur K16
        qa_lines: Lignes de questions-r√©ponses
        k: Nombre de r√©sultats √† retourner

    Returns:
        Tuple[List[Dict], Dict[str, float]]: R√©sultats et timings
    """
    try:
        query = input("\nVotre question (q pour quitter): ")

        if query.lower() in ['q', 'quit', 'exit']:
            return None, None

        return search_once(model, searcher, qa_lines, query, k)

    except EOFError:
        print("\nMode non-interactif d√©tect√©. Voici un exemple de recherche:")
        example_query = "Qui a invent√© la th√©orie de la relativit√©?"
        print(f"Question: {example_query}")

        results, timings = search_once(model, searcher, qa_lines, example_query, k)
        return results, timings

def search_command(args: argparse.Namespace) -> int:
    """
    Commande pour effectuer une recherche interactive dans un terminal.

    Args:
        args: Arguments de ligne de commande

    Returns:
        int: Code de retour (0 pour succ√®s, autre pour erreur)
    """
    # Initialisation du gestionnaire de configuration
    config_manager = ConfigManager(args.config)

    # R√©cup√©ration des param√®tres
    search_config = config_manager.get_section("search")
    files_config = config_manager.get_section("files")
    build_config = config_manager.get_section("build_tree")
    prepare_config = config_manager.get_section("prepare_data")

    try:
        print(f"üîç Mode recherche K16 interactive...")
        print(f"  - Vecteurs: {args.vectors_file}")
        print(f"  - Arbre: {args.tree_file}")
        print(f"  - QA: {args.qa_file}")
        print(f"  - Mode: {args.mode}")
        print(f"  - K (nombre de r√©sultats): {args.k}")
        print(f"  - Type de recherche: {args.search_type}")
        print(f"  - Largeur de faisceau: {args.beam_width}")

        # V√©rifier que les fichiers existent
        if not os.path.exists(args.vectors_file):
            print(f"‚ùå Fichier de vecteurs introuvable: {args.vectors_file}")
            print(f"   Utilisez la commande 'getData' pour t√©l√©charger et pr√©parer les donn√©es:")
            print(f"   python -m k16.cli getData --config {args.config}")
            return 1

        if not os.path.exists(args.tree_file) and not os.path.exists(args.tree_file.replace(".bsp", ".flat.npy")):
            print(f"‚ùå Fichier d'arbre introuvable: {args.tree_file}")
            print(f"   Utilisez la commande 'build' pour construire l'arbre:")
            print(f"   python -m k16.cli build {args.vectors_file} --config {args.config}")
            return 1

        if not os.path.exists(args.qa_file):
            print(f"‚ùå Fichier QA introuvable: {args.qa_file}")
            print(f"   Utilisez la commande 'getData' pour t√©l√©charger et pr√©parer les donn√©es:")
            print(f"   python -m k16.cli getData --config {args.config}")
            return 1

        # Charger les vecteurs
        print(f"‚è≥ Chargement des vecteurs depuis {args.vectors_file}...")
        vectors_reader = read_vectors(
            file_path=args.vectors_file,
            mode=args.mode,
            cache_size_mb=args.cache_size
        )
        print(f"‚úì Vecteurs charg√©s: {len(vectors_reader):,} vecteurs de dimension {vectors_reader.d}")

        # Charger l'arbre
        print(f"‚è≥ Chargement de l'arbre depuis {args.tree_file}...")
        tree = load_tree(args.tree_file, mmap_tree=(args.mode == "mmap"))
        print(f"‚úì Arbre charg√©")

        # Charger les questions et r√©ponses
        print(f"‚è≥ Chargement des questions et r√©ponses depuis {args.qa_file}...")
        with open(args.qa_file, "r", encoding="utf-8") as f:
            qa_lines = f.readlines()
        print(f"‚úì {len(qa_lines):,} questions-r√©ponses charg√©es")

        # Cr√©er le chercheur
        from k16.search.searcher import Searcher
        searcher = Searcher(
            k16tree=tree,
            vectors_reader=vectors_reader,
            use_faiss=args.use_faiss,
            search_type=args.search_type,
            beam_width=args.beam_width
        )

        # Charger le mod√®le d'embeddings
        try:
            from sentence_transformers import SentenceTransformer
        except ImportError:
            print("‚ö†Ô∏è Biblioth√®que 'sentence-transformers' non install√©e. Installation en cours...")
            import subprocess, sys
            subprocess.check_call([sys.executable, "-m", "pip", "install", "sentence-transformers"])
            from sentence_transformers import SentenceTransformer

        print(f"‚è≥ Chargement du mod√®le {args.model}...")
        model = SentenceTransformer(args.model)
        print(f"‚úì Mod√®le charg√©")

        # Statistiques de l'arbre
        stats = tree.flat_tree.get_statistics()
        compression_stats = stats.get('compression', {})

        print("\nüìä Statistiques de l'arbre:")
        print(f"  ‚Üí N≈ìuds: {stats.get('n_nodes', '?'):,}")
        print(f"  ‚Üí Feuilles: {stats.get('n_leaves', '?'):,}")
        print(f"  ‚Üí Profondeur: {stats.get('max_depth', '?')}")

        # Lancement de l'interface interactive
        print(f"\nüí¨ Interface de recherche interactive K16")
        print(f"  ‚Üí Tapez votre question et appuyez sur Entr√©e")
        print(f"  ‚Üí Tapez 'q' pour quitter")

        # En mode interactif, continuer jusqu'√† ce que l'utilisateur quitte
        try:
            while True:
                results, timings = search_interactive(model, searcher, qa_lines, k=args.k)

                if results is None:
                    print("\nüëã Au revoir!")
                    break

                # Afficher les r√©sultats
                print(format_results(results, timings))
        except KeyboardInterrupt:
            print("\nüëã Recherche interrompue. Au revoir!")

    except Exception as e:
        print(f"\n‚ùå Erreur: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1

    return 0
```

### k16/utils/cli_api.py

```python
"""
Module pour exposer les fonctionnalit√©s de K16 via une API REST.
Utilise FastAPI pour exposer un endpoint de recherche.
"""

import os
import time
import argparse
import numpy as np
from typing import Dict, Any, List, Optional

from k16.utils.config import ConfigManager
from k16.io.reader import read_vectors, load_tree
from k16.search.searcher import Searcher

# Variables globales pour stocker les ressources
model = None
searcher = None
qa_lines = None
vectors_reader = None
tree = None

def load_resources(config_manager):
    """Charge les ressources n√©cessaires (mod√®le, vecteurs, arbre, qa) √† partir de la configuration."""
    global model, searcher, qa_lines, vectors_reader, tree
    
    # R√©cup√©ration des param√®tres
    search_config = config_manager.get_section("search")
    files_config = config_manager.get_section("files")
    prepare_config = config_manager.get_section("prepare_data")
    
    # Chemins des fichiers
    vectors_path = os.path.join(files_config["vectors_dir"], files_config["default_vectors"])
    tree_path = os.path.join(files_config["trees_dir"], files_config["default_tree"])
    qa_path = os.path.join(files_config["vectors_dir"], files_config.get("default_qa", "qa.txt"))
    
    # V√©rifier que les fichiers existent
    if not os.path.exists(vectors_path):
        print(f"‚ùå Fichier de vecteurs introuvable: {vectors_path}")
        return False
        
    if not os.path.exists(tree_path) and not os.path.exists(tree_path.replace(".bsp", ".flat.npy")):
        print(f"‚ùå Fichier d'arbre introuvable: {tree_path}")
        return False
        
    if not os.path.exists(qa_path):
        print(f"‚ùå Fichier QA introuvable: {qa_path}")
        return False
    
    # Charger les vecteurs
    print(f"‚è≥ Chargement des vecteurs depuis {vectors_path}...")
    vectors_reader = read_vectors(
        file_path=vectors_path,
        mode=search_config["mode"],
        cache_size_mb=search_config["cache_size_mb"]
    )
    print(f"‚úì Vecteurs charg√©s: {len(vectors_reader):,} vecteurs de dimension {vectors_reader.d}")
    
    # Charger l'arbre
    print(f"‚è≥ Chargement de l'arbre depuis {tree_path}...")
    tree = load_tree(tree_path, mmap_tree=(search_config["mode"] == "mmap"))
    print(f"‚úì Arbre charg√©")
    
    # Charger les questions et r√©ponses
    print(f"‚è≥ Chargement des questions et r√©ponses depuis {qa_path}...")
    with open(qa_path, "r", encoding="utf-8") as f:
        qa_lines = f.readlines()
    print(f"‚úì {len(qa_lines):,} questions-r√©ponses charg√©es")
    
    # Cr√©er le chercheur
    searcher = Searcher(
        k16tree=tree,
        vectors_reader=vectors_reader,
        use_faiss=search_config["use_faiss"],
        search_type=search_config["search_type"],
        beam_width=search_config.get("beam_width", 3)
    )
    
    # Charger le mod√®le d'embeddings
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        print("‚ö†Ô∏è Biblioth√®que 'sentence-transformers' non install√©e. Installation en cours...")
        import subprocess, sys
        subprocess.check_call([sys.executable, "-m", "pip", "install", "sentence-transformers"])
        from sentence_transformers import SentenceTransformer
    
    model_name = prepare_config.get("model", "intfloat/multilingual-e5-large")
    print(f"‚è≥ Chargement du mod√®le {model_name}...")
    model = SentenceTransformer(model_name)
    print(f"‚úì Mod√®le charg√©")
    
    # Statistiques de l'arbre
    stats = tree.flat_tree.get_statistics()
    print("\nüìä Statistiques de l'arbre:")
    print(f"  ‚Üí N≈ìuds: {stats.get('n_nodes', '?'):,}")
    print(f"  ‚Üí Feuilles: {stats.get('n_leaves', '?'):,}")
    print(f"  ‚Üí Profondeur: {stats.get('max_depth', '?')}")
    
    return True

def setup_app():
    """Configure et retourne l'application FastAPI."""
    # Importer les d√©pendances n√©cessaires
    try:
        from fastapi import FastAPI, HTTPException, Query
        from fastapi.middleware.cors import CORSMiddleware
        from pydantic import BaseModel
    except ImportError:
        print("‚ö†Ô∏è Les biblioth√®ques 'fastapi' et 'uvicorn' sont n√©cessaires pour l'API. Installation en cours...")
        import subprocess, sys
        subprocess.check_call([sys.executable, "-m", "pip", "install", "fastapi", "uvicorn[standard]"])
        from fastapi import FastAPI, HTTPException, Query
        from fastapi.middleware.cors import CORSMiddleware
        from pydantic import BaseModel

    # Mod√®les Pydantic pour la validation des donn√©es
    class SearchQuery(BaseModel):
        query: str
        k: Optional[int] = 10

    class SearchResult(BaseModel):
        question: str
        answer: str
        similarity: float
        index: int

    class SearchResponse(BaseModel):
        results: List[SearchResult]
        timings: Dict[str, float]
        stats: Dict[str, Any]
    
    # Application FastAPI
    app = FastAPI(
        title="K16 Search API",
        description="API for fast vector search using K16 hierarchical tree",
        version="1.0.0",
    )

    # Configuration CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # Ajuster en production!
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    @app.get("/")
    async def root():
        """Endpoint racine avec informations de base."""
        return {
            "name": "K16 Search API",
            "version": "1.0.0",
            "description": "API for fast vector search using K16 hierarchical tree",
            "endpoints": {
                "/search": "Search for similar questions",
                "/stats": "Get statistics about the loaded tree",
                "/health": "Check API health status"
            }
        }

    @app.get("/health")
    async def health():
        """V√©rification de l'√©tat de sant√© de l'API."""
        if not model or not searcher or not qa_lines:
            return {
                "status": "error",
                "message": "Resources not fully loaded"
            }
        return {
            "status": "ok",
            "loaded_resources": {
                "model": model.__class__.__name__,
                "vectors": len(vectors_reader) if vectors_reader else 0,
                "qa_lines": len(qa_lines) if qa_lines else 0,
                "tree_loaded": searcher is not None
            }
        }

    @app.get("/stats")
    async def get_stats():
        """R√©cup√®re les statistiques de l'arbre et des vecteurs."""
        if not searcher or not tree:
            raise HTTPException(status_code=503, detail="Tree not loaded")
        
        stats = tree.flat_tree.get_statistics()
        
        return {
            "tree": {
                "nodes": stats.get("n_nodes", 0),
                "leaves": stats.get("n_leaves", 0),
                "max_depth": stats.get("max_depth", 0),
                "compression": stats.get("compression", {})
            },
            "vectors": {
                "count": len(vectors_reader) if vectors_reader else 0,
                "dimensions": vectors_reader.d if vectors_reader else 0
            },
            "qa": {
                "count": len(qa_lines) if qa_lines else 0
            },
            "search": {
                "type": searcher.search_type,
                "beam_width": searcher.beam_width if searcher.search_type == "beam" else None
            }
        }

    @app.post("/search", response_model=SearchResponse)
    async def search(query_data: SearchQuery):
        """
        Recherche des questions similaires √† la requ√™te.
        
        Args:
            query_data: Contient la requ√™te et le nombre de r√©sultats souhait√©s
            
        Returns:
            Les r√©sultats de la recherche et les informations de timing
        """
        if not model or not searcher or not qa_lines:
            raise HTTPException(status_code=503, detail="Resources not fully loaded")
        
        # Encoder la requ√™te
        encode_start = time.time()
        query_vector = model.encode(f"query: {query_data.query}", normalize_embeddings=True)
        encode_time = time.time() - encode_start
        
        # Recherche avec l'arbre
        tree_search_start = time.time()
        tree_candidates = searcher.search_tree(query_vector)
        tree_search_time = time.time() - tree_search_start
        
        # Filtrer pour obtenir les k meilleurs
        filter_start = time.time()
        indices_with_scores = []
        
        # Utiliser VectorReader optimis√©
        top_indices = searcher.filter_candidates(tree_candidates, query_vector, query_data.k)
        top_vectors = vectors_reader[top_indices]
        scores = [np.dot(query_vector, top_vectors[i]) for i in range(len(top_indices))]
        
        # Trier par score d√©croissant
        sorted_pairs = sorted(zip(top_indices, scores), key=lambda x: x[1], reverse=True)
        indices_with_scores = [(idx, score) for idx, score in sorted_pairs]
        
        filter_time = time.time() - filter_start
        
        # R√©cup√©rer les r√©sultats
        results = []
        for idx, score in indices_with_scores:
            if idx < len(qa_lines):
                parts = qa_lines[idx].strip().split(" ||| ")
                if len(parts) == 2:
                    question, answer = parts
                    results.append(SearchResult(
                        question=question,
                        answer=answer,
                        similarity=float(score),
                        index=idx
                    ))
        
        # Calculer les statistiques et les temps
        timings = {
            "encode_ms": encode_time * 1000,
            "tree_search_ms": tree_search_time * 1000,
            "filter_ms": filter_time * 1000,
            "total_ms": (encode_time + tree_search_time + filter_time) * 1000
        }
        
        stats = {
            "candidates_count": len(tree_candidates)
        }
        
        return SearchResponse(
            results=results,
            timings=timings,
            stats=stats
        )
    
    return app

def api_command(args: argparse.Namespace) -> int:
    """
    Commande pour lancer l'API FastAPI.
    
    Args:
        args: Arguments de ligne de commande
        
    Returns:
        int: Code de retour (0 pour succ√®s, autre pour erreur)
    """
    # Initialisation du gestionnaire de configuration
    config_manager = ConfigManager(args.config)
    
    try:
        # V√©rifier que FastAPI est install√©
        try:
            import uvicorn
        except ImportError:
            print("‚ö†Ô∏è Les biblioth√®ques 'fastapi' et 'uvicorn' sont n√©cessaires pour l'API. Installation en cours...")
            import subprocess, sys
            subprocess.check_call([sys.executable, "-m", "pip", "install", "fastapi", "uvicorn[standard]"])
            import uvicorn
            
        # R√©cup√©rer les param√®tres API depuis le fichier config
        api_config = config_manager.get_section("api")

        # D√©finir les valeurs par d√©faut si la section API n'existe pas ou est incompl√®te
        default_host = "127.0.0.1"
        default_port = 8000
        default_reload = False
        
        host = args.host if args.host else api_config.get("host", default_host)
        port = args.port if args.port else api_config.get("port", default_port)
        reload = args.reload if args.reload is not None else api_config.get("reload", default_reload)
        
        print(f"üåê D√©marrage de l'API K16...")
        print(f"  - Configuration: {args.config}")
        print(f"  - Adresse: {host}:{port}")
        print(f"  - Rechargement auto: {'Activ√©' if reload else 'D√©sactiv√©'}")
        
        # Charger les ressources
        if not load_resources(config_manager):
            return 1
        
        # Configurer l'application et remplacer l'app globale
        global app
        app = setup_app()

        # D√©marrer l'API
        print(f"\nüöÄ D√©marrage du serveur API sur http://{host}:{port}")
        print(f"  ‚Üí Documentation Swagger: http://{host}:{port}/docs")
        print(f"  ‚Üí Documentation ReDoc: http://{host}:{port}/redoc")
        print(f"  ‚Üí Pour arr√™ter le serveur: CTRL+C\n")

        # D√©marrer Uvicorn
        if reload:
            # En mode reload, on doit utiliser un chemin d'importation
            import inspect
            module_path = inspect.getmodule(api_command).__name__
            uvicorn.run(
                f"{module_path}:app",
                host=host,
                port=port,
                reload=reload,
                log_level="info"
            )
        else:
            # En mode normal, on peut passer l'app directement
            uvicorn.run(
                app,
                host=host,
                port=port,
                log_level="info"
            )
        
    except Exception as e:
        print(f"\n‚ùå Erreur: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

# Variable globale pour l'application qui sera configur√©e lors de l'ex√©cution
# Ne pas initialiser ici pour √©viter les erreurs d'importation si FastAPI n'est pas install√©
app = None
```

### k16/utils/cli_build.py

```python
"""
Module pour la construction d'arbres K16.
Fournit des fonctions pour construire des arbres optimis√©s.
"""

import os
import time
import datetime
import argparse
from typing import Any

from k16.utils.config import ConfigManager
from k16.builder.builder import build_optimized_tree

def format_time(seconds: float) -> str:
    """Formate le temps en heures, minutes, secondes."""
    return str(datetime.timedelta(seconds=int(seconds)))

def build_command(args: argparse.Namespace) -> int:
    """
    Commande pour construire un arbre optimis√©.
    
    Args:
        args: Arguments de ligne de commande
        
    Returns:
        int: Code de retour (0 pour succ√®s, autre pour erreur)
    """
    # Initialisation du gestionnaire de configuration
    config_manager = ConfigManager(args.config)

    # R√©cup√©ration des param√®tres pour la construction de l'arbre
    build_config = config_manager.get_section("build_tree")
    files_config = config_manager.get_section("files")
    flat_tree_config = config_manager.get_section("flat_tree")

    # Enregistrer le temps de d√©part pour calculer la dur√©e totale
    total_start_time = time.time()

    try:
        print(f"üöÄ Construction d'un arbre K16 optimis√©...")
        print(f"  - Vecteurs: {args.vectors_file}")
        print(f"  - Sortie: {args.tree_file}")
        print(f"  - Profondeur max: {args.max_depth}")
        print(f"  - Taille max feuille: {args.max_leaf_size}")
        print(f"  - Max data: {args.max_data}")
        print(f"  - Dimensions r√©duites: {args.max_dims}")
        print(f"  - HNSW: {'Activ√©' if args.hnsw else 'D√©sactiv√©'}")
        print(f"  - K adaptatif: {'Activ√©' if args.k_adaptive else 'D√©sactiv√©'}")

        # Construction de l'arbre optimis√© en une seule fonction
        flat_tree = build_optimized_tree(
            vectors=args.vectors_file,
            output_file=args.tree_file,
            max_depth=args.max_depth,
            max_leaf_size=args.max_leaf_size,
            max_data=args.max_data,
            max_dims=args.max_dims,
            use_hnsw=args.hnsw,
            k=args.k,
            k_adaptive=args.k_adaptive,
            verbose=True
        )

        total_time = time.time() - total_start_time
        print(f"\n‚úì Construction de l'arbre optimis√© termin√©e en {format_time(total_time)}")

        # Instructions pour l'utilisation du script de test
        print("\nPour tester la recherche dans cet arbre :")
        print(f"  python -m k16.cli test {args.vectors_file} {args.tree_file} --k 100")
        print(f"  ou, en utilisant la configuration :")
        print(f"  python -m k16.cli test --config {args.config}")
        print(f"\nPour faire des recherches interactives :")
        print(f"  python -m k16.cli search --config {args.config}")

    except Exception as e:
        print(f"\n‚ùå Erreur: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1

    return 0
```

### k16/builder/clustering.py

```python
"""
Module de clustering pour K16.
Impl√©mente les algorithmes de clustering utilis√©s pour construire l'arbre K16.
Optimis√© pour les instructions SIMD AVX2/AVX-512.
"""

import numpy as np
import time
import platform
import os
from typing import Tuple, List, Dict, Any, Optional, Union
import multiprocessing
from joblib import Parallel, delayed
from tqdm.auto import tqdm

import faiss

try:
    from kneed import KneeLocator
    KNEEDLE_AVAILABLE = True
except ImportError:
    KNEEDLE_AVAILABLE = False
    print("‚ö†Ô∏è Module kneed n'est pas install√©. La m√©thode du coude utilisera une approche simplifi√©e.")

from k16.core.tree import TreeNode
from k16.utils.optimization import configure_simd

# D√©tection des capacit√©s SIMD
def optimize_for_simd():
    """Configure l'environnement pour maximiser l'utilisation des instructions SIMD."""
    # Appliquer les optimisations de base
    configure_simd()
    
    # D√©tection des capacit√©s SIMD
    cpu_info = platform.processor().lower()
    has_avx2 = "avx2" in cpu_info
    has_avx512 = "avx512" in cpu_info

    # Afficher les informations sur les optimisations SIMD d√©tect√©es
    if has_avx512:
        print("‚úì Optimisation pour AVX-512 (512 bits)")
    elif has_avx2:
        print("‚úì Optimisation pour AVX2 (256 bits)")
    else:
        print("‚úì Optimisation pour SSE (128 bits)")

    # V√©rifier si on a un nombre de clusters optimal pour SIMD
    if platform.machine() in ('x86_64', 'AMD64', 'x86'):
        print("‚úì Architecture x86_64 d√©tect√©e - k=16 est optimal pour SIMD")

    return has_avx2, has_avx512

# Appliquer les optimisations SIMD au d√©marrage
HAS_AVX2, HAS_AVX512 = optimize_for_simd()

def kmeans_faiss(vectors: np.ndarray, k: int, gpu: bool = False, niter: int = 5, remove_empty: bool = True) -> Tuple[np.ndarray, np.ndarray]:
    """
    K-means sph√©rique optimis√© pour les embeddings normalis√©s.
    Utilise la similarit√© cosinus (produit scalaire) au lieu de la distance euclidienne.

    Args:
        vectors: Les vecteurs √† clusteriser (shape: [n, d])
        k: Nombre de clusters
        gpu: Utiliser le GPU si disponible
        niter: Nombre d'it√©rations
        remove_empty: Si True, supprime les clusters vides et ajuste k en cons√©quence

    Returns:
        Tuple[np.ndarray, np.ndarray]: (centroids, labels)
    """
    # S'assurer que les vecteurs sont en float32
    vectors = vectors.astype(np.float32)

    # V√©rifier si les vecteurs sont normalis√©s
    norms = np.linalg.norm(vectors, axis=1)
    if not np.allclose(norms, 1.0, atol=1e-3):
        # Normaliser si n√©cessaire
        vectors = vectors / norms[:, np.newaxis]

    # Dimension des vecteurs
    d = vectors.shape[1]
    n = vectors.shape[0]

    # Ne pas cr√©er plus de clusters que de points
    if k > n:
        k = n

    # Utiliser l'impl√©mentation K-means optimis√©e de FAISS
    try:
        # Cr√©er l'objet K-means FAISS
        kmeans = faiss.Kmeans(d, k, niter=niter, verbose=False, spherical=True, gpu=gpu)

        # Entra√Æner le mod√®le
        kmeans.train(vectors)

        # Obtenir les centro√Ødes
        centroids = kmeans.centroids.copy()

        # Assigner les points aux clusters
        index = faiss.IndexFlatIP(d)
        if gpu:
            try:
                res = faiss.StandardGpuResources()
                index = faiss.GpuIndexFlatIP(res, d)
            except:
                pass  # Fallback au CPU

        index.add(centroids)
        _, labels = index.search(vectors, 1)
        labels = labels.reshape(-1)

        # G√©rer les clusters vides si demand√©
        if remove_empty:
            cluster_sizes = np.bincount(labels, minlength=k)
            empty_clusters = np.where(cluster_sizes == 0)[0]

            if len(empty_clusters) > 0 and len(empty_clusters) < k - 1:
                print(f"  ‚ö†Ô∏è Suppression de {len(empty_clusters)} clusters vides...")

                # Garder seulement les centro√Ødes des clusters non vides
                valid_indices = np.where(cluster_sizes > 0)[0]
                new_k = len(valid_indices)

                if new_k >= 2:
                    # Filtrer les centro√Ødes
                    final_centroids = centroids[valid_indices]

                    # R√©-√©tiqueter les points
                    index.reset()
                    index.add(final_centroids)
                    _, final_labels = index.search(vectors, 1)
                    final_labels = final_labels.reshape(-1)

                    print(f"  ‚úì K r√©duit de {k} √† {new_k} pour √©viter les clusters vides")
                    return final_centroids, final_labels

        return centroids, labels

    except Exception as e:
        # Fallback vers sklearn si FAISS √©choue
        print(f"‚ö†Ô∏è FAISS K-means failed ({e}), using sklearn fallback")
        from sklearn.cluster import KMeans
        from sklearn.preprocessing import normalize

        # Normaliser les vecteurs
        vectors_norm = normalize(vectors, norm='l2')

        # K-means avec sklearn
        kmeans = KMeans(n_clusters=k, max_iter=niter, n_init=1, random_state=42)
        labels = kmeans.fit_predict(vectors_norm)

        # V√©rifier les clusters vides
        if remove_empty:
            cluster_sizes = np.bincount(labels, minlength=k)
            empty_clusters = np.where(cluster_sizes == 0)[0]

            if len(empty_clusters) > 0 and len(empty_clusters) < k-1:
                print(f"  ‚ö†Ô∏è Suppression de {len(empty_clusters)} clusters vides (version CPU)...")

                # Garder seulement les centro√Ødes des clusters non vides
                valid_indices = np.where(cluster_sizes > 0)[0]
                new_k = len(valid_indices)

                # Relabel les points
                new_labels = np.zeros_like(labels)
                for new_idx, old_idx in enumerate(valid_indices):
                    new_labels[labels == old_idx] = new_idx

                # Extraire les centro√Ødes valides
                valid_centroids = kmeans.cluster_centers_[valid_indices]

                # Normaliser les centro√Ødes
                centroids = normalize(valid_centroids, norm='l2')
                labels = new_labels

                print(f"  ‚úì K r√©duit de {k} √† {new_k} pour √©viter les clusters vides")
                return centroids, labels

        # Normaliser les centro√Ødes
        centroids = normalize(kmeans.cluster_centers_, norm='l2')

    return centroids, labels


def spherical_kmeans_plusplus(vectors: np.ndarray, k: int) -> np.ndarray:
    """
    Initialisation K-means++ adapt√©e pour le clustering sph√©rique.
    S√©lectionne intelligemment les centro√Ødes initiaux.

    Args:
        vectors: Vecteurs normalis√©s
        k: Nombre de clusters

    Returns:
        np.ndarray: Centro√Ødes initiaux
    """
    n, d = vectors.shape

    # Si k = n, simplement retourner tous les vecteurs comme centro√Ødes
    if k >= n:
        # Copier les vecteurs pour √©viter les modifications accidentelles
        return vectors.copy()

    centroids = np.zeros((k, d), dtype=np.float32)

    # Choisir le premier centro√Øde al√©atoirement
    centroids[0] = vectors[np.random.randint(0, n)]

    # Utiliser FAISS pour acc√©l√©rer
    index = faiss.IndexFlatIP(d)

    for i in range(1, k):
        # Calculer les distances au centro√Øde le plus proche
        index.reset()
        index.add(centroids[:i])
        similarities, _ = index.search(vectors, 1)

        # Convertir similarit√©s en distances angulaires
        similarities = np.clip(similarities.reshape(-1), -1, 1)
        angular_distances = np.arccos(similarities)

        # Probabilit√© proportionnelle au carr√© de la distance
        probabilities = angular_distances ** 2
        # √âviter la division par z√©ro si toutes les distances sont nulles
        if np.sum(probabilities) > 0:
            probabilities = probabilities / np.sum(probabilities)
        else:
            # Si toutes les distances sont nulles, choisir uniform√©ment
            probabilities = np.ones(n) / n

        # S√©lectionner le prochain centro√Øde
        cumulative_probs = np.cumsum(probabilities)
        r = np.random.rand()

        # S'assurer que l'indice est valide
        if cumulative_probs[-1] > 0:
            idx = np.searchsorted(cumulative_probs, r)
            if idx >= n:  # Pour √©viter les erreurs d'indexation
                idx = n - 1
        else:
            # En cas de probl√®me avec les probabilit√©s, choisir un indice al√©atoire
            idx = np.random.randint(0, n)

        centroids[i] = vectors[idx]

    return centroids

def find_optimal_k(vectors: np.ndarray, k_min: int = 2, k_max: int = 32, gpu: bool = False) -> int:
    """
    Trouve le nombre optimal de clusters k en utilisant la m√©thode du coude.
    Adapt√© pour le k-means sph√©rique avec distance angulaire.

    Args:
        vectors: Les vecteurs √† clusteriser
        k_min: Nombre minimum de clusters √† tester
        k_max: Nombre maximum de clusters √† tester
        gpu: Utiliser le GPU si disponible

    Returns:
        int: Nombre optimal de clusters k
    """
    # Limiter k_max par rapport au nombre de vecteurs
    k_max = min(k_max, len(vectors) // 2) if len(vectors) > 2 else k_min
    k_range = range(k_min, k_max + 1)

    # Si nous avons trop peu de vecteurs, retourner le minimum
    if len(k_range) <= 1:
        return k_min

    inertias = []

    # Calculer l'inertie pour diff√©rentes valeurs de k
    for k in tqdm(k_range, desc="Recherche du k optimal"):
        centroids, labels = kmeans_faiss(vectors, k, gpu=gpu, niter=5)  # Moins d'it√©rations pour la recherche

        # Calculer l'inertie adapt√©e au clustering sph√©rique
        inertia = 0
        for i in range(k):
            cluster_vectors = vectors[labels == i]
            if len(cluster_vectors) > 0:
                # Calculer les similarit√©s avec le centro√Øde
                similarities = np.dot(cluster_vectors, centroids[i])
                # Convertir en distances angulaires et sommer
                angular_distances = np.arccos(np.clip(similarities, -1, 1))
                inertia += np.sum(angular_distances ** 2)

        inertias.append(inertia)

    # Utiliser KneeLocator si disponible, sinon utiliser une approche simplifi√©e
    if KNEEDLE_AVAILABLE:
        try:
            kneedle = KneeLocator(
                list(k_range), inertias,
                curve="convex", direction="decreasing", S=1.0
            )

            # Si un coude est trouv√©, l'utiliser
            if kneedle.knee is not None:
                optimal_k = kneedle.knee
                return optimal_k
        except:
            pass  # Si KneeLocator √©choue, utiliser l'approche simplifi√©e

    # Approche simplifi√©e: analyse des diff√©rences
    inertia_diffs = [inertias[i-1] - inertias[i] for i in range(1, len(inertias))]

    # Normaliser les diff√©rences
    if max(inertia_diffs) > 0:
        normalized_diffs = [d / max(inertia_diffs) for d in inertia_diffs]

        # Trouver l'indice o√π la diff√©rence normalis√©e devient inf√©rieure √† 0.2 (heuristique)
        for i, diff in enumerate(normalized_diffs):
            if diff < 0.2:
                return k_range[i]

    # Par d√©faut, retourner la m√©diane des k test√©s
    return (k_min + k_max) // 2

def select_closest_vectors(vectors: np.ndarray, all_indices: List[int], centroid: np.ndarray, max_data: int) -> np.ndarray:
    """
    S√©lectionne les max_data vecteurs les plus proches du centro√Øde.

    Args:
        vectors: Tous les vecteurs disponibles
        all_indices: Indices de tous les vecteurs disponibles
        centroid: Centro√Øde de r√©f√©rence
        max_data: Nombre maximum de vecteurs √† s√©lectionner

    Returns:
        np.ndarray: Tableau des indices des max_data vecteurs les plus proches du centro√Øde
    """
    # Calculer la similarit√© entre chaque vecteur et le centro√Øde
    similarities = np.dot(vectors, centroid)

    # Trier les indices par similarit√© d√©croissante
    sorted_indices = np.argsort(-similarities)

    # S√©lectionner les max_data indices les plus proches
    selected_count = min(max_data, len(sorted_indices))
    selected_local_indices = sorted_indices[:selected_count]

    # Convertir les indices locaux en indices globaux
    if isinstance(all_indices, list):
        all_indices = np.array(all_indices, dtype=np.int32)
    
    selected_global_indices = all_indices[selected_local_indices]

    return selected_global_indices

def select_closest_natural_vectors(leaf_vectors: np.ndarray, leaf_indices: Union[List[int], np.ndarray],
                                  global_vectors: np.ndarray, global_indices: Union[List[int], np.ndarray],
                                  centroid: np.ndarray, max_data: int, sibling_indices: Optional[List[np.ndarray]] = None) -> np.ndarray:
    """
    S√©lectionne d'abord les vecteurs qui tombent naturellement dans la feuille,
    puis compl√®te avec les fr√®res/s≈ìurs, et enfin avec les plus proches au global si n√©cessaire.

    Args:
        leaf_vectors: Vecteurs qui tombent naturellement dans cette feuille
        leaf_indices: Indices des vecteurs qui tombent naturellement dans cette feuille
        global_vectors: Tous les vecteurs disponibles
        global_indices: Tous les indices disponibles
        centroid: Centro√Øde de r√©f√©rence
        max_data: Nombre maximum de vecteurs √† s√©lectionner
        sibling_indices: Liste des indices des fr√®res/s≈ìurs (autres clusters du m√™me niveau)

    Returns:
        np.ndarray: Tableau des indices des max_data vecteurs les plus proches du centro√Øde
    """
    # Conversion en arrays numpy si n√©cessaire
    if isinstance(leaf_indices, list):
        leaf_indices = np.array(leaf_indices, dtype=np.int32)
    if isinstance(global_indices, list):
        global_indices = np.array(global_indices, dtype=np.int32)
    
    # D'abord, prioriser les vecteurs qui tombent naturellement dans la feuille
    leaf_indices_set = set(leaf_indices.tolist())

    # Si la feuille contient d√©j√† assez de vecteurs, pas besoin d'en chercher d'autres
    if len(leaf_indices) >= max_data:
        # Calculer la similarit√© entre les vecteurs de la feuille et le centro√Øde
        similarities = np.dot(leaf_vectors, centroid)
        # Trier les indices par similarit√© d√©croissante
        sorted_local_indices = np.argsort(-similarities)
        # Prendre les max_data plus proches
        selected_count = min(max_data, len(sorted_local_indices))
        return leaf_indices[sorted_local_indices[:selected_count]]

    # Sinon, compl√©ter avec les fr√®res/s≈ìurs d'abord, puis au global
    result = leaf_indices.tolist()  # Commencer avec les vecteurs naturels
    used_indices = set(leaf_indices.tolist())  # Maintenir un set de tous les indices utilis√©s

    # √âtape 1: Ajouter des vecteurs des fr√®res/s≈ìurs si disponibles
    if sibling_indices is not None:
        sibling_candidates = []
        for sibling_idx_array in sibling_indices:
            if len(sibling_idx_array) > 0:
                sibling_candidates.extend(sibling_idx_array.tolist())

        if sibling_candidates:
            # Calculer la similarit√© des vecteurs fr√®res/s≈ìurs avec le centro√Øde
            sibling_candidates = np.array(sibling_candidates, dtype=np.int32)
            sibling_vectors = global_vectors[sibling_candidates]
            sibling_similarities = np.dot(sibling_vectors, centroid)
            sibling_sorted_indices = np.argsort(-sibling_similarities)

            # Ajouter les meilleurs fr√®res/s≈ìurs qui ne sont pas d√©j√† utilis√©s
            for idx in sibling_sorted_indices:
                global_idx = sibling_candidates[idx]
                if global_idx not in used_indices:
                    result.append(global_idx)
                    used_indices.add(global_idx)  # Mettre √† jour le set d'exclusion
                    if len(result) >= max_data:
                        return np.array(result, dtype=np.int32)

    # √âtape 2: Si encore pas assez, compl√©ter avec les plus proches au global
    # Calculer la similarit√© entre tous les vecteurs et le centro√Øde
    similarities = np.dot(global_vectors, centroid)

    # Trier les indices par similarit√© d√©croissante
    sorted_indices = np.argsort(-similarities)

    # Ajouter les vecteurs les plus proches qui ne sont pas d√©j√† utilis√©s
    for i in sorted_indices:
        global_idx = global_indices[i]
        if global_idx not in used_indices:
            result.append(global_idx)
            used_indices.add(global_idx)  # Mettre √† jour le set d'exclusion
            if len(result) >= max_data:
                break

    return np.array(result, dtype=np.int32)

def build_tree_node(vectors: np.ndarray, current_indices: Union[List[int], np.ndarray], global_vectors: np.ndarray, global_indices: Union[List[int], np.ndarray],
                   level: int, max_depth: int, k_adaptive: bool, k: int, k_min: int, k_max: int,
                   max_leaf_size: int, max_data: int, use_gpu: bool = False, sibling_indices: Optional[List[np.ndarray]] = None) -> TreeNode:
    """
    Construit un n≈ìud de l'arbre optimis√©.

    Args:
        vectors: Les vecteurs assign√©s √† ce n≈ìud
        current_indices: Les indices des vecteurs assign√©s √† ce n≈ìud
        global_vectors: Tous les vecteurs de la collection compl√®te
        global_indices: Tous les indices de la collection compl√®te
        level: Niveau actuel dans l'arbre
        max_depth: Profondeur maximale de l'arbre
        k_adaptive: Utiliser la m√©thode du coude pour d√©terminer k
        k: Nombre fixe de clusters (si k_adaptive=False)
        k_min, k_max: Limites pour k adaptatif
        max_leaf_size: Taille maximale d'une feuille pour l'arr√™t de la subdivision
        max_data: MAX_DATA - Nombre de vecteurs les plus proches √† stocker dans chaque feuille
        use_gpu: Utiliser le GPU pour K-means

    Returns:
        TreeNode: Un n≈ìud de l'arbre
    """
    # Conversion en arrays numpy si n√©cessaire
    if isinstance(global_indices, list):
        global_indices = np.array(global_indices, dtype=np.int32)
    if isinstance(current_indices, list):
        current_indices = np.array(current_indices, dtype=np.int32)

    # V√©rifier si nous avons assez de vecteurs pour continuer
    if len(vectors) == 0:
        # Cr√©er un n≈ìud vide avec un centro√Øde al√©atoire normalis√©
        random_vector = np.random.randn(global_vectors.shape[1])
        random_centroid = random_vector / np.linalg.norm(random_vector)
        empty_node = TreeNode(level=level)
        empty_node.centroid = random_centroid
        return empty_node

    # Calculer le centro√Øde de ce n≈ìud (normalis√© pour le produit scalaire)
    centroid = np.mean(vectors, axis=0)
    # Normaliser le centro√Øde pour la similarit√© cosinus
    norm = np.linalg.norm(centroid)
    if norm > 0:  # √âviter la division par z√©ro
        centroid = centroid / norm
    else:
        # Si le centro√Øde est un vecteur nul (cas rare), cr√©er un vecteur al√©atoire normalis√©
        random_vector = np.random.randn(vectors.shape[1])
        centroid = random_vector / np.linalg.norm(random_vector)

    # Cr√©er une feuille si les conditions sont remplies
    if level >= max_depth or len(vectors) <= max_leaf_size:
        leaf = TreeNode(level=level)
        leaf.centroid = centroid

        # S√©lectionner les max_data vecteurs les plus proches du centro√Øde
        # Priorit√© aux vecteurs qui tombent naturellement dans cette feuille, puis fr√®res/s≈ìurs, puis global
        leaf.set_indices(select_closest_natural_vectors(
            vectors, current_indices,
            global_vectors, global_indices,
            centroid, max_data, sibling_indices
        ))

        return leaf

    # D√©terminer le nombre de clusters √† utiliser
    if k_adaptive:
        if len(vectors) > 10000:
            sample_size = min(10000, len(vectors))
            sample_indices = np.random.choice(len(vectors), sample_size, replace=False)
            sample_vectors = vectors[sample_indices]
            used_k = find_optimal_k(sample_vectors, k_min, k_max, gpu=use_gpu)
        else:
            used_k = find_optimal_k(vectors, k_min, k_max, gpu=use_gpu)
    else:
        used_k = min(k, len(vectors))  # Ne pas cr√©er plus de clusters que de vecteurs

    # Afficher la progression seulement pour les premiers niveaux
    if level <= 2:
        print(f"  ‚Üí Niveau {level+1}: Clustering K-means avec k={used_k} sur {len(vectors):,} vecteurs...")

    # Appliquer K-means pour obtenir k clusters avec FAISS
    # En activant la suppression des clusters vides
    centroids, labels = kmeans_faiss(vectors, used_k, gpu=use_gpu, remove_empty=True)

    # Ajuster used_k au nombre r√©el de clusters apr√®s suppression des vides
    used_k = len(centroids)

    # V√©rifier que les centro√Ødes sont normalis√©s
    for i in range(len(centroids)):
        norm = np.linalg.norm(centroids[i])
        if norm > 0:  # √âviter la division par z√©ro
            centroids[i] = centroids[i] / norm

    # Cr√©er le noeud actuel avec son centro√Øde
    node = TreeNode(level=level)
    node.centroid = centroid

    # Cr√©er des groupes pour chaque cluster en utilisant numpy
    cluster_indices = [[] for _ in range(used_k)]
    cluster_vectors = [[] for _ in range(used_k)]

    # Assigner chaque vecteur √† son cluster
    for i, cluster_idx in enumerate(labels):
        cluster_vectors[cluster_idx].append(vectors[i])
        cluster_indices[cluster_idx].append(current_indices[i])
    
    # Convertir les listes en tableaux numpy
    cluster_vectors_np = [np.array(vec_list) if vec_list else np.array([]) for vec_list in cluster_vectors]
    cluster_indices_np = [np.array(idx_list, dtype=np.int32) if idx_list else np.array([], dtype=np.int32) for idx_list in cluster_indices]

    # Construire les enfants pour chaque cluster (r√©cursivement)
    for i in range(used_k):
        if len(cluster_vectors_np[i]) > 0:
            # Pr√©parer les indices des fr√®res/s≈ìurs (tous les autres clusters du m√™me niveau)
            sibling_indices_list = [cluster_indices_np[j] for j in range(used_k) if j != i and len(cluster_indices_np[j]) > 0]

            # Pour les n≈ìuds internes, passer les vecteurs et indices actuels
            child = build_tree_node(
                cluster_vectors_np[i],
                cluster_indices_np[i],
                global_vectors,
                global_indices,
                level + 1,
                max_depth,
                k_adaptive,
                k,
                k_min,
                k_max,
                max_leaf_size,
                max_data,
                use_gpu,
                sibling_indices_list
            )
            node.add_child(child)
        else:
            # Si un cluster est vide, cr√©er une feuille bas√©e sur le centro√Øde
            empty_centroid = centroids[i]
            empty_node = TreeNode(level=level + 1)
            empty_node.centroid = empty_centroid

            # S√©lectionner les vecteurs globaux les plus proches de ce centro√Øde vide
            similarities = np.dot(global_vectors, empty_centroid)
            sorted_indices = np.argsort(-similarities)

            # S√©lectionner jusqu'√† max_data indices les plus proches
            selected_count = min(max_data, len(sorted_indices))
            empty_node.set_indices(global_indices[sorted_indices[:selected_count]])

            node.add_child(empty_node)

    # Bien que node.centroids soit d√©j√† mis √† jour dans add_child,
    # on s'assure qu'il contient tous les centro√Ødes align√©s avec children
    node.set_children_centroids()
    
    return node

def process_cluster(i: int, vectors: np.ndarray, cluster_indices: np.ndarray, cluster_vectors: np.ndarray,
                   global_vectors: np.ndarray, global_indices: np.ndarray, level: int, max_depth: int,
                   k_adaptive: bool, k: int, k_min: int, k_max: int, max_leaf_size: int, max_data: int,
                   centroids: np.ndarray, use_gpu: bool = False) -> Tuple[int, Optional[TreeNode]]:
    """
    Fonction pour traiter un cluster en parall√®le.

    Args:
        i: Indice du cluster
        vectors: Tous les vecteurs
        cluster_indices: Indices des vecteurs dans ce cluster
        cluster_vectors: Vecteurs dans ce cluster
        global_vectors: Tous les vecteurs de la collection compl√®te
        global_indices: Tous les indices de la collection compl√®te
        level: Niveau actuel dans l'arbre
        max_depth: Profondeur maximale de l'arbre
        k_adaptive: Utiliser la m√©thode du coude pour d√©terminer k
        k: Nombre fixe de clusters (si k_adaptive=False)
        k_min, k_max: Limites pour k adaptatif
        max_leaf_size: Taille maximale d'une feuille pour l'arr√™t de la subdivision
        max_data: MAX_DATA - Nombre de vecteurs les plus proches √† stocker dans chaque feuille
        centroids: Les centro√Ødes de tous les clusters
        use_gpu: Utiliser le GPU pour K-means

    Returns:
        Tuple[int, Optional[TreeNode]]: (indice du cluster, n≈ìud construit)
    """
    if len(cluster_vectors) > 0:
        return i, build_tree_node(
            cluster_vectors,
            cluster_indices,
            global_vectors,
            global_indices,
            level,
            max_depth,
            k_adaptive,
            k,
            k_min,
            k_max,
            max_leaf_size,
            max_data,
            use_gpu
        )
    else:
        # Si un cluster est vide, cr√©er une feuille bas√©e sur le centro√Øde
        # Normaliser le centro√Øde pour la similarit√© cosinus
        centroid = centroids[i]
        centroid = centroid / np.linalg.norm(centroid)

        # S√©lectionner les vecteurs globaux les plus proches de ce centro√Øde
        similarities = np.dot(global_vectors, centroid)
        sorted_indices = np.argsort(-similarities)

        # Cr√©er une feuille avec le centro√Øde normalis√©
        leaf = TreeNode(level=level)
        leaf.centroid = centroid

        # S√©lectionner jusqu'√† max_data indices les plus proches
        selected_count = min(max_data, len(sorted_indices))
        leaf.set_indices(global_indices[sorted_indices[:selected_count]])

        return i, leaf

def build_tree(vectors: np.ndarray, max_depth: int = 6, k: int = 16, k_adaptive: bool = False, 
              k_min: int = 2, k_max: int = 32, max_leaf_size: int = 100, max_data: int = 3000, 
              max_workers: Optional[int] = None, use_gpu: bool = False) -> TreeNode:
    """
    Construit un arbre √† profondeur variable avec 'k' branches √† chaque niveau,
    et pr√©-calcule les max_data indices les plus proches pour chaque feuille.

    Args:
        vectors: Les vecteurs √† indexer
        max_depth: Profondeur maximale de l'arbre
        k: Nombre de branches par n≈ìud (si k_adaptive=False)
        k_adaptive: Utiliser la m√©thode du coude pour d√©terminer k automatiquement
        k_min: Nombre minimum de clusters pour k adaptatif
        k_max: Nombre maximum de clusters pour k adaptatif
        max_leaf_size: Taille maximale d'une feuille pour l'arr√™t de la subdivision
        max_data: MAX_DATA - Nombre de vecteurs les plus proches √† stocker dans chaque feuille
        max_workers: Nombre maximum de processus parall√®les
        use_gpu: Utiliser le GPU pour K-means si disponible

    Returns:
        TreeNode: Racine de l'arbre
    """
    if max_workers is None:
        max_workers = multiprocessing.cpu_count()

    # V√©rifier si le GPU est vraiment disponible
    gpu_available = False
    if use_gpu:
        try:
            res = faiss.StandardGpuResources()
            # Test simple de cr√©ation d'un index GPU
            test_index = faiss.GpuIndexFlatIP(res, 128)
            gpu_available = True
            print("‚úì GPU FAISS d√©tect√© et disponible")
        except (AttributeError, RuntimeError) as e:
            print(f"‚ö†Ô∏è GPU demand√© mais non disponible: {e}. Utilisation du CPU √† la place.")
            use_gpu = False

    gpu_str = "GPU" if use_gpu else "CPU"
    print(f"‚è≥ Construction de l'arbre optimis√© avec FAISS sur {gpu_str} (max {max_depth} niveaux) avec {'k adaptatif' if k_adaptive else f'k={k}'},")
    print(f"   max_leaf_size={max_leaf_size}, MAX_DATA={max_data}, max_workers={max_workers}")

    start_time = time.time()

    # Statistiques sur les feuilles
    leaf_sizes = []
    leaf_depths = []

    # Initialiser les indices globaux comme un tableau numpy
    global_indices = np.arange(len(vectors), dtype=np.int32)

    # Utiliser k=16 par d√©faut avec ajustement intelligent
    # pour les petits datasets ou les niveaux profonds
    vectors_count = len(vectors)
    min_points_per_cluster = max(1, max_leaf_size // 2)

    # D√©termine le nombre optimal de clusters (k)
    if k_adaptive:
        # Mode adaptatif traditionnel (coude)
        if vectors_count > 10000:
            sample_size = 10000
            sample_indices = np.random.choice(vectors_count, sample_size, replace=False)
            sample_vectors = vectors[sample_indices]
            root_k = find_optimal_k(sample_vectors, k_min, k_max, gpu=use_gpu)
        else:
            root_k = find_optimal_k(vectors, k_min, k_max, gpu=use_gpu)
    else:
        # Ajustement automatique - k est r√©duit si le nombre de points est trop petit
        # k=16 est optimal pour SIMD, mais pas si on a trop peu de points
        if vectors_count < k * min_points_per_cluster:
            # Calculer un k qui garantit au moins min_points_per_cluster points par cluster
            adjusted_k = max(2, min(k, vectors_count // min_points_per_cluster))
            if adjusted_k != k:
                print(f"  ‚Üí Ajustement automatique: k={k} ‚Üí k={adjusted_k} (car {vectors_count} points < {k}√ó{min_points_per_cluster})")
            root_k = adjusted_k
        else:
            # Utiliser k=16 standard (optimal pour SIMD)
            root_k = k

    # Appliquer K-means pour le premier niveau avec FAISS
    print(f"  ‚Üí Niveau 1: Clustering K-means avec k={root_k} sur {len(vectors):,} vecteurs...")
    centroids, labels = kmeans_faiss(vectors, root_k, gpu=use_gpu, remove_empty=True)

    # Ajuster root_k apr√®s suppression des clusters vides
    root_k = len(centroids)

    # Cr√©er le noeud racine (avec centro√Øde normalis√©)
    root_centroid = np.mean(centroids, axis=0)
    root_centroid = root_centroid / np.linalg.norm(root_centroid)
    root = TreeNode(level=0)
    root.centroid = root_centroid

    # Cr√©er des groupes pour chaque cluster en utilisant numpy
    # Utiliser une liste de tableaux pour am√©liorer l'efficacit√©
    cluster_vectors = [[] for _ in range(root_k)]
    cluster_indices = [[] for _ in range(root_k)]

    # Assigner chaque vecteur √† son cluster
    for i, cluster_idx in enumerate(labels):
        cluster_vectors[cluster_idx].append(vectors[i])
        cluster_indices[cluster_idx].append(global_indices[i])
    
    # Convertir en tableaux numpy
    cluster_vectors_np = [np.array(vec_list) if vec_list else np.array([]) for vec_list in cluster_vectors]
    cluster_indices_np = [np.array(idx_list, dtype=np.int32) if idx_list else np.array([], dtype=np.int32) for idx_list in cluster_indices]

    # Construction parall√®le des sous-arbres de premier niveau
    print(f"  ‚Üí Construction parall√®le des sous-arbres avec {max_workers} workers...")

    # Cr√©er la liste des arguments pour chaque cluster
    tasks = []
    for i in range(root_k):
        tasks.append((
            i,
            vectors,  # Pas utilis√© directement dans le traitement des clusters
            cluster_indices_np[i] if i < len(cluster_indices_np) else np.array([], dtype=np.int32),
            cluster_vectors_np[i] if i < len(cluster_vectors_np) else np.array([]),
            vectors,  # Vecteurs globaux pour pr√©-calculer les indices proches
            global_indices,  # Indices globaux pour pr√©-calculer les indices proches
            1,  # Niveau des sous-arbres (1)
            max_depth,
            k_adaptive,
            k,
            k_min,
            k_max,
            max_leaf_size,
            max_data,
            centroids,  # Passer les centro√Ødes pour traiter les clusters vides
            use_gpu
        ))

    # Ex√©cuter les t√¢ches en parall√®le avec joblib
    results = Parallel(n_jobs=max_workers, verbose=10)(
        delayed(process_cluster)(*task) for task in tasks
    )

    # Reconstruire l'arbre √† partir des r√©sultats
    for i, child in results:
        if child is not None:
            root.add_child(child)
        else:
            # Ce cas ne devrait pas arriver avec le nouveau process_cluster
            # mais gardons-le par pr√©caution
            empty_node = TreeNode(level=1)
            empty_node.centroid = centroids[i]
            root.add_child(empty_node)
    
    # S'assurer que root.centroids est bien initialis√©
    root.set_children_centroids()
    
    # Collecter des statistiques sur les feuilles
    def collect_leaf_stats(node, level=0):
        if not node.children:  # C'est une feuille
            leaf_sizes.append(len(node.indices))
            leaf_depths.append(level)
        else:
            for child in node.children:
                collect_leaf_stats(child, level + 1)
    
    collect_leaf_stats(root)
    
    elapsed = time.time() - start_time

    # Afficher des statistiques sur les feuilles
    if leaf_sizes:
        avg_size = sum(leaf_sizes) / len(leaf_sizes)
        min_size = min(leaf_sizes)
        max_size = max(leaf_sizes)
        avg_depth = sum(leaf_depths) / len(leaf_depths)
        min_depth = min(leaf_depths)
        max_depth_seen = max(leaf_depths)

        print(f"‚úì Construction de l'arbre optimis√© termin√©e en {elapsed:.2f}s")
        print(f"  ‚Üí Statistiques des feuilles:")
        print(f"     - Nombre de feuilles   : {len(leaf_sizes):,}")
        print(f"     - Taille moyenne       : {avg_size:.1f} vecteurs")
        print(f"     - Taille minimale      : {min_size} vecteurs")
        print(f"     - Taille maximale      : {max_size} vecteurs")
        print(f"     - Profondeur moyenne   : {avg_depth:.1f}")
        print(f"     - Profondeur minimale  : {min_depth}")
        print(f"     - Profondeur maximale  : {max_depth_seen}")
    else:
        print(f"‚úì Construction de l'arbre optimis√© termin√©e en {elapsed:.2f}s")

    return root
```

### k16/builder/builder.py

```python
"""
Constructeur simplifi√© d'arbres K16 optimis√©s.
Une seule fonction qui fait tout: construction, r√©duction, HNSW, optimisation.
"""

import os
from typing import Optional, Union, Dict, Any
import numpy as np

from k16.utils.config import ConfigManager
from k16.core.tree import K16Tree, TreeNode
from k16.core.flat_tree import TreeFlat
from k16.io.reader import VectorReader
from k16.utils.optimization import configure_simd

def build_optimized_tree(
    vectors: Union[np.ndarray, str],
    output_file: str,
    config: Optional[Dict[str, Any]] = None,
    max_depth: Optional[int] = None,
    max_leaf_size: Optional[int] = None,
    max_data: Optional[int] = None,
    max_dims: Optional[int] = None,
    use_hnsw: Optional[bool] = None,
    k: Optional[int] = None,
    k_adaptive: Optional[bool] = None,
    verbose: bool = True
) -> TreeFlat:
    """
    Construit un arbre K16 optimis√© en une seule fonction.

    Cette fonction fait tout:
    1. Chargement des vecteurs si n√©cessaire
    2. Construction de l'arbre hi√©rarchique avec k=16 par d√©faut
    3. R√©duction dimensionnelle
    4. Am√©lioration HNSW si activ√©e
    5. Optimisation des centro√Ødes (√©conomie ~56% d'espace)
    6. Sauvegarde de l'arbre

    Args:
        vectors: Soit un tableau numpy de vecteurs, soit un chemin vers un fichier de vecteurs
        output_file: Chemin vers le fichier de sortie pour sauvegarder l'arbre
        config: Configuration personnalis√©e (facultatif, sinon utilise config.yaml)
        max_depth: Profondeur maximale de l'arbre (facultatif)
        max_leaf_size: Taille maximale des feuilles (facultatif)
        max_data: Nombre de vecteurs √† stocker par feuille (facultatif)
        max_dims: Nombre de dimensions √† conserver (facultatif)
        use_hnsw: Activer l'am√©lioration HNSW (facultatif)
        k: Nombre de branches par n≈ìud (16 par d√©faut)
        k_adaptive: Utiliser k adaptatif (d√©sactiv√© par d√©faut)
        verbose: Afficher les messages de progression

    Returns:
        L'arbre optimis√© construit
    """
    # S'assurer que les optimisations SIMD sont configur√©es
    configure_simd()
    
    # 1. Charger la configuration
    if config is None:
        config_manager = ConfigManager()
        build_config = config_manager.get_section("build_tree")
        flat_config = config_manager.get_section("flat_tree")
    else:
        build_config = config.get("build_tree", {})
        flat_config = config.get("flat_tree", {})
    
    # 2. Utiliser les param√®tres explicites ou les valeurs de configuration
    max_depth = max_depth if max_depth is not None else build_config.get("max_depth", 32)
    max_leaf_size = max_leaf_size if max_leaf_size is not None else build_config.get("max_leaf_size", 50)
    max_data = max_data if max_data is not None else build_config.get("max_data", 256)  # Optimis√© pour SIMD (multiple de 16)
    max_dims = max_dims if max_dims is not None else flat_config.get("max_dims", 512)  # Optimis√© pour SIMD (multiple de 16)
    k_adaptive = k_adaptive if k_adaptive is not None else build_config.get("k_adaptive", False)
    use_hnsw = use_hnsw if use_hnsw is not None else build_config.get("use_hnsw_improvement", True)

    # Autres param√®tres de configuration
    k = k if k is not None else build_config.get("k", 16)  # Valeur fixe k=16 par d√©faut
    k_min = build_config.get("k_min", 2)
    k_max = build_config.get("k_max", 32)
    max_workers = build_config.get("max_workers", None)
    reduction_method = flat_config.get("reduction_method", "directional")
    
    # 3. Pr√©parer les vecteurs
    if isinstance(vectors, str):
        if verbose:
            print(f"‚è≥ Chargement des vecteurs depuis {vectors}...")
        vectors_reader = VectorReader(file_path=vectors, mode="ram")
        if verbose:
            print(f"‚úì {len(vectors_reader):,} vecteurs charg√©s (dim {vectors_reader.d})")
        vectors_data = vectors_reader.vectors
    else:
        vectors_data = vectors
        vectors_reader = None
    
    # 4. Cr√©er le r√©pertoire de sortie si n√©cessaire
    os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)
    
    # 5. Construction de l'arbre hi√©rarchique avec k-means
    if verbose:
        print(f"‚è≥ Construction de l'arbre K16 avec max_depth={max_depth}, "
              f"max_leaf_size={max_leaf_size}, max_data={max_data}, "
              f"{'k adaptatif' if k_adaptive else f'k={k}'}")
              
    # Importation locale pour √©viter les d√©pendances circulaires
    from k16.builder.clustering import build_tree as clustering_build_tree
    
    tree = clustering_build_tree(
        vectors=vectors_data,
        max_depth=max_depth,
        k=k,
        k_adaptive=k_adaptive,
        k_min=k_min,
        k_max=k_max,
        max_leaf_size=max_leaf_size,
        max_data=max_data,
        max_workers=max_workers
    )
    
    # 6. Calcul de la r√©duction dimensionnelle et conversion en K16Tree
    k16tree = K16Tree(tree)
    
    if verbose:
        print(f"‚è≥ Calcul de la r√©duction dimensionnelle (max_dims={max_dims}, method={reduction_method})...")
    
    k16tree.compute_dimensional_reduction(max_dims=max_dims, method=reduction_method)
    
    # 7. Conversion en structure plate
    if verbose:
        print(f"‚è≥ Conversion en structure plate...")
    
    flat_tree = TreeFlat.from_tree(k16tree)
    k16tree.flat_tree = flat_tree
    
    # 8. Am√©lioration optionnelle avec HNSW
    if use_hnsw:
        if verbose:
            print(f"‚è≥ Am√©lioration avec HNSW...")
        
        if vectors_reader is None:
            # Cr√©er un lecteur de vecteurs si on n'en a pas d√©j√† un
            vectors_reader = VectorReader(vectors=vectors_data)
            
        k16tree = k16tree.improve_with_hnsw(vectors_reader, max_data)
        flat_tree = k16tree.flat_tree
        
        if verbose:
            print(f"‚úì Am√©lioration HNSW termin√©e!")
    elif verbose:
        print(f"‚ÑπÔ∏è Am√©lioration HNSW d√©sactiv√©e")
    
    # 9. Sauvegarde de l'arbre (avec optimisation automatique)
    if output_file:
        # Assurer l'extension correcte
        if not (output_file.endswith(".flat.npy") or output_file.endswith(".flat")):
            output_file = os.path.splitext(output_file)[0] + ".flat.npy"
            
        if verbose:
            print(f"‚è≥ Sauvegarde de l'arbre vers {output_file}...")
            
        flat_tree.save(output_file)  # L'optimisation est faite automatiquement
        
        if verbose:
            print(f"‚úì Arbre optimis√© sauvegard√© dans {output_file}")
    
    # 10. Nettoyer
    if vectors_reader is not None and isinstance(vectors, str):
        vectors_reader.close()
    
    return flat_tree

def build_tree(
    vectors: np.ndarray,
    max_depth: int = 6, 
    max_leaf_size: int = 100, 
    max_data: int = 256,
    max_dims: int = 512,
    use_hnsw: bool = True,
    k: int = 16,
    k_adaptive: bool = False,
    output_file: Optional[str] = None
) -> K16Tree:
    """
    Fonction simplifi√©e pour construire un arbre K16.
    
    Args:
        vectors: Vecteurs √† indexer
        max_depth: Profondeur maximale de l'arbre
        max_leaf_size: Taille maximale d'une feuille pour l'arr√™t de la subdivision
        max_data: Nombre de vecteurs √† stocker par feuille
        max_dims: Nombre de dimensions √† conserver pour la r√©duction dimensionnelle
        use_hnsw: Activer l'am√©lioration HNSW
        k: Nombre de branches par n≈ìud
        k_adaptive: Utiliser k adaptatif
        output_file: Chemin vers le fichier de sortie (facultatif)
        
    Returns:
        K16Tree: Arbre K16 construit
    """
    # Utiliser build_optimized_tree avec des param√®tres par d√©faut
    flat_tree = build_optimized_tree(
        vectors=vectors,
        output_file=output_file or "",  # Cha√Æne vide pour ne pas sauvegarder si None
        max_depth=max_depth,
        max_leaf_size=max_leaf_size,
        max_data=max_data,
        max_dims=max_dims,
        use_hnsw=use_hnsw,
        k=k,
        k_adaptive=k_adaptive,
        verbose=True
    )
    
    # Cr√©er un K16Tree √† partir de l'arbre plat
    tree = K16Tree(None)
    tree.flat_tree = flat_tree
    
    return tree
```

### k16/core/tree.py

```python
"""
Module de structures d'arbre pour K16.
D√©finit les diff√©rentes classes pour repr√©senter l'arbre K16.
"""

import numpy as np
import time
from collections import defaultdict
from typing import List, Any, Optional, Dict, Union, TYPE_CHECKING

# Imports relatifs pour le nouveau package
if TYPE_CHECKING:
    from k16.io.reader import VectorReader

class TreeNode:
    """
    Noeud de base pour l'arbre K16.
    Repr√©sente un n≈ìud dans l'arbre hi√©rarchique construit pour la recherche de vecteurs similaires.
    """
    
    def __init__(self, centroid: Optional[np.ndarray] = None, level: int = 0):
        """
        Initialise un n≈ìud d'arbre K16.
        
        Args:
            centroid: Vecteur centro√Øde repr√©sentant ce n≈ìud (optionnel)
            level: Niveau du n≈ìud dans l'arbre (0 = racine)
        """
        self.centroid = centroid  # Centro√Øde du n≈ìud
        self.level = level        # Niveau dans l'arbre
        self.children = []        # Pour les n≈ìuds internes: liste des noeuds enfants
        self.centroids = None     # Pour les n≈ìuds internes: tableau numpy des centro√Ødes des enfants (align√© avec children)
        self.indices = np.array([], dtype=np.int32)  # Pour les feuilles: tableau numpy des MAX_DATA indices les plus proches
        
        # NOUVEAU : R√©duction dimensionnelle pour les ENFANTS
        self.children_top_dims = None  # np.ndarray[max_dims] - indices des dimensions importantes pour les enfants
        self.children_d_head = None    # int - nombre de dimensions conserv√©es pour les enfants
        
    def is_leaf(self) -> bool:
        """
        V√©rifie si ce n≈ìud est une feuille.
        
        Returns:
            bool: True si le n≈ìud est une feuille (pas d'enfants), False sinon
        """
        return len(self.children) == 0
    
    def add_child(self, child: 'TreeNode') -> None:
        """
        Ajoute un n≈ìud enfant √† ce n≈ìud.
        
        Args:
            child: N≈ìud enfant √† ajouter
        """
        self.children.append(child)
        
        # Mettre √† jour le tableau des centro√Ødes
        if self.centroids is None:
            self.centroids = np.array([child.centroid])
        else:
            self.centroids = np.vstack([self.centroids, child.centroid])
    
    def set_children_centroids(self) -> None:
        """
        Construit le tableau des centro√Ødes √† partir des centro√Ødes des enfants.
        √Ä appeler apr√®s avoir ajout√© tous les enfants pour garantir l'alignement.
        """
        if not self.children:
            return
            
        centroids = [child.centroid for child in self.children]
        self.centroids = np.array(centroids)
    
    def set_indices(self, indices: Union[List[int], np.ndarray]) -> None:
        """
        D√©finit les indices associ√©s √† ce n≈ìud (pour les feuilles).
        
        Args:
            indices: Liste ou tableau des indices des vecteurs les plus proches du centro√Øde
        """
        if isinstance(indices, list):
            self.indices = np.array(indices, dtype=np.int32)
        else:
            self.indices = indices.astype(np.int32)
    
    def get_size(self) -> int:
        """
        Calcule la taille du sous-arbre enracin√© √† ce n≈ìud.
        
        Returns:
            int: Nombre total de n≈ìuds dans le sous-arbre
        """
        size = 1  # Ce n≈ìud
        for child in self.children:
            size += child.get_size()
        return size
    
    def __str__(self) -> str:
        """Repr√©sentation sous forme de cha√Æne pour le d√©bogage."""
        if self.is_leaf():
            return f"Leaf(level={self.level}, indices={len(self.indices)})"
        else:
            return f"Node(level={self.level}, children={len(self.children)})"

class K16Tree:
    """
    Classe principale pour l'arbre K16.
    G√®re un arbre hi√©rarchique optimis√© pour la recherche rapide de vecteurs similaires.
    Peut utiliser une structure plate optimis√©e pour des performances maximales.
    """

    def __init__(self, root: Optional[TreeNode] = None):
        """
        Initialise un arbre K16.

        Args:
            root: N≈ìud racine de l'arbre (optionnel)
        """
        self.root = root
        self.stats = {}  # Statistiques sur l'arbre
        self.flat_tree = None  # Version optimis√©e en structure plate

    def set_root(self, root: TreeNode) -> None:
        """
        D√©finit le n≈ìud racine de l'arbre.

        Args:
            root: N≈ìud racine √† d√©finir
        """
        self.root = root
        # R√©initialiser l'arbre plat car il n'est plus valide
        self.flat_tree = None
    
    def get_height(self) -> int:
        """
        Calcule la hauteur de l'arbre (profondeur maximale des feuilles).
        
        Returns:
            int: Hauteur de l'arbre
        """
        if not self.root:
            return 0
        
        def get_node_height(node: TreeNode) -> int:
            if node.is_leaf():
                return node.level
            return max(get_node_height(child) for child in node.children)
        
        return get_node_height(self.root)
    
    def get_leaf_count(self) -> int:
        """
        Compte le nombre de feuilles dans l'arbre.
        
        Returns:
            int: Nombre de feuilles
        """
        if not self.root:
            return 0
        
        def count_leaves(node: TreeNode) -> int:
            if node.is_leaf():
                return 1
            return sum(count_leaves(child) for child in node.children)
        
        return count_leaves(self.root)
    
    def get_node_count(self) -> int:
        """
        Compte le nombre total de n≈ìuds dans l'arbre.
        
        Returns:
            int: Nombre total de n≈ìuds
        """
        if not self.root:
            return 0
        return self.root.get_size()
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Calcule diverses statistiques sur l'arbre.
        
        Returns:
            Dict: Dictionnaire de statistiques
        """
        if not self.root:
            return {"error": "Arbre vide"}
        
        stats = {
            "node_count": 0,
            "leaf_count": 0,
            "max_depth": 0,
            "min_leaf_depth": float('inf'),
            "avg_leaf_depth": 0,
            "leaf_depths": [],
            "branching_factors": {},
            "avg_branching_factor": 0,
            "leaf_sizes": [],
            "avg_leaf_size": 0,
            "min_leaf_size": float('inf'),
            "max_leaf_size": 0,
            "total_indices": 0,
            "nodes_with_reduction": 0,
            "avg_dimensions_kept": 0
        }
        
        def traverse(node: TreeNode) -> None:
            nonlocal stats
            
            stats["node_count"] += 1
            
            # Mise √† jour de la profondeur maximale
            stats["max_depth"] = max(stats["max_depth"], node.level)
            
            # Statistiques sur la r√©duction dimensionnelle
            if node.children_top_dims is not None and node.children_d_head is not None:
                stats["nodes_with_reduction"] += 1
            
            if node.is_leaf():
                # C'est une feuille
                stats["leaf_count"] += 1
                stats["min_leaf_depth"] = min(stats["min_leaf_depth"], node.level)
                stats["leaf_depths"].append(node.level)
                
                # Statistiques des indices
                leaf_size = len(node.indices)
                stats["leaf_sizes"].append(leaf_size)
                stats["min_leaf_size"] = min(stats["min_leaf_size"], leaf_size)
                stats["max_leaf_size"] = max(stats["max_leaf_size"], leaf_size)
                stats["total_indices"] += leaf_size
            else:
                # C'est un n≈ìud interne
                branch_count = len(node.children)
                if branch_count in stats["branching_factors"]:
                    stats["branching_factors"][branch_count] += 1
                else:
                    stats["branching_factors"][branch_count] = 1
                
                # Analyser les enfants
                for child in node.children:
                    traverse(child)
        
        # Parcourir l'arbre
        traverse(self.root)
        
        # Calculer les moyennes
        if stats["leaf_count"] > 0:
            stats["avg_leaf_depth"] = sum(stats["leaf_depths"]) / stats["leaf_count"]
            stats["avg_leaf_size"] = sum(stats["leaf_sizes"]) / stats["leaf_count"]
        
        internal_nodes = stats["node_count"] - stats["leaf_count"]
        if internal_nodes > 0:
            total_branches = sum(k * v for k, v in stats["branching_factors"].items())
            stats["avg_branching_factor"] = total_branches / internal_nodes
        
        if stats["nodes_with_reduction"] > 0:
            # Calculer la moyenne des dimensions conserv√©es
            total_dims = 0
            nodes_counted = 0
            
            def count_dims(node: TreeNode) -> None:
                nonlocal total_dims, nodes_counted
                if node.children_d_head is not None:
                    total_dims += node.children_d_head
                    nodes_counted += 1
                for child in node.children:
                    count_dims(child)
            
            count_dims(self.root)
            if nodes_counted > 0:
                stats["avg_dimensions_kept"] = total_dims / nodes_counted
        
        # Conserver les statistiques dans l'instance
        self.stats = stats
        
        return stats
    
    def compute_dimensional_reduction(self, max_dims: int = None, method: str = "variance") -> None:
        """
        Calcule la r√©duction de dimension locale pour chaque n≈ìud interne.
        Chaque n≈ìud identifie les dimensions qui s√©parent le mieux ses enfants.

        Args:
            max_dims: Nombre de dimensions √† conserver. Si None, utilise le param√®tre
                      de configuration max_dims ou une valeur par d√©faut.
            method: M√©thode de r√©duction de dimension: "variance", "directional"
        """
        if not self.root:
            raise ValueError("Arbre vide - impossible de calculer la r√©duction de dimension")

        dims = int(self.root.centroid.shape[0])

        # Retrieve config parameters if max_dims not provided
        if max_dims is None:
            try:
                from k16.utils.config import ConfigManager
                cm = ConfigManager()
                max_dims = cm.get("flat_tree", "max_dims", 128)  # Valeur par d√©faut: 128 dimensions
            except Exception:
                max_dims = 128  # Valeur par d√©faut si la config n'est pas disponible

        # S'assurer que max_dims est valide
        max_dims = min(max(1, max_dims), dims)  # Entre 1 et dims

        print(f"‚è≥ Calcul de la r√©duction de dimension par n≈ìud (max_dims={max_dims}, method={method})...")

        nodes_processed = 0
        start_time = time.time()

        def compute_node_reduction(node: TreeNode) -> None:
            nonlocal nodes_processed

            # Ne pas calculer de r√©duction pour les feuilles
            if node.is_leaf():
                return

            # S'assurer que le n≈ìud a des enfants avec centro√Ødes
            if not node.children or len(node.children) < 2:
                # Pas assez d'enfants pour faire une r√©duction significative
                # Utiliser toutes les dimensions
                node.children_top_dims = np.arange(min(max_dims, dims), dtype=np.int32)
                node.children_d_head = min(max_dims, dims)
                nodes_processed += 1
                for child in node.children:
                    compute_node_reduction(child)
                return

            # Extraire les centro√Ødes des enfants
            child_centroids = np.vstack([child.centroid for child in node.children])

            if method == "directional":
                # Analyse directionnelle : focus sur les dimensions qui s√©parent le mieux les clusters
                # Calculer la s√©paration de chaque dimension
                dim_separation = np.zeros(dims)

                # Pour chaque paire de centro√Ødes, calculer leur s√©paration
                n_centroids = child_centroids.shape[0]
                for i in range(n_centroids):
                    for j in range(i+1, n_centroids):  # Uniquement les paires uniques
                        # Calcul de la s√©paration par dimension
                        dimension_diff = np.abs(child_centroids[i] - child_centroids[j])
                        dim_separation += dimension_diff

                # Trier les dimensions par s√©paration d√©croissante
                dims_sorted = np.argsort(-dim_separation)
                node.children_top_dims = np.ascontiguousarray(dims_sorted[:max_dims], dtype=np.int32)
                node.children_d_head = max_dims

            else:
                # M√©thode par variance (par d√©faut)
                var = np.var(child_centroids, axis=0)
                dims_sorted = np.argsort(-var)
                node.children_top_dims = np.ascontiguousarray(dims_sorted[:max_dims], dtype=np.int32)
                node.children_d_head = max_dims

            nodes_processed += 1

            # Afficher la progression pour les gros arbres
            if nodes_processed % 1000 == 0:
                elapsed = time.time() - start_time
                print(f"  ‚Üí {nodes_processed} n≈ìuds trait√©s ({elapsed:.1f}s)")

            # Appliquer r√©cursivement aux enfants
            for child in node.children:
                compute_node_reduction(child)

        # Commencer le calcul depuis la racine
        compute_node_reduction(self.root)

        elapsed = time.time() - start_time
        method_name = "Analyse directionnelle" if method == "directional" else ("variance")
        print(f"‚úì R√©duction de dimension par n≈ìud calcul√©e ({method_name})")
        print(f"  ‚Üí {nodes_processed} n≈ìuds trait√©s en {elapsed:.2f}s")
        print(f"  ‚Üí {dims} ‚Üí {max_dims} dimensions par n≈ìud")
    
    def __str__(self) -> str:
        """Repr√©sentation sous forme de cha√Æne pour le d√©bogage."""
        if not self.root and not self.flat_tree:
            return "Empty Tree"

        stats = self.get_statistics()

        if self.flat_tree:
            return (f"K16Tree(nodes={stats['node_count']}, "
                    f"leaves={stats['leaf_count']}, "
                    f"height={stats['max_depth']}, "
                    f"nodes_with_reduction={stats['nodes_with_reduction']}, "
                    f"optimized=True)")
        else:
            return (f"K16Tree(nodes={stats['node_count']}, "
                    f"leaves={stats['leaf_count']}, "
                    f"height={stats['max_depth']}, "
                    f"nodes_with_reduction={stats['nodes_with_reduction']})")
    
    def save_statistics(self, file_path: str) -> None:
        """
        Sauvegarde les statistiques de l'arbre dans un fichier texte.
        
        Args:
            file_path: Chemin du fichier de sortie
        """
        stats = self.get_statistics()
        
        with open(file_path, "w") as f:
            f.write("STATISTIQUES DE L'ARBRE K16\n")
            f.write("===========================\n\n")
            
            f.write("Structure g√©n√©rale\n")
            f.write("-----------------\n")
            f.write(f"Nombre total de n≈ìuds : {stats['node_count']}\n")
            f.write(f"Nombre de feuilles    : {stats['leaf_count']}\n")
            f.write(f"Profondeur maximale   : {stats['max_depth']}\n")
            f.write(f"Profondeur min feuille: {stats['min_leaf_depth']}\n")
            f.write(f"Profondeur moy feuille: {stats['avg_leaf_depth']:.2f}\n\n")
            
            f.write("Facteurs de branchement\n")
            f.write("----------------------\n")
            f.write(f"Facteur de branchement moyen: {stats['avg_branching_factor']:.2f}\n")
            for branches, count in sorted(stats["branching_factors"].items()):
                f.write(f"  {branches} branches: {count} n≈ìuds\n")
            f.write("\n")
            
            f.write("Statistiques des feuilles\n")
            f.write("------------------------\n")
            f.write(f"Taille moyenne: {stats['avg_leaf_size']:.2f} indices\n")
            f.write(f"Taille min    : {stats['min_leaf_size']} indices\n")
            f.write(f"Taille max    : {stats['max_leaf_size']} indices\n")
            f.write(f"Total indices : {stats['total_indices']} indices\n\n")
            
            f.write("R√©duction dimensionnelle\n")
            f.write("-----------------------\n")
            f.write(f"N≈ìuds avec r√©duction : {stats['nodes_with_reduction']}\n")
            f.write(f"Dimensions moyennes  : {stats['avg_dimensions_kept']:.1f}\n")

    def improve_with_hnsw(self, vectors_reader, max_data: Optional[int] = None) -> 'K16Tree':
        """
        Am√©liore l'arbre avec HNSW pour optimiser les candidats de chaque feuille.
        Cette fonction est maintenant une fa√ßade qui utilise la version dans TreeFlat.
        Supprime automatiquement les feuilles non mises √† jour (pruning) pour √©conomiser de l'espace.

        Args:
            vectors_reader: Lecteur de vecteurs
            max_data: Nombre maximum de candidats par feuille (utilise la config si None)

        Returns:
            K16Tree: Nouvelle instance d'arbre am√©lior√©
        """
        if not self.flat_tree:
            raise ValueError("L'arbre doit √™tre converti en structure plate avant l'am√©lioration HNSW")

        # Utiliser la version dans flat_tree
        print("üîÑ D√©l√©gation de l'am√©lioration HNSW √† la structure plate...")
        improved_flat_tree = self.flat_tree.improve_with_hnsw(vectors_reader, max_data)

        # Cr√©er une nouvelle instance d'arbre avec l'arbre plat am√©lior√©
        improved_tree = K16Tree(self.root)
        improved_tree.flat_tree = improved_flat_tree

        return improved_tree
```

### k16/core/flat_tree.py

```python
"""
Repr√©sentation plate compacte en m√©moire d'un arbre K16 (*K16Tree*).

Version optimis√©e : utilise les centro√Ødes r√©duits pour √©conomiser ~56% d'espace.
Inclut des fonctionnalit√©s pour supprimer les feuilles non utilis√©es.
"""

from __future__ import annotations

import os
import json
import time
from typing import List, Optional, TYPE_CHECKING, Dict, Any, Set
from collections import defaultdict

import numpy as np

from k16.core.tree import TreeNode, K16Tree

if TYPE_CHECKING:  # uniquement pour les v√©rificateurs de types statiques
    from k16.io.reader import VectorReader

# V√©rifier si Numba est disponible
try:
    import numba
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False


class TreeFlat:
    """
    Repr√©sentation plate compacte en m√©moire d'un arbre K16.

    Version optimis√©e pour √©conomiser l'espace m√©moire:
    - Ne stocke que les centro√Ødes r√©duits, pas les centro√Ødes complets
    - Utilis√©e automatiquement pour toutes les op√©rations de recherche
    """

    # ------------------------------------------------------------------
    # Construction helpers
    # ------------------------------------------------------------------
    def __init__(self, dims: int, max_depth: int):
        self.dims = dims              # original embedding dimension
        self.depth = 0                # will be set after construction
        self.max_depth = max_depth

        # Structures pour tous les n≈ìuds
        # Format classique (pendant la construction) ou format optimis√© (apr√®s chargement)
        self.node_centroids: Optional[np.ndarray] = None           # (n_nodes, dims) float32
        self.node_centroids_reduced: Optional[np.ndarray] = None   # (n_nodes, max_d_head) float32

        # R√©duction dimensionnelle : chaque n≈ìud d√©finit les dimensions pour ses enfants
        self.node_children_top_dims: Optional[np.ndarray] = None   # (n_nodes, max_dims) int32
        self.node_children_d_head: Optional[np.ndarray] = None     # (n_nodes,) int32

        self.node_levels: Optional[np.ndarray] = None              # (n_nodes,) int32
        self.node_is_leaf: Optional[np.ndarray] = None             # (n_nodes,) bool

        # Structure de navigation
        self.node_children_count: Optional[np.ndarray] = None      # (n_nodes,) int32
        self.node_children_start: Optional[np.ndarray] = None      # (n_nodes,) int32
        self.children_indices: Optional[np.ndarray] = None         # (total_children,) int32

        # Leaves data
        self.leaf_offset: Optional[np.ndarray] = None              # (n_nodes,) int32  / -1 for internal nodes
        self.leaf_data: Optional[np.ndarray] = None                # (total_leaf_indices,) int32
        self.leaf_bounds: Optional[np.ndarray] = None              # (n_leaves+1,) int32

        # Misc stats
        self.n_nodes: int = 0
        self.n_leaves: int = 0
        self.n_levels: int = 0

        # Mapping nodes (used for construction)
        self.nodes_list: List[TreeNode] = []

    # ------------------------------------------------------------------
    # Factory from hierarchical tree
    # ------------------------------------------------------------------
    @classmethod
    def from_tree(cls, tree: K16Tree) -> "TreeFlat":
        if not tree.root:
            raise ValueError("Empty source tree")

        dims = int(tree.root.centroid.shape[0])
        stats = tree.get_statistics()
        max_depth = stats["max_depth"] + 1  # include root level 0

        ft = cls(dims, max_depth)

        # ------------------------------------------------------------------
        # Collecter tous les n≈ìuds dans un ordre DFS
        # ------------------------------------------------------------------
        ft.nodes_list = []
        node_to_idx = {}
        
        def collect_dfs(node: TreeNode) -> int:
            """Collecte les n≈ìuds en DFS et retourne l'index du n≈ìud."""
            idx = len(ft.nodes_list)
            ft.nodes_list.append(node)
            node_to_idx[node] = idx
            
            # Collecter les enfants
            for child in node.children:
                collect_dfs(child)
            
            return idx
        
        collect_dfs(tree.root)
        
        total_nodes = len(ft.nodes_list)
        ft.n_nodes = total_nodes
        
        # ------------------------------------------------------------------
        # Pr√©parer les structures de donn√©es
        # ------------------------------------------------------------------
        # R√©cup√©rer max_dims depuis la configuration
        try:
            from k16.utils.config import ConfigManager
            config_manager = ConfigManager()
            max_dims = config_manager.get("flat_tree", "max_dims", 128)  # Valeur par d√©faut: 128 dimensions
        except Exception:
            max_dims = 128  # Valeur par d√©faut si la config n'est pas disponible

        # Trouver la dimension maximale de r√©duction utilis√©e, mais en respectant max_dims
        max_reduction_dim = min(max_dims, max(
            (node.children_d_head if node.children_d_head is not None else dims
             for node in ft.nodes_list),
            default=dims
        ))
        
        # Allouer les tableaux
        ft.node_centroids = np.zeros((total_nodes, dims), dtype=np.float32)
        ft.node_children_top_dims = np.zeros((total_nodes, max_reduction_dim), dtype=np.int32)
        ft.node_children_d_head = np.zeros(total_nodes, dtype=np.int32)
        ft.node_levels = np.zeros(total_nodes, dtype=np.int32)
        ft.node_is_leaf = np.zeros(total_nodes, dtype=bool)
        ft.node_children_count = np.zeros(total_nodes, dtype=np.int32)
        ft.node_children_start = np.zeros(total_nodes, dtype=np.int32)
        
        # Compter le nombre total d'enfants
        total_children = sum(len(node.children) for node in ft.nodes_list)
        ft.children_indices = np.zeros(total_children, dtype=np.int32)
        
        # Allouer pour les feuilles
        ft.leaf_offset = np.full(total_nodes, -1, dtype=np.int32)
        
        # Compter les feuilles et leurs indices
        leaf_nodes = []
        total_leaf_indices = 0
        for node in ft.nodes_list:
            if node.is_leaf():
                leaf_nodes.append(node)
                total_leaf_indices += len(node.indices)
        
        ft.n_leaves = len(leaf_nodes)
        ft.leaf_data = np.zeros(total_leaf_indices, dtype=np.int32)
        ft.leaf_bounds = np.zeros(ft.n_leaves + 1, dtype=np.int32)
        
        # ------------------------------------------------------------------
        # Remplir les structures
        # ------------------------------------------------------------------
        children_ptr = 0
        leaf_idx = 0
        leaf_data_ptr = 0
        ft.leaf_bounds[0] = 0
        
        for i, node in enumerate(ft.nodes_list):
            # Niveau et type de n≈ìud
            ft.node_levels[i] = node.level
            ft.node_is_leaf[i] = node.is_leaf()
            
            # Centro√Øde complet (d√©j√† normalis√©)
            ft.node_centroids[i] = node.centroid

            # R√©duction dimensionnelle pour les enfants
            if node.children_top_dims is not None and node.children_d_head is not None:
                # Ce n≈ìud a d√©fini des dimensions pour ses enfants
                d_head = node.children_d_head
                ft.node_children_d_head[i] = d_head
                ft.node_children_top_dims[i, :d_head] = node.children_top_dims[:d_head]
            else:
                # Pas de r√©duction (utiliser les dimensions dans la limite de max_reduction_dim)
                limited_dims = min(dims, max_reduction_dim)
                ft.node_children_d_head[i] = limited_dims
                ft.node_children_top_dims[i, :limited_dims] = np.arange(limited_dims, dtype=np.int32)
            
            # Enfants
            if not node.is_leaf():
                ft.node_children_count[i] = len(node.children)
                ft.node_children_start[i] = children_ptr
                
                # Ajouter les indices des enfants
                for child in node.children:
                    child_idx = node_to_idx[child]
                    ft.children_indices[children_ptr] = child_idx
                    children_ptr += 1
            
            # Donn√©es des feuilles
            if node.is_leaf():
                ft.leaf_offset[i] = leaf_idx
                
                idxs = node.indices
                n_idx = len(idxs)
                ft.leaf_data[leaf_data_ptr:leaf_data_ptr + n_idx] = idxs
                leaf_data_ptr += n_idx
                ft.leaf_bounds[leaf_idx + 1] = leaf_data_ptr
                
                leaf_idx += 1
        
        # Stats finales
        ft.n_levels = max_depth
        ft.depth = max_depth - 1
        
        print(f"‚úì Structure TreeFlat cr√©√©e avec r√©duction coh√©rente parent-enfants")
        print(f"  ‚Üí {ft.n_nodes} n≈ìuds, {ft.n_leaves} feuilles")
        print(f"  ‚Üí Dimension de r√©duction max: {max_reduction_dim} (configur√©e: {max_dims})")
        
        return ft

    # ------------------------------------------------------------------
    # Navigation helpers
    # ------------------------------------------------------------------
    def _get_children_indices(self, node_idx: int) -> np.ndarray:
        """R√©cup√®re les indices globaux des enfants d'un n≈ìud."""
        if self.node_is_leaf[node_idx]:
            return np.array([], dtype=np.int32)
        
        count = self.node_children_count[node_idx]
        if count == 0:
            return np.array([], dtype=np.int32)
        
        start = self.node_children_start[node_idx]
        return self.children_indices[start:start + count]
    
    def _is_leaf(self, node_idx: int) -> bool:
        """V√©rifie si un n≈ìud est une feuille."""
        return self.node_is_leaf[node_idx]

    # ------------------------------------------------------------------
    # Single-path search
    # ------------------------------------------------------------------
    def get_leaf_indices(self, query: np.ndarray) -> np.ndarray:
        """
        Find the leaf node index for a given query vector.
        Utilise les centro√Ødes r√©duits automatiquement.
        Optimis√© pour les instructions SIMD (AVX2/AVX-512).

        Args:
            query: The query vector.

        Returns:
            The global node index of the leaf node.
        """
        # Assurer float32 et contigu√Øt√© m√©moire pour optimisations SIMD
        query_f32 = np.ascontiguousarray(query, dtype=np.float32)
        node_idx = 0  # Commencer √† la racine

        while not self._is_leaf(node_idx):
            # R√©cup√©rer les enfants
            children_indices = self._get_children_indices(node_idx)
            if len(children_indices) == 0:
                break

            # Utiliser les dimensions d√©finies par CE N≈íUD pour ses enfants
            d_head = self.node_children_d_head[node_idx]
            top_dims = self.node_children_top_dims[node_idx, :d_head]

            # Projeter la requ√™te sur les dimensions importantes
            query_projected = query_f32[top_dims]

            # Normaliser la requ√™te projet√©e (optimis√© pour SIMD)
            query_norm = np.linalg.norm(query_projected)
            if query_norm > 0:  # √âviter division par z√©ro
                query_projected = query_projected / query_norm

            # Calculer les similarit√©s avec TOUS les enfants d'un coup (exploite SIMD)
            similarities = np.zeros(len(children_indices), dtype=np.float32)

            for i, child_idx in enumerate(children_indices):
                # Utiliser les centro√Ødes r√©duits si disponibles, sinon projeter les complets
                if self.node_centroids_reduced is not None:
                    # Version optimis√©e avec centro√Ødes pr√©-r√©duits
                    child_centroid_projected = self.node_centroids_reduced[child_idx, :d_head]
                else:
                    # Version standard qui projette √† la vol√©e
                    child_centroid_full = self.node_centroids[child_idx]
                    child_centroid_projected = child_centroid_full[top_dims]

                # Calcul de similarit√© (dot product optimis√© SIMD)
                similarities[i] = np.dot(query_projected, child_centroid_projected)

            # Trouver le meilleur enfant (plus efficace que la boucle pr√©c√©dente)
            best_idx = np.argmax(similarities)
            if similarities[best_idx] > -np.inf:
                node_idx = children_indices[best_idx]
            else:
                break

        return node_idx
    
    def search_tree_single(self, query: np.ndarray) -> np.ndarray:
        """Recherche simple dans l'arbre."""
        global_node_idx = self.get_leaf_indices(query)
        if self.leaf_offset[global_node_idx] >= 0:
            leaf_idx = self.leaf_offset[global_node_idx]
            start = self.leaf_bounds[leaf_idx]
            end = self.leaf_bounds[leaf_idx + 1]
            return self.leaf_data[start:end]

        return np.array([], dtype=np.int32)

    # ------------------------------------------------------------------
    # Beam search
    # ------------------------------------------------------------------
    def search_tree_beam(self, query: np.ndarray, beam_width: int = 3) -> np.ndarray:
        """
        Recherche par faisceau dans l'arbre.
        Utilise les centro√Ødes r√©duits automatiquement.
        Optimis√© pour les instructions SIMD (AVX2/AVX-512).

        Args:
            query: Vecteur de requ√™te
            beam_width: Largeur du faisceau

        Returns:
            Indices des vecteurs dans les feuilles visit√©es
        """
        if beam_width <= 1:
            return self.search_tree_single(query)

        # IMPORTANT : D'abord obtenir les r√©sultats du single search
        # pour garantir qu'on ne fait jamais pire
        single_search_results = self.search_tree_single(query)

        # Assurer float32 et contigu√Øt√© m√©moire pour optimisations SIMD
        query_f32 = np.ascontiguousarray(query, dtype=np.float32)

        # Beam: liste de tuples (node_idx, score)
        beam = [(0, 1.0)]  # Commencer √† la racine
        visited_leaves = []  # Liste des node_idx des feuilles visit√©es

        while beam:
            # Prendre le meilleur n≈ìud du beam
            node_idx, parent_score = beam.pop(0)

            # Si c'est une feuille, l'ajouter aux r√©sultats
            if self._is_leaf(node_idx):
                visited_leaves.append(node_idx)
                continue

            # Explorer les enfants
            children_indices = self._get_children_indices(node_idx)
            n_children = len(children_indices)
            if n_children == 0:
                continue

            # Utiliser les dimensions du PARENT pour comparer ses enfants
            d_head = self.node_children_d_head[node_idx]
            top_dims = self.node_children_top_dims[node_idx, :d_head]
            query_projected = query_f32[top_dims]

            # Normaliser la requ√™te projet√©e (optimis√© pour SIMD)
            query_norm = np.linalg.norm(query_projected)
            if query_norm > 0:  # √âviter division par z√©ro
                query_projected = query_projected / query_norm

            # Pr√©allouer le tableau de similarit√©s pour exploiter SIMD
            similarities = np.zeros(n_children, dtype=np.float32)

            # Vectorisation du calcul des similarit√©s
            if self.node_centroids_reduced is not None:
                # Version optimis√©e: calculer toutes les similarit√©s en une fois
                # Extraire tous les centro√Ødes r√©duits d'un coup
                child_centroids_batch = np.zeros((n_children, d_head), dtype=np.float32)
                for i, child_idx in enumerate(children_indices):
                    child_centroids_batch[i] = self.node_centroids_reduced[child_idx, :d_head]

                # Produit scalaire vectoris√© (exploite SIMD)
                similarities = np.dot(child_centroids_batch, query_projected)
            else:
                # Version standard
                for i, child_idx in enumerate(children_indices):
                    # Projeter chaque centro√Øde
                    child_centroid_full = self.node_centroids[child_idx]
                    child_centroid_projected = child_centroid_full[top_dims]
                    # Calcul de similarit√©
                    similarities[i] = np.dot(query_projected, child_centroid_projected)

            # Trouver les top-k indices (plus rapide que le tri complet)
            if beam_width >= n_children:
                # Tous les enfants sont inclus, pas besoin de trier
                top_indices = np.arange(n_children)
            else:
                # Utiliser partition pour obtenir les top-k indices efficacement
                top_indices = np.argpartition(-similarities, beam_width-1)[:beam_width]

            # Ajouter les meilleurs enfants au beam
            for i in top_indices:
                beam.append((children_indices[i], similarities[i]))

            # Trier le beam par score (uniquement si n√©cessaire)
            if len(beam) > beam_width:
                beam.sort(key=lambda x: x[1], reverse=True)
                beam = beam[:beam_width]

        # Collecter les indices depuis TOUTES les feuilles visit√©es
        # Pr√©allouer un tableau suffisamment grand (estimation)
        max_leaf_data = 0
        for node_idx in visited_leaves:
            if self.leaf_offset[node_idx] >= 0:
                leaf_idx = self.leaf_offset[node_idx]
                leaf_size = self.leaf_bounds[leaf_idx + 1] - self.leaf_bounds[leaf_idx]
                max_leaf_data += leaf_size

        # Allouer l'espace pour les indices collect√©s
        beam_idx = np.zeros(max_leaf_data, dtype=np.int32)
        idx_count = 0

        # Collecter tous les indices
        for node_idx in visited_leaves:
            if self.leaf_offset[node_idx] >= 0:
                leaf_idx = self.leaf_offset[node_idx]
                start = self.leaf_bounds[leaf_idx]
                end = self.leaf_bounds[leaf_idx + 1]
                leaf_size = end - start
                beam_idx[idx_count:idx_count + leaf_size] = self.leaf_data[start:end]
                idx_count += leaf_size

        # R√©duire √† la taille r√©elle
        beam_idx = beam_idx[:idx_count]

        # IMPORTANT : Combiner avec les r√©sultats du single search
        # pour garantir qu'on ne fait jamais pire
        all_idx = np.unique(np.concatenate([single_search_results, beam_idx]))

        return all_idx

    # unified entry point
    def search_tree(
        self,
        query: np.ndarray,
        beam_width: int = 1,
        vectors_reader: Optional["VectorReader"] = None,
        k: Optional[int] = None,
    ) -> np.ndarray:
        """
        Point d'entr√©e unifi√© pour la recherche avec optimisations SIMD.

        Cette m√©thode utilise les instructions SIMD pour le filtrage final
        des candidats, b√©n√©ficiant d'une taille de max_data multiple de 16.
        """
        # Garantir que la requ√™te est contigu√´ en m√©moire (optimal pour SIMD)
        query = np.ascontiguousarray(query, dtype=np.float32)

        # Trouver les candidats avec recherche simple ou faisceau
        if beam_width <= 1:
            candidates = self.search_tree_single(query)
        else:
            candidates = self.search_tree_beam(query, beam_width)

        # Si un lecteur de vecteurs est fourni, filtrer les candidats par similarit√©
        if vectors_reader is not None and len(candidates) > 0:
            # Utiliser dot vectoris√© pour b√©n√©ficier de SIMD
            # R√©cup√©rer tous les vecteurs candidats d'un coup
            # pour une meilleure utilisation du cache et des registres SIMD
            scores = vectors_reader.dot(candidates.tolist(), query)

            if k is not None and 0 < k < len(candidates):
                # Utiliser argpartition pour trouver les top-k (plus efficace que tri complet)
                # Cette m√©thode est optimis√©e pour SIMD sur les tableaux de taille 2^n
                top_local = np.argpartition(-scores, k - 1)[:k]
                # Trier seulement les top-k √©l√©ments (beaucoup plus rapide)
                top_sorted = top_local[np.argsort(-scores[top_local])]
                candidates = candidates[top_sorted]
            else:
                # Tri complet (si n√©cessaire)
                candidates = candidates[np.argsort(-scores)]
        return candidates

    # ------------------------------------------------------------------
    # Sauvegarde / chargement
    # ------------------------------------------------------------------
    def save(self, file_path: str, mmap_dir: bool = False, minimal: bool = True) -> None:
        """
        Sauvegarde de la structure plate.
        Par d√©faut, optimise automatiquement en ne stockant que les centro√Ødes r√©duits.

        Args:
            file_path: Chemin du fichier de sauvegarde
            mmap_dir: Si True, cr√©e un r√©pertoire de fichiers numpy pour le memory-mapping
            minimal: Si True (par d√©faut), sauvegarde uniquement les structures essentielles
        """
        # Optimisation: cr√©er des centro√Ødes r√©duits pour √©conomiser de l'espace
        print("‚è≥ Optimisation: cr√©ation des centro√Ødes r√©duits...")

        # Trouver la dimension maximale de r√©duction
        max_d_head = int(np.max(self.node_children_d_head))

        # Cr√©er un mapping enfant -> parent
        child_to_parent = {}
        for parent_idx in range(self.n_nodes):
            children = self._get_children_indices(parent_idx)
            for child_idx in children:
                child_to_parent[int(child_idx)] = parent_idx

        # Allouer les centro√Ødes r√©duits
        node_centroids_reduced = np.zeros((self.n_nodes, max_d_head), dtype=np.float32)

        # Projeter les centro√Ødes
        for i in range(self.n_nodes):
            if i == 0:  # Racine
                # Pour la racine, utiliser ses propres dimensions
                d_head = self.node_children_d_head[i]
                top_dims = self.node_children_top_dims[i, :d_head]
                node_centroids_reduced[i, :d_head] = self.node_centroids[i][top_dims]
            elif i in child_to_parent:
                # N≈ìud avec parent
                parent_idx = child_to_parent[i]
                d_head = self.node_children_d_head[parent_idx]
                top_dims = self.node_children_top_dims[parent_idx, :d_head]
                node_centroids_reduced[i, :d_head] = self.node_centroids[i][top_dims]
            else:
                # N≈ìud sans parent connu (ne devrait pas arriver)
                d_head = self.node_children_d_head[i]
                top_dims = self.node_children_top_dims[i, :d_head]
                node_centroids_reduced[i, :d_head] = self.node_centroids[i][top_dims]

        # Calculer l'√©conomie d'espace
        full_size = self.node_centroids.size * self.node_centroids.itemsize
        reduced_size = node_centroids_reduced.size * node_centroids_reduced.itemsize
        reduction = (1 - reduced_size / full_size) * 100

        print(f"‚úì Optimisation termin√©e: √©conomie {reduction:.1f}% d'espace")

        # Sauvegarde avec les centro√Ødes r√©duits plut√¥t que complets
        if mmap_dir:
            base = os.path.splitext(file_path)[0]
            os.makedirs(base, exist_ok=True)
            meta = {
                "dims": self.dims,
                "depth": self.depth,
                "n_levels": self.n_levels,
                "optimized": True  # Marquer comme optimis√©
            }

            # Ajouter les statistiques non essentielles uniquement si minimal=False
            if not minimal:
                meta.update({
                    "max_depth": self.max_depth,
                    "n_nodes": self.n_nodes,
                    "n_leaves": self.n_leaves,
                })

            # Enregistrement des m√©tadonn√©es
            with open(os.path.join(base, 'meta.json'), 'w') as f:
                json.dump(meta, f)

            # S√©rialisation des tableaux numpy s√©par√©ment (avec centro√Ødes r√©duits)
            np.save(os.path.join(base, 'node_centroids_reduced.npy'), node_centroids_reduced)
            np.save(os.path.join(base, 'node_children_top_dims.npy'), self.node_children_top_dims)
            np.save(os.path.join(base, 'node_children_d_head.npy'), self.node_children_d_head)
            np.save(os.path.join(base, 'node_levels.npy'), self.node_levels)
            np.save(os.path.join(base, 'node_is_leaf.npy'), self.node_is_leaf)
            np.save(os.path.join(base, 'node_children_count.npy'), self.node_children_count)
            np.save(os.path.join(base, 'node_children_start.npy'), self.node_children_start)
            np.save(os.path.join(base, 'children_indices.npy'), self.children_indices)
            np.save(os.path.join(base, 'leaf_offset.npy'), self.leaf_offset)
            np.save(os.path.join(base, 'leaf_data.npy'), self.leaf_data)
            np.save(os.path.join(base, 'leaf_bounds.npy'), self.leaf_bounds)
        else:
            # Structures essentielles pour la recherche (avec centro√Ødes r√©duits)
            data = {
                "dims": self.dims,
                "depth": self.depth,
                "n_levels": self.n_levels,
                "optimized": True,  # Marquer comme optimis√©
                "node_centroids_reduced": node_centroids_reduced,  # Centro√Ødes r√©duits
                "node_children_top_dims": self.node_children_top_dims,
                "node_children_d_head": self.node_children_d_head,
                "node_levels": self.node_levels,
                "node_is_leaf": self.node_is_leaf,
                "node_children_count": self.node_children_count,
                "node_children_start": self.node_children_start,
                "children_indices": self.children_indices,
                "leaf_offset": self.leaf_offset,
                "leaf_data": self.leaf_data,
                "leaf_bounds": self.leaf_bounds,
            }

            # Ajouter les propri√©t√©s non essentielles uniquement si minimal=False
            if not minimal:
                data.update({
                    "max_depth": self.max_depth,
                    "n_nodes": self.n_nodes,
                    "n_leaves": self.n_leaves,
                })

            np.save(file_path, data, allow_pickle=True)

        if minimal:
            print(f"‚úì Sauvegarde en mode minimal optimis√© (structures essentielles uniquement)")
        else:
            print(f"‚úì Sauvegarde en mode complet optimis√© (toutes les structures)")

    @classmethod
    def load(cls, file_path: str, mmap_mode: Optional[str] = None) -> "TreeFlat":
        """
        Charge la structure plate.

        D√©tecte automatiquement le format optimis√© ou standard.
        """
        base = os.path.splitext(file_path)[0]
        if mmap_mode and os.path.isdir(base):
            return cls._load_from_mmap_dir(base)

        # Charger les donn√©es
        if mmap_mode:
            data = np.load(file_path, allow_pickle=True, mmap_mode=mmap_mode).item()
        else:
            data = np.load(file_path, allow_pickle=True).item()

        # Cr√©er l'objet
        obj = cls(data["dims"], data["depth"] + 1)

        # Charger les champs de base
        obj.depth = data["depth"]
        obj.n_levels = data["n_levels"]
        obj.n_nodes = len(data["node_levels"])
        obj.n_leaves = len(data["leaf_bounds"]) - 1

        # D√©tecter si c'est un format optimis√©
        is_optimized = "node_centroids_reduced" in data or data.get("optimized", False)

        # Charger les centro√Ødes (complets ou r√©duits)
        if is_optimized and "node_centroids_reduced" in data:
            # Format optimis√©
            obj.node_centroids = None
            obj.node_centroids_reduced = data["node_centroids_reduced"]
            print(f"‚úì Chargement d'un fichier optimis√© (√©conomie ~56% m√©moire)")
        elif "node_centroids" in data:
            # Format standard
            obj.node_centroids = data["node_centroids"]
            obj.node_centroids_reduced = None
            print(f"‚úì Chargement d'un fichier standard")

        # Charger les structures de navigation
        obj.node_children_top_dims = data["node_children_top_dims"]
        obj.node_children_d_head = data["node_children_d_head"]
        obj.node_levels = data["node_levels"]
        obj.node_is_leaf = data["node_is_leaf"]
        obj.node_children_count = data["node_children_count"]
        obj.node_children_start = data["node_children_start"]
        obj.children_indices = data["children_indices"]
        obj.leaf_offset = data["leaf_offset"]
        obj.leaf_data = data["leaf_data"]
        obj.leaf_bounds = data["leaf_bounds"]

        return obj

    @classmethod
    def _load_from_mmap_dir(cls, base: str) -> "TreeFlat":
        """
        Charge la structure plate depuis un r√©pertoire mmap.
        """
        # Chargement des m√©tadonn√©es
        meta_path = os.path.join(base, 'meta.json')
        with open(meta_path, 'r') as f:
            meta = json.load(f)

        # Cr√©er l'objet
        obj = cls(int(meta['dims']), int(meta['depth']) + 1)

        # Charger les champs de base
        obj.depth = int(meta['depth'])
        obj.n_levels = int(meta['n_levels'])

        # D√©terminer si c'est un format optimis√©
        is_optimized = meta.get("optimized", False)
        centroids_reduced_path = os.path.join(base, 'node_centroids_reduced.npy')

        # Charger les centro√Ødes (complets ou r√©duits)
        if is_optimized and os.path.exists(centroids_reduced_path):
            # Format optimis√©
            obj.node_centroids = None
            obj.node_centroids_reduced = np.load(centroids_reduced_path, mmap_mode='r')
            print(f"‚úì Chargement d'un r√©pertoire mmap optimis√© (√©conomie ~56% m√©moire)")
        else:
            # Format standard
            centroids_path = os.path.join(base, 'node_centroids.npy')
            obj.node_centroids = np.load(centroids_path, mmap_mode='r')
            obj.node_centroids_reduced = None
            print(f"‚úì Chargement d'un r√©pertoire mmap standard")

        # Charger les structures de navigation
        obj.node_children_top_dims = np.load(os.path.join(base, 'node_children_top_dims.npy'), mmap_mode='r')
        obj.node_children_d_head = np.load(os.path.join(base, 'node_children_d_head.npy'), mmap_mode='r')
        obj.node_levels = np.load(os.path.join(base, 'node_levels.npy'), mmap_mode='r')
        obj.node_is_leaf = np.load(os.path.join(base, 'node_is_leaf.npy'), mmap_mode='r')
        obj.node_children_count = np.load(os.path.join(base, 'node_children_count.npy'), mmap_mode='r')
        obj.node_children_start = np.load(os.path.join(base, 'node_children_start.npy'), mmap_mode='r')
        obj.children_indices = np.load(os.path.join(base, 'children_indices.npy'), mmap_mode='r')
        obj.leaf_offset = np.load(os.path.join(base, 'leaf_offset.npy'), mmap_mode='r')
        obj.leaf_data = np.load(os.path.join(base, 'leaf_data.npy'), mmap_mode='r')
        obj.leaf_bounds = np.load(os.path.join(base, 'leaf_bounds.npy'), mmap_mode='r')

        # Calculer les valeurs manquantes
        obj.n_nodes = len(obj.node_levels)
        obj.n_leaves = len(obj.leaf_bounds) - 1

        return obj

    # ------------------------------------------------------------------
    # Stats helper
    # ------------------------------------------------------------------
    def get_statistics(self) -> Dict[str, Any]:
        """Obtenir les statistiques de la structure plate."""
        stats: Dict[str, Any] = {
            "n_nodes": self.n_nodes,
            "n_leaves": self.n_leaves,
            "max_depth": self.depth,
            "n_levels": self.n_levels,
            "dims": self.dims,
            "nodes_with_reduction": 0,
            "avg_dimensions_kept": 0,
        }
        
        # Compter les n≈ìuds avec r√©duction
        if self.node_children_d_head is not None:
            # Les n≈ìuds avec r√©duction sont ceux o√π d_head < dims
            nodes_with_reduction = np.sum(self.node_children_d_head < self.dims)
            stats["nodes_with_reduction"] = int(nodes_with_reduction)
            
            # Moyenne des dimensions conserv√©es (pour les n≈ìuds non-feuilles)
            non_leaf_mask = ~self.node_is_leaf
            if np.any(non_leaf_mask):
                stats["avg_dimensions_kept"] = float(np.mean(self.node_children_d_head[non_leaf_mask]))
        
        if self.n_leaves > 0:
            sizes = [self.leaf_bounds[i + 1] - self.leaf_bounds[i] for i in range(self.n_leaves)]
            stats.update(
                avg_leaf_size=sum(sizes) / self.n_leaves,
                min_leaf_size=min(sizes),
                max_leaf_size=max(sizes),
                total_indices=len(self.leaf_data),
            )
        return stats

    # ------------------------------------------------------------------
    # HNSW Improvement avec pruning automatique
    # ------------------------------------------------------------------
    def improve_with_hnsw(self, vectors_reader, max_data: Optional[int] = None) -> 'TreeFlat':
        """
        Am√©liore l'arbre avec HNSW pour optimiser les candidats de chaque feuille.
        Supprime automatiquement les feuilles non mises √† jour (pruning) pour √©conomiser de l'espace.

        Args:
            vectors_reader: Lecteur de vecteurs
            max_data: Nombre maximum de candidats par feuille

        Returns:
            TreeFlat: Nouvelle instance d'arbre am√©lior√©
        """
        # Charger la configuration
        try:
            from k16.utils.config import ConfigManager
            config_manager = ConfigManager()
            build_config = config_manager.get_section("build_tree")

            if max_data is None:
                max_data = build_config.get("max_data", 200)
            hnsw_batch_size = build_config.get("hnsw_batch_size", 1000)
            grouping_batch_size = build_config.get("grouping_batch_size", 5000)
            hnsw_m = build_config.get("hnsw_m", 16)
            hnsw_ef_construction = build_config.get("hnsw_ef_construction", 200)
        except:
            if max_data is None:
                max_data = 200
            hnsw_batch_size = 1000
            grouping_batch_size = 5000
            hnsw_m = 16
            hnsw_ef_construction = 200

        print(f"üéØ Am√©lioration de l'arbre avec HNSW")
        print(f"  - Max data par feuille : {max_data}")
        print(f"  - Batch size HNSW : {hnsw_batch_size}")
        print(f"  - Batch size groupement : {grouping_batch_size}")
        print(f"  - Pruning des feuilles non mises √† jour : Activ√© (automatique)")

        # 1. Grouper les vecteurs par feuilles
        leaf_groups = self._group_vectors_by_leaf_signature(vectors_reader, grouping_batch_size)

        # 2. Calculer les centro√Ødes
        leaf_centroids = self._compute_leaf_centroids(leaf_groups, vectors_reader)

        # 3. Construire l'index HNSW global
        hnsw_index = self._build_global_hnsw_index(vectors_reader, hnsw_m, hnsw_ef_construction)

        # 4. Am√©liorer les candidats avec HNSW
        improved_candidates = self._improve_leaf_candidates_with_hnsw(
            leaf_groups, leaf_centroids, hnsw_index, max_data, hnsw_batch_size
        )

        # 5. Cr√©er une nouvelle instance d'arbre avec les candidats am√©lior√©s
        improved_tree = self._update_flat_tree_candidates(
            leaf_groups, improved_candidates
        )

        # Appliquer automatiquement le pruning des feuilles non mises √† jour
        # R√©cup√©rer les signatures de feuilles mises √† jour
        updated_signatures = set(improved_candidates.keys())

        # Convertir signatures en indices de feuilles
        signature_to_leaf_mapping = self._precompute_leaf_signatures()
        updated_leaves_set = {
            signature_to_leaf_mapping[signature]
            for signature in updated_signatures
            if signature in signature_to_leaf_mapping
        }

        # Appliquer le pruning
        improved_tree = improved_tree.prune_unused_leaves(updated_leaves_set)

        print("‚úÖ Am√©lioration HNSW termin√©e!")
        return improved_tree

    def _group_vectors_by_leaf_signature(self, vectors_reader, batch_size: int = 5000) -> Dict[str, List[int]]:
        """Groupe les vecteurs par signature de feuille."""
        print(f"üîÑ Groupement des vecteurs par feuilles (batch_size={batch_size})...")

        # Groupement par n≈ìud feuille
        node_groups = defaultdict(list)
        leaf_groups = defaultdict(list)
        start_time = time.time()

        total_vectors = len(vectors_reader)
        num_batches = (total_vectors + batch_size - 1) // batch_size

        for batch_idx in range(num_batches):
            batch_start = batch_idx * batch_size
            batch_end = min(batch_start + batch_size, total_vectors)

            # Progr√®s
            progress = (batch_end / total_vectors) * 100
            elapsed = time.time() - start_time
            eta = elapsed * (total_vectors / batch_end - 1) if batch_end > 0 else 0
            print(f"  Batch {batch_idx + 1}/{num_batches} - Progress: {progress:.1f}% - ETA: {eta:.1f}s")

            # Traiter chaque vecteur du batch
            for vector_idx in range(batch_start, batch_end):
                vector = vectors_reader[vector_idx]

                # Utiliser get_leaf_indices pour trouver le n≈ìud feuille
                global_node_idx = self.get_leaf_indices(vector)

                # V√©rifier si c'est bien une feuille
                if self.leaf_offset[global_node_idx] >= 0:
                    # Ajouter ce vecteur au groupe de son n≈ìud feuille
                    node_groups[global_node_idx].append(vector_idx)

                    # Cr√©er √©galement le groupement par signature des candidats
                    leaf_idx = self.leaf_offset[global_node_idx]
                    start = self.leaf_bounds[leaf_idx]
                    end = self.leaf_bounds[leaf_idx + 1]
                    candidates = self.leaf_data[start:end]
                    signature = ",".join(map(str, sorted(candidates)))
                    leaf_groups[signature].append(vector_idx)

        elapsed_time = time.time() - start_time
        print(f"‚úì Groupement termin√© en {elapsed_time:.2f}s")
        print(f"‚úì {len(node_groups)} n≈ìuds feuilles uniques identifi√©s")
        print(f"‚úì {len(leaf_groups)} signatures de feuilles uniques identifi√©es")

        return leaf_groups

    def _compute_leaf_centroids(self, leaf_groups: Dict[str, List[int]], vectors_reader) -> Dict[str, np.ndarray]:
        """Calcule le centro√Øde normalis√© L2 pour chaque feuille."""
        print("üßÆ Calcul des centro√Ødes de feuilles...")

        leaf_centroids = {}
        signatures = list(leaf_groups.keys())
        print(f"  üìä Calcul de {len(signatures)} centro√Ødes")

        for i, signature in enumerate(signatures):
            if i % 1000 == 0:
                print(f"  Centro√Øde {i+1}/{len(signatures)} - {len(leaf_groups[signature])} vecteurs")

            vector_indices = leaf_groups[signature]

            # R√©cup√©rer les vecteurs de cette feuille
            vectors = vectors_reader[vector_indices]

            # Calculer la moyenne
            centroid = np.mean(vectors, axis=0, dtype=np.float32)

            # Normaliser L2
            norm_squared = np.sum(centroid * centroid)
            if norm_squared > 0:
                centroid = centroid / np.sqrt(norm_squared)

            leaf_centroids[signature] = centroid.astype(vectors.dtype)

        print(f"‚úì {len(leaf_centroids)} centro√Ødes calcul√©s et normalis√©s")
        return leaf_centroids

    def _build_global_hnsw_index(self, vectors_reader, m: int = 16, ef_construction: int = 200):
        """Construit un index HNSW global sur tous les vecteurs."""
        print("üèóÔ∏è  Construction de l'index HNSW global...")

        try:
            import faiss
        except ImportError:
            raise ImportError("FAISS est requis pour l'am√©lioration HNSW. Installez-le avec : pip install faiss-cpu")

        # Param√®tres HNSW
        dimension = vectors_reader.vectors.shape[1]

        print(f"  - Dimension : {dimension}")
        print(f"  - Nombre de vecteurs : {len(vectors_reader)}")
        print(f"  - Param√®tre M : {m}")
        print(f"  - efConstruction : {ef_construction}")

        # Cr√©er l'index HNSW
        index = faiss.IndexHNSWFlat(dimension, m)
        index.hnsw.efConstruction = ef_construction

        # Ajouter tous les vecteurs
        start_time = time.time()
        index.add(vectors_reader.vectors)

        build_time = time.time() - start_time
        print(f"‚úì Index HNSW construit en {build_time:.2f}s")
        print(f"  - {index.ntotal} vecteurs index√©s")

        return index

    def _improve_leaf_candidates_with_hnsw(self, leaf_groups: Dict[str, List[int]],
                                         leaf_centroids: Dict[str, np.ndarray],
                                         hnsw_index, max_data: int, batch_size: int = 1000) -> Dict[str, List[int]]:
        """Am√©liore les candidats de chaque feuille en utilisant HNSW."""
        print(f"üéØ Am√©lioration des candidats avec HNSW (max_data={max_data}, batch_size={batch_size})...")

        improved_candidates = {}

        # Param√®tre de recherche HNSW
        hnsw_index.hnsw.efSearch = min(max_data * 2, 1000)

        # Pr√©parer les donn√©es pour le traitement par batch
        signatures = list(leaf_groups.keys())
        centroids_list = [leaf_centroids[sig] for sig in signatures]

        # Convertir en matrice numpy
        centroids_matrix = np.array(centroids_list)

        print(f"  üì¶ Traitement de {len(centroids_matrix)} centro√Ødes par batches de {batch_size}")

        # Traitement par batches
        k = min(max_data, hnsw_index.ntotal)

        for batch_start in range(0, len(centroids_matrix), batch_size):
            batch_end = min(batch_start + batch_size, len(centroids_matrix))
            batch_centroids = centroids_matrix[batch_start:batch_end]

            # Progr√®s
            progress = (batch_end / len(centroids_matrix)) * 100
            print(f"  Batch {batch_start//batch_size + 1}/{(len(centroids_matrix) + batch_size - 1)//batch_size} - Progress: {progress:.1f}%")

            # Recherche HNSW par batch
            distances, indices = hnsw_index.search(batch_centroids, k)

            # Assigner les r√©sultats
            for i, batch_idx in enumerate(range(batch_start, batch_end)):
                signature = signatures[batch_idx]
                new_candidates = indices[i].tolist()
                improved_candidates[signature] = new_candidates

        print(f"‚úì {len(improved_candidates)} feuilles am√©lior√©es")
        return improved_candidates

    def _update_flat_tree_candidates(self, leaf_groups: Dict[str, List[int]],
                                   improved_candidates: Dict[str, List[int]]) -> 'TreeFlat':
        """Met √† jour l'arbre plat avec les nouveaux candidats am√©lior√©s."""
        print("üîÑ Mise √† jour de l'arbre plat...")

        # Cr√©er une copie de l'arbre plat
        new_flat_tree = TreeFlat(self.dims, self.max_depth)

        # Copier toutes les propri√©t√©s
        new_flat_tree.depth = self.depth
        new_flat_tree.n_nodes = self.n_nodes
        new_flat_tree.n_leaves = self.n_leaves
        new_flat_tree.n_levels = self.n_levels
        
        # Copier les structures de n≈ìuds
        new_flat_tree.node_centroids = self.node_centroids.copy() if self.node_centroids is not None else None
        new_flat_tree.node_children_top_dims = self.node_children_top_dims.copy() if self.node_children_top_dims is not None else None
        new_flat_tree.node_children_d_head = self.node_children_d_head.copy() if self.node_children_d_head is not None else None
        new_flat_tree.node_levels = self.node_levels.copy() if self.node_levels is not None else None
        new_flat_tree.node_is_leaf = self.node_is_leaf.copy() if self.node_is_leaf is not None else None
        new_flat_tree.node_children_count = self.node_children_count.copy() if self.node_children_count is not None else None
        new_flat_tree.node_children_start = self.node_children_start.copy() if self.node_children_start is not None else None
        new_flat_tree.children_indices = self.children_indices.copy() if self.children_indices is not None else None
        new_flat_tree.leaf_offset = self.leaf_offset.copy() if self.leaf_offset is not None else None

        # Pr√©calculer le mapping signature -> leaf_idx
        signature_to_leaf_mapping = self._precompute_leaf_signatures()

        # Remplacer les donn√©es dans leaf_data
        print("  üîÑ Remplacement des candidats...")

        new_leaf_data_segments = []
        new_leaf_bounds = [0]

        total_leaves = self.n_leaves
        updated_leaves = 0

        # Cr√©er un mapping inverse
        leaf_to_signature = {leaf_idx: signature for signature, leaf_idx in signature_to_leaf_mapping.items()}

        for leaf_idx in range(total_leaves):
            if leaf_idx in leaf_to_signature:
                signature = leaf_to_signature[leaf_idx]
                if signature in improved_candidates:
                    # Utiliser les nouveaux candidats
                    new_candidates = improved_candidates[signature]
                    new_candidates_array = np.array(new_candidates, dtype=np.int32)
                    new_leaf_data_segments.append(new_candidates_array)
                    updated_leaves += 1
                else:
                    # Garder les anciens candidats
                    start = self.leaf_bounds[leaf_idx]
                    end = self.leaf_bounds[leaf_idx + 1]
                    old_candidates = self.leaf_data[start:end]
                    new_leaf_data_segments.append(old_candidates)
            else:
                # Garder les anciens candidats
                start = self.leaf_bounds[leaf_idx]
                end = self.leaf_bounds[leaf_idx + 1]
                old_candidates = self.leaf_data[start:end]
                new_leaf_data_segments.append(old_candidates)

            # Mettre √† jour leaf_bounds
            new_bound = new_leaf_bounds[-1] + len(new_leaf_data_segments[-1])
            new_leaf_bounds.append(new_bound)

        # Reconstruire leaf_data
        new_flat_tree.leaf_data = np.concatenate(new_leaf_data_segments) if new_leaf_data_segments else np.array([], dtype=np.int32)
        new_flat_tree.leaf_bounds = np.array(new_leaf_bounds, dtype=np.int32)

        print(f"  ‚úì {updated_leaves}/{total_leaves} feuilles mises √† jour")
        print(f"  ‚úì Nouvelle taille leaf_data : {len(new_flat_tree.leaf_data)}")

        return new_flat_tree

    def _precompute_leaf_signatures(self) -> Dict[str, int]:
        """Pr√©calcule un mapping des signatures de feuilles vers leurs indices."""
        print("üó∫Ô∏è  Pr√©calcul du mapping signatures -> feuilles...")

        signature_to_leaf = {}

        for leaf_idx in range(self.n_leaves):
            # Extraire les candidats de cette feuille
            start = self.leaf_bounds[leaf_idx]
            end = self.leaf_bounds[leaf_idx + 1]
            candidates = self.leaf_data[start:end]

            # Cr√©er la signature
            signature = ",".join(map(str, sorted(candidates)))
            signature_to_leaf[signature] = leaf_idx

        print(f"  ‚úì {len(signature_to_leaf)} signatures de feuilles pr√©calcul√©es")
        return signature_to_leaf

    # ------------------------------------------------------------------
    # Pruning (suppression des feuilles non utilis√©es)
    # ------------------------------------------------------------------
    def prune_unused_leaves(self, updated_leaves_set: Set[int]) -> 'TreeFlat':
        """
        √âlimine les feuilles qui n'ont pas √©t√© mises √† jour lors de l'am√©lioration HNSW.

        Args:
            updated_leaves_set: Ensemble des indices des feuilles mises √† jour

        Returns:
            Nouvelle instance de TreeFlat avec uniquement les feuilles mises √† jour
        """
        print("üîç Suppression des feuilles non mises √† jour...")

        # 1. Cr√©er une copie de l'arbre plat
        pruned_tree = TreeFlat(self.dims, self.max_depth)

        # 2. Copier toutes les propri√©t√©s
        pruned_tree.depth = self.depth
        pruned_tree.n_nodes = self.n_nodes
        pruned_tree.n_levels = self.n_levels

        # 3. Copier les structures de n≈ìuds (inchang√©es)
        if self.node_centroids is not None:
            pruned_tree.node_centroids = self.node_centroids.copy()
        if self.node_centroids_reduced is not None:
            pruned_tree.node_centroids_reduced = self.node_centroids_reduced.copy()
        pruned_tree.node_children_top_dims = self.node_children_top_dims.copy()
        pruned_tree.node_children_d_head = self.node_children_d_head.copy()
        pruned_tree.node_levels = self.node_levels.copy()
        pruned_tree.node_is_leaf = self.node_is_leaf.copy()
        pruned_tree.node_children_count = self.node_children_count.copy()
        pruned_tree.node_children_start = self.node_children_start.copy()
        pruned_tree.children_indices = self.children_indices.copy()
        pruned_tree.leaf_offset = self.leaf_offset.copy()

        print(f"  ‚Üí {len(updated_leaves_set)}/{self.n_leaves} feuilles ont √©t√© mises √† jour")

        # 4. Reconstruire leaf_data avec uniquement les feuilles mises √† jour
        new_leaf_data_segments = []
        new_leaf_bounds = [0]
        new_leaf_mapping = {}  # mapping ancien_idx -> nouvel_idx

        new_leaf_idx = 0
        for leaf_idx in range(self.n_leaves):
            if leaf_idx in updated_leaves_set:
                # R√©cup√©rer les donn√©es de cette feuille
                start = self.leaf_bounds[leaf_idx]
                end = self.leaf_bounds[leaf_idx + 1]
                leaf_data = self.leaf_data[start:end]

                # Ajouter au nouveau leaf_data
                new_leaf_data_segments.append(leaf_data)
                new_bound = new_leaf_bounds[-1] + len(leaf_data)
                new_leaf_bounds.append(new_bound)

                # Mettre √† jour le mapping
                new_leaf_mapping[leaf_idx] = new_leaf_idx
                new_leaf_idx += 1

        # 5. Mettre √† jour leaf_data et leaf_bounds
        pruned_tree.leaf_data = np.concatenate(new_leaf_data_segments) if new_leaf_data_segments else np.array([], dtype=np.int32)
        pruned_tree.leaf_bounds = np.array(new_leaf_bounds, dtype=np.int32)

        # 6. Mettre √† jour leaf_offset
        # -1 pour les n≈ìuds internes et les feuilles non mises √† jour
        pruned_tree.leaf_offset = np.full(self.n_nodes, -1, dtype=np.int32)

        # Pour chaque n≈ìud, s'il s'agit d'une feuille mise √† jour, mettre √† jour son offset
        for i in range(self.n_nodes):
            if self.node_is_leaf[i]:
                old_leaf_idx = self.leaf_offset[i]
                if old_leaf_idx in new_leaf_mapping:
                    pruned_tree.leaf_offset[i] = new_leaf_mapping[old_leaf_idx]

        # 7. Mettre √† jour les statistiques
        pruned_tree.n_leaves = len(new_leaf_bounds) - 1

        # Calculer les √©conomies r√©alis√©es
        original_leaf_data_size = len(self.leaf_data)
        pruned_leaf_data_size = len(pruned_tree.leaf_data)
        reduction_percentage = ((original_leaf_data_size - pruned_leaf_data_size) / original_leaf_data_size) * 100

        print(f"‚úÖ Suppression des feuilles non mises √† jour termin√©e!")
        print(f"  ‚Üí Taille originale de leaf_data: {original_leaf_data_size}")
        print(f"  ‚Üí Taille r√©duite de leaf_data: {pruned_leaf_data_size}")
        print(f"  ‚Üí R√©duction: {reduction_percentage:.1f}% d'espace")

        return pruned_tree
```

